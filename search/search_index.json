{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Python Utility classes for Earthcube","text":"<p>Rendered Documentation with Code Documentation</p>"},{"location":"#earthcube_utilities","title":"earthcube_utilities","text":"<p>Utilites used in the Earthcube Geocodes Infrastructure A set of modules to connect to the backend services, query the graph store, and to gather infomrmation about  the Science on Schema JSONLD stored in the graph store.</p> <ul> <li>GeoCODES </li> <li>search </li> <li>and Notebooks</li> </ul>"},{"location":"#notebook-proxy","title":"Notebook Proxy","text":"<p>mknb.py  Creates parameterized NoteBook gists (from a template) for opening in binder or collab</p> <p>Please try the search &amp; click on its' feedback, incl for use-case that we can try to attain </p>"},{"location":"#summarize","title":"Summarize","text":"<p>Summarize is  tool to materialize a set of triples that are used to improve search performance.</p>"},{"location":"#artifacts","title":"Artifacts","text":""},{"location":"#notebook-proxy_1","title":"Notebook Proxy:","text":"<p>There is no pypi package, since this is code that runs in a docker container/local server</p> <p>https://hub.docker.com/repository/docker/nsfearthcube/mknb</p> <p>See README.md in src/notebook_proxy</p>"},{"location":"#earthcube-utilties","title":"Earthcube Utilties:","text":"<p>https://pypi.org/project/earthcube-utilities/</p> <p>python3 -m pip install earthcube-utilities</p> <p>Code Documentation</p>"},{"location":"#earthcube-utility-summarize","title":"Earthcube Utility Summarize:","text":"<p>https://test.pypi.org/project/earthcube-summarize/</p> <p>python3 -m pip install --index-url https://test.pypi.org/simple/ earthcube_summarize</p> <p>Documentation</p>"},{"location":"earthcube_utilities/","title":"Earthcube Utilities","text":""},{"location":"earthcube_utilities/#users","title":"Users","text":""},{"location":"earthcube_utilities/#earthcube-utilities_1","title":"Earthcube Utilities:","text":"<p>https://pypi.org/project/earthcube-utilities/</p>"},{"location":"earthcube_utilities/#manual-install","title":"Manual Install","text":"<p><code>python3 -m pip install  earthcube-utilities</code></p>"},{"location":"earthcube_utilities/#developers","title":"Developers","text":""},{"location":"earthcube_utilities/#scripts","title":"scripts","text":"<p>When installed via pip:</p> <p><code>query_graph SPARQL_FILE --graphendpoint https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/</code></p> <p><code>check_sitemap SITEMAP_URL --output FILE --no-check-url</code></p> <p><code>generategraphstats --graphendpoint https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/ -s3 localhost:9000 --s3bucket gleaner</code></p> <p>from console scripts</p>"},{"location":"earthcube_utilities/#local-development-mode","title":"local development mode","text":"<pre><code>cd earthcube_utiltiies\npip3 install -e .</code></pre>"},{"location":"earthcube_utilities/#developement","title":"Developement","text":"<p>create a virutal env and activate</p> <p><code>source {envname}/bin/activate</code></p> <p>use editable install</p> <pre><code>cd earthcube_utiltiies\npip3 install -e .</code></pre> <p>If you edit the pyproject.toml and want to test an added script,  <pre><code>cd summarize\npip3 uninstall -e earthcube_utiltiies\npip3 install -e .</code></pre></p>"},{"location":"earthcube_utilities/#building-a-test-package","title":"building a test package","text":""},{"location":"earthcube_utilities/#test-packaging","title":"test packaging","text":"<p>Locally,  see if a package builds <code>python3 -m pip install build</code></p> <p>in build/lib you can see what files are included in package</p>"},{"location":"earthcube_utilities/#build-a-wheel","title":"build a wheel","text":"<p>to see what is added to a package, </p> <p><code>python -m build --wheel</code></p> <p>dist directory will contain the package. This is actually a zip file so unzip to see  what got included</p>"},{"location":"earthcube_utilities/#planning","title":"Planning:","text":"<p>The planned functionality will be found in the docs folder, Earthcube Utilities Functionality</p>"},{"location":"earthcube_utilities/earthcube_utilities_code/","title":"Earthcube Utilities Code Documentation","text":"<p>Note</p> <p>this does not work with multirepo plug, so will need ot be linked </p>"},{"location":"earthcube_utilities/earthcube_utilities_code/#code-documentation","title":"Code Documentation","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.bucketutil","title":"<code>bucketutil</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.bucketutil.EcConfig","title":"<code>EcConfig</code>","text":"<p>               Bases: <code>object</code></p> <p>Parameters that might be common to commands</p> Source code in <code>earthcube_utilities/src/ec/bucketutil.py</code> <pre><code>class EcConfig(object):\n    \"\"\" Parameters that might be common to commands\"\"\"\n    def __init__(self, cfgfile=None, s3server=None, s3bucket=None, upload=None, output=None,debug=False):\n        if cfgfile:\n            s3endpoint, bucket, glnr = getGleaner(cfgfile)\n            minio = glnr.get(\"minio\")\n            # passed paramters override the config parameters\n            self.s3server = s3server if s3server else s3endpoint\n            self.bucket = s3bucket if s3bucket else bucket\n            self.glnr = glnr\n        else:\n            self.s3server = s3server\n            self.bucket = s3bucket\n        self.output= output\n        self.upload = upload\n        self.debug = debug\n\n    # lets put checks as methods in here.\n    # that way some checks if we can connect can be done in one place\n    def hasS3(self) -&gt; bool:\n         if   ( is_empty(self.s3server) or is_empty(self.bucket) ):\n             log.fatal(f\" must provide a gleaner config or (s3endpoint and s3bucket)]\")\n             raise Exception(\"must provide a gleaner config or (s3endpoint and s3bucket)]\")\n         return True\n    def hasS3Upload(self) -&gt; bool:\n         if  not self.upload and ( is_empty(self.s3server) or is_empty(self.bucket) ):\n             log.fatal(f\" must provide a gleaner config or (s3endpoint and s3bucket)]\")\n             raise Exception(\"must provide a gleaner config or (s3endpoint and s3bucket)]\")\n         return True</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.bucketutil.cullurls","title":"<code>cullurls(cfgfile, s3server, s3bucket, upload, output, debug, source)</code>","text":"<p>for a given url, find the sha of the file</p> Source code in <code>earthcube_utilities/src/ec/bucketutil.py</code> <pre><code>@cli.command()\n@click.option('--source', help='One or more repositories (--source a --source b)', multiple=True)\n@common_params\ndef cullurls(cfgfile, s3server, s3bucket, upload, output, debug, source):\n    \"\"\" for a given url, find the sha of the file\"\"\"\n    ctx = EcConfig(cfgfile, s3server, s3bucket, upload, output, debug)\n    ctx.hasS3()\n    s3Minio = s3.MinioDatastore(ctx.s3server, None)\n    utc = pytz.UTC\n    df = s3Minio.listDuplicateUrls(ctx.bucket, source)\n    try:\n        if len(df) &lt;= 0:\n            logging.info(f\"No object has been found\")\n            return\n\n        df = df[df['Url Duplicates'] &gt; 1]\n        if len(df) &lt;= 0:\n            logging.info(f\" Nothing to cull\")\n            return\n        # delete duplicated objects that are older than 7 days ago\n        df = df[df['Date'] &lt; utc.localize(datetime.datetime.now() - datetime.timedelta(days=7))]\n        removeObjectList = df.get('Name').values.tolist()\n        for r in removeObjectList:\n            s3Minio.removeObject(ctx.bucket, str(r))\n            logging.info('Removed object: ' + str(r))\n    except Exception as e:\n        logging.info(e)\n    return</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.bucketutil.download","title":"<code>download(cfgfile, s3server, s3bucket, upload, output, debug, urn)</code>","text":"<p>For a given URN(s), download the files and the Metadata</p> Source code in <code>earthcube_utilities/src/ec/bucketutil.py</code> <pre><code>@cli.command()\n@click.option('--urn', help='One or more urns (--urn a --urn b)', multiple=True, required=True)\n@common_params\ndef download(cfgfile, s3server, s3bucket, upload, output, debug, urn):\n    \"\"\"For a given URN(s), download the files and the Metadata\"\"\"\n    ctx = EcConfig(cfgfile, s3server, s3bucket, upload, output, debug)\n    s3Minio = s3.MinioDatastore(ctx.s3server, None)\n    ctx.hasS3()\n\n    for sha in urn:\n        # need to parse the source and actual sha/id out of the urn\n        # and then do the path and creat path\n        parts = parts_from_urn(sha)\n        mypath = parts.get('source')\n        id = parts.get('id')\n        if not os.path.isdir(mypath):\n            os.makedirs(mypath)\n        outFileName = f\"{mypath}/{id}.jsonld\"\n        log.info(outFileName)\n        outFile = open(outFileName, \"wb\")\n        o = s3Minio.getJsonLD(ctx.bucket, mypath, id)\n        outFile.write(o)\n        outFile.close()\n        outFileName = f\"{mypath}/{id}.jsonld.meta.txt\"\n        log.info(outFileName)\n        outFile = open(outFileName, \"wb\")\n        o = s3Minio.getJsonLDMetadata(ctx.bucket, mypath, id)\n        outFile.write( bytes(json.dumps(o), 'utf8'))\n        outFile.close()\n    return</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.bucketutil.duplicateurls","title":"<code>duplicateurls(cfgfile, s3server, s3bucket, upload, output, debug, source)</code>","text":"<p>Find Possible Duplicates based on the URL that was used to summon JSONLD. Note, the may be ok, if a given URL has more than one JSONLD in the HTML.</p> Source code in <code>earthcube_utilities/src/ec/bucketutil.py</code> <pre><code>@cli.command()\n@click.option('--source', help='One or more repositories (--source a --source b)', multiple=True)\n@common_params\ndef duplicateurls(cfgfile, s3server, s3bucket, upload, output, debug, source):\n    \"\"\" Find Possible Duplicates based on the URL that was used to summon JSONLD.\n    Note, the may be ok, if a given URL has more than one JSONLD in the HTML.\n    \"\"\"\n    ctx = EcConfig(cfgfile, s3server, s3bucket, upload, output, debug)\n    ctx.hasS3()\n    s3Minio = s3.MinioDatastore(ctx.s3server, None)\n    df = s3Minio.listDuplicateUrls(ctx.bucket, source)\n    try:\n        if len(df) &lt;= 0:\n            logging.info(f\"No object has been found\")\n            return\n\n        df = df[df['Url Duplicates'] &gt; 1]\n        if len(df) &lt;= 0:\n            logging.info(f\"No duplicate has been found\")\n            return\n\n        df['Example'] = df.apply(lambda f: f['Name'] + ', ' + str(f['Date']), axis=1)\n        df = df.groupby(['Source', 'Url'], group_keys=True, dropna=False) \\\n            .agg({'Name': 'count', 'Example': lambda x: x.iloc[0:5].tolist()}).reset_index()\n        df = df.rename(columns={'Name': 'Duplicates'})\n\n        res = df.to_csv(index=False)\n        sys.stdout.write(res)\n        if output:\n            output.write(bytes(res, 'utf-8'))\n        if upload:\n            s3Minio.putReportFile(ctx.bucket, \"all\", \"bucketutil_dupliactes.csv\", res)\n    except Exception as e:\n        logging.info(e)\n    return 0</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.bucketutil.sourceurl","title":"<code>sourceurl(cfgfile, s3server, s3bucket, upload, output, debug, url, summon, milled)</code>","text":"<p>for a given url, find the sha of the file</p> Source code in <code>earthcube_utilities/src/ec/bucketutil.py</code> <pre><code>@cli.command()\n@click.option('--url', help='the X-Amz-Meta-Url in metadata', required=True)\n@click.option('--milled', help='include milled', default=False)\n@click.option('--summoned', help='include summon only', default=True)\n@common_params\ndef sourceurl(cfgfile, s3server, s3bucket, upload, output, debug, url, summon, milled):\n    \"\"\" for a given url, find the sha of the file\"\"\"\n    ctx = EcConfig(cfgfile, s3server, s3bucket, upload, output, debug)\n    ctx.hasS3()\n    s3Minio = s3.MinioDatastore(ctx.s3server, None)\n    paths = list()\n    o_list = list()\n    if summon:\n        paths = list(s3Minio.listPath(ctx.bucket, 'summoned/', recursive=False))\n    if milled:\n        paths.extend(list(s3Minio.listPath(ctx.bucket, 'milled/', recursive=False)))\n    for path in paths:\n        try:\n            jsonlds = s3Minio.listPath(ctx.bucket, path.object_name, include_user_meta=True)\n            objs = map(lambda f: s3Minio.s3client.stat_object(f.bucket_name, f.object_name), jsonlds)\n            o_list.extend(list(filter(lambda f: f.metadata.get('X-Amz-Meta-Url') == url, objs)))\n        except Exception as e:\n            logging.error(e)\n\n    if len(o_list)&gt;0:\n        for o in o_list:\n            outFileName = f\"{o.metadata.get('X-Amz-Meta-Uniqueid')}.jsonld\"\n            log.info(outFileName)\n            outFile = open(outFileName, \"wb\")\n            s3ObjectInfo = {\"bucket_name\": ctx.bucket, \"object_name\": o.object_name}\n            resp = s3Minio.getFileFromStore(s3ObjectInfo)\n            outFile.write(resp)\n            outFile.close()\n            outFileName = f\"{o.metadata.get('X-Amz-Meta-Uniqueid')}.jsonld.meta.txt\"\n            log.info(outFileName)\n            outFile = open(outFileName, \"wb\")\n            s3ObjectInfo = {\"bucket_name\": ctx.bucket, \"object_name\": o.object_name}\n            tags = s3Minio.getFileMetadataFromStore(s3ObjectInfo)\n            outFile.write(bytes(json.dumps(tags, indent=4), 'utf8'))\n            outFile.close()\n    else:\n        log.info(f\"Nothing has been found for url: \", url)\n    return</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.bucketutil.urls","title":"<code>urls(cfgfile, s3server, s3bucket, upload, output, debug, source)</code>","text":"<p>Retreive the URL harvested for a give bucket. T There may be duplicate URL's, if an HTML has more than one JSONLD</p> Source code in <code>earthcube_utilities/src/ec/bucketutil.py</code> <pre><code>@cli.command()\n@click.option('--source', help='A repository', required=True)\n@common_params\ndef urls(cfgfile, s3server, s3bucket, upload, output, debug, source):\n    \"\"\"Retreive the URL harvested for a give bucket. T\n    There may be duplicate URL's, if an HTML has more than one JSONLD\"\"\"\n    ctx = EcConfig(cfgfile, s3server, s3bucket, upload, output, debug)\n    s3Minio = s3.MinioDatastore(ctx.s3server, None)\n    ctx.hasS3()\n\n    res = s3Minio.listSummonedUrls(ctx.bucket, source)\n    res = pd.DataFrame(res).to_csv(index=False, quoting=csv.QUOTE_NONNUMERIC)\n    sys.stdout.write(res)\n    if output:\n        log.info(f\"report for {source} appended to file\")\n        output.write(bytes(res, 'utf-8'))\n    if upload:\n        s3Minio.putReportFile(ctx.bucket, source, \"bucketutil_urls.csv\", res)\n    return</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.collection","title":"<code>collection</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.collection.rocrate_collection","title":"<code>rocrate_collection</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.collection.rocrate_collection.SosRocrate","title":"<code>SosRocrate</code>","text":"<p>               Bases: <code>ROCrate</code></p> <p>Science on Schema(SOS) ROCrate Object. ROCrate plus methods to easily add SOS information to an ROCrate</p> USE <p>Consuming an RO-Crate:</p> <p>use SosRocrate instead of RoCrate</p> Source code in <code>earthcube_utilities/src/ec/collection/rocrate_collection.py</code> <pre><code>class SosRocrate(ROCrate):\n    \"\"\" Science on Schema(SOS) ROCrate Object.\n    ROCrate plus methods to easily add SOS information to an ROCrate\n\n    USE:\n     [Consuming an RO-Crate](https://github.com/ResearchObject/ro-crate-py#consuming-an-ro-crate):\n    use SosRocrate instead of RoCrate\n\n    \"\"\"\n    def nameCrate(self, aName):\n        self.nameCrate(aName)\n\n    # this is the creator of the crate, it should be a sos_person\n    # probably not correct, but it's a start\n    def addSosCreator(self, crate, username):\n        properties = {\"name\": username}\n\n        creator = Person(crate, identifier=_createIdentifier(username), properties=properties)\n        self.creator=creator\n\n    # we are  of the crate, it should be a sos_person\n    # probably not correct, but it's a start\n    def addSosPublisher(self, crate, identifier=None, name=None):\n        properties = {\"name\": name}\n        if identifier is None:\n            identifier = name\n        creator = Person(crate, identifier=_createIdentifier(identifier), properties=properties)\n        self.publisher = creator\n\n        # pption 1 it's a file\n\n\n\n\n    def addSosDistribution(self, crate, dataset_jsonld, distribution_to_add=None, distType=DATASET__DATA_DOWNLOAD):\n        aurl = dataset_jsonld.get('url')\n        name = dataset_jsonld.get('name')\n        if distribution_to_add is None:\n            #kw = {\"fetch_remote\": fetch_remote, \"validate_url\": validate_url}\n            kw = {\"name\": name}\n            self.add_file(aurl, properties=kw)\n        elif distType == DATASET__DATA_DOWNLOAD :\n            pass\n        elif distType == DATASET__DATASET:\n            self.add_dataset(source=dataset_jsonld)\n\n    def addSosServicesAsEntity(self,crate, url=None, name=None):\n        kw = {\"name\": name}\n        self.add_file(source=url, properties=kw)\n        pass\n\n    def addSosURL(self, url=None, name=None):\n        properties = {\"name\":name}\n        self.add_file(source=url, properties=properties)\n        #self.add_file(url, identifier=CreateIdentifier(url), properties=kw)\n\n# two possible ways to add the whole JSONLD.\n    # 1. File: url plus the jsonld\n    # 2. Dataset (aka directory): url to geocodes, plus jsonld\n\n    # these need to check that these are datasets ;)\n    def addSosDatasetAsFile(self, dataset_jsonld,  url):\n        properties = dataset_jsonld\n        self.add_file(source=url, properties=properties)\n### humm https://github.com/ResearchObject/ro-crate-py#adding-entities-with-an-arbitrary-type\n    def addSosDatasetAsDataset(self, dataset_jsonld, urn, distType=DATASET__DATASET):\n        self.add_dataset(dest_path=urn, properties=dataset_jsonld)\n\n\n    # this does not get added to hasParts... aka does not get Identifieed as a Dataset\n    def addSosDatasetAsCtxEntity(self, dataset_jsonld, urn, distType=DATASET__DATASET):\n        \"\"\"Not useful. this does not get added to hasParts... aka does not get Identifieed as a Dataset \"\"\"\n        datasetCtxEntity =ContextEntity(self, identifier=urn, properties=dataset_jsonld)\n\n        self.add(datasetCtxEntity)\n    def addSosContact(self, sos_contact):\n            pass</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.collection.rocrate_collection.SosRocrate.addSosDatasetAsCtxEntity","title":"<code>addSosDatasetAsCtxEntity(dataset_jsonld, urn, distType=DATASET__DATASET)</code>","text":"<p>Not useful. this does not get added to hasParts... aka does not get Identifieed as a Dataset</p> Source code in <code>earthcube_utilities/src/ec/collection/rocrate_collection.py</code> <pre><code>def addSosDatasetAsCtxEntity(self, dataset_jsonld, urn, distType=DATASET__DATASET):\n    \"\"\"Not useful. this does not get added to hasParts... aka does not get Identifieed as a Dataset \"\"\"\n    datasetCtxEntity =ContextEntity(self, identifier=urn, properties=dataset_jsonld)\n\n    self.add(datasetCtxEntity)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore","title":"<code>datastore</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3","title":"<code>s3</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.MinioDatastore","title":"<code>MinioDatastore</code>","text":"<p>               Bases: <code>bucketDatastore</code></p> <p>Instance of a minio datastore with utility methods to retreive information</p> Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>class MinioDatastore(bucketDatastore):\n    \"\"\" Instance of a minio datastore with utility methods to retreive information\"\"\"\n    def __init__(self, s3endpoint, options={},default_bucket=\"gleaner\"):\n        \"\"\" Initilize with\n        Parameters:\n            s3endpoint: endpoint. If this is aws, include the region. eg s3.us-west-2.amazon....\n            options: creditials, other parameters to pass to client\n            default_bucket: 'gleaner'\n            \"\"\"\n        self.endpoint = s3endpoint\n        self.options = {} if options is None else options # old code has none...\n        self.default_bucket= default_bucket\n        logging.info(str(options))\n        self.s3client  =minio.Minio(s3endpoint, **self.options ) # this will neeed to be fixed with authentication\n\n\n    def listPath(self, bucket, path, include_user_meta=False, recursive=True):\n        \"\"\" returns the filelist for a path with the starting path removed from the list\"\"\"\n        resp = self.s3client.list_objects(bucket, path, include_user_meta=include_user_meta, recursive=recursive)\n        # the returned list includes the path\n        o_list = filter(lambda f: f.object_name != path, resp)\n        return o_list\n\n    def listDuplicateUrls(self, bucket, sources, include_user_meta=False, recursive=False):\n        if is_empty(sources):\n            raise Exception(\"must provide sources\")\n\n        sources_to_run = sources\n        dfs = pandas.DataFrame()\n        paths = list(self.listPath(bucket, \"summoned/\", include_user_meta=include_user_meta, recursive=recursive))\n        for p in paths:\n            if not find(sources_to_run, lambda x: f\"{self.paths.get('summon')}/{x}/\" == p.object_name):\n                continue\n            try:\n                jsonlds = self.listPath(bucket, p.object_name)\n                objs = map(lambda f: self.s3client.stat_object(f.bucket_name, f.object_name), jsonlds)\n                o_list = list(map(lambda f: {'Source': p.object_name,\n                                             'Url': f.metadata.get('X-Amz-Meta-Url'),\n                                             'Name': f.object_name,\n                                             'Date': f.last_modified,\n                                             }, objs))\n            except Exception as e:\n                logging.error(e)\n            df = pandas.DataFrame(o_list)\n            df['Url Duplicates'] = df.groupby(['Source', 'Url'])['Name'].transform('count')\n            dfs = pandas.concat([dfs, df])\n        return dfs\n\n\n    def countPath(self, bucket, path):\n        count = len(list(self.listPath(bucket, path)))\n        return count\n\n    def getFileFromStore(self, s3ObjectInfo):\n        \"\"\" get an s3 file from teh store\n        Parameters:\n          s3ObjectInfo: {\"bucket_name\":obj.bucket_name, \"object_name\":obj.object_name }\n\n        \"\"\"\n        resp = self.s3client.get_object(s3ObjectInfo[\"bucket_name\"], s3ObjectInfo[\"object_name\"])\n        return resp.data\n    def DataframeFromPath(self, bucket, path, include_user_meta=False):\n        pathFiles = list(self.listPath(bucket,path,include_user_meta=include_user_meta))\n\n        objs = map(lambda f: self.s3client.stat_object(f.bucket_name, f.object_name), pathFiles)\n        data = map(lambda f: { \"metadata\": f.metadata, \"bucket_name\":f.bucket_name, \"object_name\":f.object_name}, objs )\n        # does not work... should, but does not\n        # data = list(map(lambda f:\n        #                 pick(f,  'bucket_name', 'object_name')\n        #                 , objs ))\n\n        df = pandas.DataFrame(data=data)\n        return df\n\n    def getFileMetadataFromStore(self, s3ObjectInfo):\n        \"\"\" get metadata s3 file from teh store\n               Parameters:\n                 s3ObjectInfo: {\"bucket_name\":obj.bucket_name, \"object_name\":obj.object_name }\n\n               \"\"\"\n        s3obj = self.s3client.stat_object(s3ObjectInfo.get('bucket_name'), s3ObjectInfo.get('object_name'))\n        md = s3obj.metadata\n        user_meta = list()\n        for o in md:\n            if o.startswith(\"X-Amz-Meta\"):\n               user_meta.append({\"name\": o, \"value\": md[o]})\n        # this needs to return the metadata\n        return user_meta\n\n    def putReportFile(self, bucket, repo, filename, json_str, date=\"latest\", copy_to_date=True):\n        path = f\"{self.paths['report']}/{repo}/{date}/{filename}\"\n        f = BytesIO()\n        length = f.write(bytes(json_str, 'utf-8'))\n        f.seek(0)\n        resp = self.s3client.put_object(bucket, path, f,length=length)\n        if copy_to_date:\n            today_str = datetime.now().strftime(\"%Y%m%d\")\n            path = f\"{self.paths['report']}/{repo}/{today_str}/{filename}\"\n            self.copyObject(resp,path)\n        return resp.bucket_name, resp.object_name\n\n    def getReportFile(self, bucket, repo, filename):\n        path = f\"{self.paths['report']}/{repo}/latest/{filename}\"\n        s3ObjectInfo = {\"bucket_name\": bucket, \"object_name\": path}\n        resp = self.getFileFromStore(s3ObjectInfo)\n        return resp\n\n    def getLatestRelaseFile(self, bucket, repo):\n        path = f\"{self.paths['graph']}/latest/summonded{repo}_latest_release.nq\"\n        s3ObjectInfo = {\"bucket_name\":bucket,\"object_name\": path}\n        resp = self.getFileFromStore(s3ObjectInfo)\n        return resp\n\n    def getRelasePaths(self, bucket):\n        path = f\"{self.paths['graph']}/latest/\"\n        files = self.listPath(bucket, path)\n        paths = list(map(lambda f:  f.object_name, files))\n        return paths\n\n    def getRoCrateFile(self, filename, bucket=\"gleaner\", user=\"public\"):\n        path = f\"/{self.paths['collection']}/{user}/{filename}\"\n        crate = self.s3client.get_object(bucket, path)\n        return crate\n\n    def removeObject(self, bucket, path):\n        self.s3client.remove_object(bucket, path)\n        return</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.MinioDatastore.__init__","title":"<code>__init__(s3endpoint, options={}, default_bucket='gleaner')</code>","text":"<p>Initilize with Parameters:     s3endpoint: endpoint. If this is aws, include the region. eg s3.us-west-2.amazon....     options: creditials, other parameters to pass to client     default_bucket: 'gleaner'</p> Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>def __init__(self, s3endpoint, options={},default_bucket=\"gleaner\"):\n    \"\"\" Initilize with\n    Parameters:\n        s3endpoint: endpoint. If this is aws, include the region. eg s3.us-west-2.amazon....\n        options: creditials, other parameters to pass to client\n        default_bucket: 'gleaner'\n        \"\"\"\n    self.endpoint = s3endpoint\n    self.options = {} if options is None else options # old code has none...\n    self.default_bucket= default_bucket\n    logging.info(str(options))\n    self.s3client  =minio.Minio(s3endpoint, **self.options ) # this will neeed to be fixed with authentication</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.MinioDatastore.getFileFromStore","title":"<code>getFileFromStore(s3ObjectInfo)</code>","text":"<p>get an s3 file from teh store Parameters:   s3ObjectInfo: {\"bucket_name\":obj.bucket_name, \"object_name\":obj.object_name }</p> Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>def getFileFromStore(self, s3ObjectInfo):\n    \"\"\" get an s3 file from teh store\n    Parameters:\n      s3ObjectInfo: {\"bucket_name\":obj.bucket_name, \"object_name\":obj.object_name }\n\n    \"\"\"\n    resp = self.s3client.get_object(s3ObjectInfo[\"bucket_name\"], s3ObjectInfo[\"object_name\"])\n    return resp.data</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.MinioDatastore.getFileMetadataFromStore","title":"<code>getFileMetadataFromStore(s3ObjectInfo)</code>","text":"<p>get metadata s3 file from teh store Parameters:   s3ObjectInfo: {\"bucket_name\":obj.bucket_name, \"object_name\":obj.object_name }</p> Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>def getFileMetadataFromStore(self, s3ObjectInfo):\n    \"\"\" get metadata s3 file from teh store\n           Parameters:\n             s3ObjectInfo: {\"bucket_name\":obj.bucket_name, \"object_name\":obj.object_name }\n\n           \"\"\"\n    s3obj = self.s3client.stat_object(s3ObjectInfo.get('bucket_name'), s3ObjectInfo.get('object_name'))\n    md = s3obj.metadata\n    user_meta = list()\n    for o in md:\n        if o.startswith(\"X-Amz-Meta\"):\n           user_meta.append({\"name\": o, \"value\": md[o]})\n    # this needs to return the metadata\n    return user_meta</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.MinioDatastore.listPath","title":"<code>listPath(bucket, path, include_user_meta=False, recursive=True)</code>","text":"<p>returns the filelist for a path with the starting path removed from the list</p> Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>def listPath(self, bucket, path, include_user_meta=False, recursive=True):\n    \"\"\" returns the filelist for a path with the starting path removed from the list\"\"\"\n    resp = self.s3client.list_objects(bucket, path, include_user_meta=include_user_meta, recursive=recursive)\n    # the returned list includes the path\n    o_list = filter(lambda f: f.object_name != path, resp)\n    return o_list</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.bucketDatastore","title":"<code>bucketDatastore</code>","text":"Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>class bucketDatastore():\n    endpoint = \"http://localhost:9000\" # basically minio\n    options = {\"secure\":True,\n               \"region\": 'us-west-2',\n#               \"access_key\": None,\n #              \"secret_key\": None\n               }\n    default_bucket=\"gleaner\"\n    paths = {\"report\":\"reports\",\n             \"summon\": \"summoned\",\n             \"milled\":\"milled\",\n             \"graph\":\"graphs\",\n             \"release\":\"graphs\",\n             \"archive\":\"archive\",\n             \"collection\":\"collections\",\n             \"sitemap\":\"sitemaps\"\n    }\n\n    def __init__(self, s3endpoint, options, default_bucket=\"gleaner\"):\n        self.endpoint = s3endpoint\n        self.options = options\n        self.default_bucket = default_bucket\n\n    def listPath(self, bucket, path, include_user_meta=False):\n        pass\n    def countPath(self, bucket, path):\n        count = len(list(self.listPath(bucket, path)))\n        return count\n\n    def DataframeFromPath(self, bucket, path, include_user_meta=False):\n        pass\n# who knows, we might implement on disk, or in a database. This just separates the data from the annotated metadata\n    def getFileFromStore(self, s3ObjectInfo):\n        pass\n    def getFileMetadataFromStore(self, s3ObjectInfo):\n        pass\n    def putTextFileToStore(self,data, s3ObjectInfo ):\n        f = BytesIO()\n        length = f.write(bytes(data, 'utf-8'))\n        f.seek(0)\n        resp = self.s3client.put_object(s3ObjectInfo.bucket_name, s3ObjectInfo.object_name, f,length=length)\n        return resp.bucket_name, resp.object_name\n    def copyObject(self,s3ObjectInfoToCopy, ObjectPath ):\n        ''' Server Side Copy, eg upload report to latest, make copy to today'''\n        self.s3client.copy_object(\n            s3ObjectInfoToCopy.bucket_name,\n            ObjectPath,\n            CopySource(s3ObjectInfoToCopy.bucket_name, s3ObjectInfoToCopy.object_name),\n           # metadata=metadata,\n            metadata_directive=REPLACE,\n        )\n\n    #### Methods for a getting information using infrastructure information\n\n    \"\"\" Method for gleaner store\"\"\"\n    def listJsonld(self,bucket, repo, include_user_meta=False):\n        \"\"\" urllist returns list of urn;s with urls\"\"\"\n        # include user meta not working.\n        path = f\"{self.paths['summon']}/{repo}/\"\n        return self.listPath(bucket, path,include_user_meta=include_user_meta)\n\n    def countJsonld(self,bucket, repo) -&gt; int:\n        count = len(list(self.listJsonld(bucket,repo)))\n        return count\n\n    def getJsonLD(self, bucket, repo, sha):\n        path = f\"{self.paths['summon']}/{repo}/{sha}.jsonld\"\n        s3ObjectInfo = {\"bucket_name\": bucket, \"object_name\": path}\n        resp = self.getFileFromStore(s3ObjectInfo)\n        return resp\n\n\n    def listSummonedUrls(self,bucket, repo):\n        \"\"\"  returns list of urns with urls\"\"\"\n        jsonlds = self.listJsonld(bucket, repo, include_user_meta=True)\n        objs = map(lambda f: self.s3client.stat_object(f.bucket_name, f.object_name), jsonlds)\n        o_list = list(map(lambda f: {\"sha\": shaFroms3Path(f.object_name),\n                                     \"url\": f.metadata.get(\"X-Amz-Meta-Url\"),\n                                     \"identifiertype\": f.metadata.get(\"X-Amz-Meta-Identifiertype\"),\n                                     \"lastmodified\": f.last_modified\n                                     }\n                          , objs))\n        return o_list\n\n    def listSummonedSha(self,bucket, repo):\n        \"\"\"  returns list of urns with urls\"\"\"\n        jsonlds = self.listJsonld(bucket, repo, include_user_meta=False)\n        objs = map(lambda f: shaFroms3Path( f.object_name, extension=\".jsonld\"), jsonlds)\n        # for ob in objs:\n        #     print(ob)\n        o_list = list(objs)\n        return o_list\n\n    def getJsonLDMetadata(self, bucket, repo, sha):\n        path = f\"{self.paths['summon']}/{repo}/{sha}.jsonld\"\n        s3ObjectInfo = {\"bucket_name\": bucket, \"object_name\": path}\n        tags = self.getFileMetadataFromStore(s3ObjectInfo)\n\n        return tags\n\n    def getOringalUrl(self, bucket, repo, sha) -&gt; str:\n        md = self.getJsonLDMetadata(bucket, repo, sha)\n        return md['Url']\n\n    '''Cleans the name of slashes... might need more in the future.'''\n    def getCleanObjectName(s3ObjectName) -&gt; str:\n        return s3ObjectName.replace('/','__')\n\n    def listMilledRdf(self,bucket, repo,urnonly=False):\n        path = f\"{self.paths['milled']}/{repo}/\"\n        return self.listPath(bucket, path)\n    def listMilledSha(self,bucket, repo,urnonly=False):\n        paths = self.listMilledRdf(bucket,repo)\n        shas = list(map(lambda p: shaFroms3Path(p.object_name, extension=\".rdf\"), paths))\n        return shas\n\n    def countMilledRdf(self,bucket, repo) -&gt; int:\n        count = len(list(self.listMilledRdf(bucket,repo)))\n        return count\n    ### methods for reporting\n    '''\n    Reporting will have to pull the original and put back to the datastore\n    '''\n\n    def listReportFile(self,bucket, repo,include_user_meta=False):\n        \"\"\" urllist returns list of urn;s with urls\"\"\"\n        # include user meta not working.\n        path = f\"{self.paths['report']}/{repo}/\"\n        return self.listPath(bucket, path,include_user_meta=include_user_meta)\n\n    def putReportFile(self, bucket, repo, filename, json_str,  date=\"latest\", copy_to_date=True):\n\n        pass\n\n    def getReportFile(self, bucket, repo, filename):\n        path = f\"{self.paths['report']}/{repo}/filename\"\n        s3ObjectInfo = {\"bucket_name\": bucket, \"object_name\": path}\n        resp = self.getFileFromStore(s3ObjectInfo)\n        return resp\n\n    def getLatestRelaseUrl(self, bucket, source, extension='nq'):\n        urls = self.getLatestRelaseUrls(bucket)\n        url = pydash.find( urls, lambda x: source in x.get(\"object_name\") and \"_release\" in x.get(\"object_name\")  and x.get(\"object_name\").endswith(extension) )\n        return url.get('url')\n\n    def getLatestRelaseUrls(self, bucket):\n        path = f\"{self.paths['release']}/latest/\"\n        urls = self.listPath(bucket,path)\n        urls = map(lambda f: { \"object_name\": f.object_name,\n                               \"url\": f\"https://{self.endpoint}/{f.bucket_name}/{f.object_name}\" }, urls)\n        return list(urls)\n    def getLatestSummaryUrl(self, bucket, source, extension='ttl'):\n        urls = self.getLatestRelaseUrls(bucket)\n        url = pydash.find( urls, lambda x: source in x.get(\"object_name\") and \"_release\" in x.get(\"object_name\")  and x.get(\"object_name\").endswith(extension) )\n        return url.get('url')\n\n    def getLatestSummaryUrls(self, bucket):\n        path = f\"{self.paths['release']}/summary/\"\n        urls = self.listPath(bucket,path)\n        urls = map(lambda f: { \"object_name\": f.object_name,\n                               \"url\": f\"https://{self.endpoint}/{f.bucket_name}/{f.object_name}\" }, urls)\n        return list(urls)\n\n    def getRoCrateFile(self, filename, bucket=\"gleaner\", user=\"public\"):\n        path = f\"{self.paths['crate']}/{user}/{filename}\"\n        s3ObjectInfo = {\"bucket_name\": bucket, \"object_name\": path}\n        resp = self.getFileFromStore(s3ObjectInfo)\n        return resp\n\n    def putRoCrateFile(self, data: str,filename, bucket=\"gleaner\", user=\"public\"):\n        path = f\"{self.paths['report']}/{user}/{filename}\"\n        s3ObjectInfo = {\"bucket_name\": bucket, \"object_name\": path}\n        return self.putTextFileToStore(data, s3ObjectInfo)\n\n    def getSitemapFile(self,bucket, repo, filename):\n        path = f\"{self.paths['sitemap']}/{repo}/filename\"\n        s3ObjectInfo = {\"bucket_name\": bucket, \"object_name\": path}\n        resp = self.getFileFromStore(s3ObjectInfo)\n        return resp\n\n    def putSitemapFile(self,data: str, repo: str,filename: str, bucket=\"gleaner\"):\n        path = f\"{self.paths['sitemap']}/{repo}/{filename}\"\n        s3ObjectInfo = {\"bucket_name\": bucket, \"object_name\": path}\n        return self.putTextFileToStore(data, s3ObjectInfo)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.bucketDatastore.copyObject","title":"<code>copyObject(s3ObjectInfoToCopy, ObjectPath)</code>","text":"<p>Server Side Copy, eg upload report to latest, make copy to today</p> Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>def copyObject(self,s3ObjectInfoToCopy, ObjectPath ):\n    ''' Server Side Copy, eg upload report to latest, make copy to today'''\n    self.s3client.copy_object(\n        s3ObjectInfoToCopy.bucket_name,\n        ObjectPath,\n        CopySource(s3ObjectInfoToCopy.bucket_name, s3ObjectInfoToCopy.object_name),\n       # metadata=metadata,\n        metadata_directive=REPLACE,\n    )</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.bucketDatastore.listJsonld","title":"<code>listJsonld(bucket, repo, include_user_meta=False)</code>","text":"<p>urllist returns list of urn;s with urls</p> Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>def listJsonld(self,bucket, repo, include_user_meta=False):\n    \"\"\" urllist returns list of urn;s with urls\"\"\"\n    # include user meta not working.\n    path = f\"{self.paths['summon']}/{repo}/\"\n    return self.listPath(bucket, path,include_user_meta=include_user_meta)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.bucketDatastore.listReportFile","title":"<code>listReportFile(bucket, repo, include_user_meta=False)</code>","text":"<p>urllist returns list of urn;s with urls</p> Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>def listReportFile(self,bucket, repo,include_user_meta=False):\n    \"\"\" urllist returns list of urn;s with urls\"\"\"\n    # include user meta not working.\n    path = f\"{self.paths['report']}/{repo}/\"\n    return self.listPath(bucket, path,include_user_meta=include_user_meta)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.bucketDatastore.listSummonedSha","title":"<code>listSummonedSha(bucket, repo)</code>","text":"<p>returns list of urns with urls</p> Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>def listSummonedSha(self,bucket, repo):\n    \"\"\"  returns list of urns with urls\"\"\"\n    jsonlds = self.listJsonld(bucket, repo, include_user_meta=False)\n    objs = map(lambda f: shaFroms3Path( f.object_name, extension=\".jsonld\"), jsonlds)\n    # for ob in objs:\n    #     print(ob)\n    o_list = list(objs)\n    return o_list</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.datastore.s3.bucketDatastore.listSummonedUrls","title":"<code>listSummonedUrls(bucket, repo)</code>","text":"<p>returns list of urns with urls</p> Source code in <code>earthcube_utilities/src/ec/datastore/s3.py</code> <pre><code>def listSummonedUrls(self,bucket, repo):\n    \"\"\"  returns list of urns with urls\"\"\"\n    jsonlds = self.listJsonld(bucket, repo, include_user_meta=True)\n    objs = map(lambda f: self.s3client.stat_object(f.bucket_name, f.object_name), jsonlds)\n    o_list = list(map(lambda f: {\"sha\": shaFroms3Path(f.object_name),\n                                 \"url\": f.metadata.get(\"X-Amz-Meta-Url\"),\n                                 \"identifiertype\": f.metadata.get(\"X-Amz-Meta-Identifiertype\"),\n                                 \"lastmodified\": f.last_modified\n                                 }\n                      , objs))\n    return o_list</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.ec_reports","title":"<code>ec_reports</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.ec_reports.EcConfig","title":"<code>EcConfig</code>","text":"<p>               Bases: <code>object</code></p> <p>Parameters that might be common to commands</p> Source code in <code>earthcube_utilities/src/ec/ec_reports.py</code> <pre><code>class EcConfig(object):\n    \"\"\" Parameters that might be common to commands\"\"\"\n    def __init__(self, cfgfile=None, s3server=None, s3bucket=None, graphendpoint=None, upload=None, output=None,debug=False):\n        if cfgfile:\n            s3endpoint, bucket, glnr = getGleaner(cfgfile)\n            minio = glnr.get(\"minio\")\n            # passed paramters override the config parameters\n            self.s3server = s3server if s3server else s3endpoint\n            self.bucket = s3bucket if s3bucket else bucket\n        else:\n            self.s3server = s3server\n            self.bucket = s3bucket\n        self.graphendpoint = graphendpoint\n        self.output = output\n        self.upload = upload\n        self.debug = debug\n\n    # lets put checks as methods in here.\n    # that way some checks if we can connect can be done in one place\n    def hasS3(self) -&gt; bool:\n         if   ( is_empty(self.s3server) or is_empty(self.bucket) ):\n             log.fatal(f\" must provide a gleaner config or (s3endpoint and s3bucket)]\")\n             raise Exception(\"must provide a gleaner config or (s3endpoint and s3bucket)]\")\n         return True\n    def hasS3Upload(self) -&gt; bool:\n         if  not self.upload and ( is_empty(self.s3server) or is_empty(self.bucket) ):\n             log.fatal(f\" must provide a gleaner config or (s3endpoint and s3bucket)]\")\n             raise Exception(\"must provide a gleaner config or (s3endpoint and s3bucket)]\")\n         return True\n    def hasGraphendpoint(self, option:bool=False, message=\"must provide graphendpoint\") -&gt; bool:\n         \"\"\" if option is not true, so if summon only, then empty is graphendpoint is ok\n            \"\"\"\n         if    not option and is_empty(self.graphendpoint) :\n             log.fatal(message)\n             raise Exception(message)\n         return True</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.ec_reports.EcConfig.hasGraphendpoint","title":"<code>hasGraphendpoint(option=False, message='must provide graphendpoint')</code>","text":"<p>if option is not true, so if summon only, then empty is graphendpoint is ok</p> Source code in <code>earthcube_utilities/src/ec/ec_reports.py</code> <pre><code>def hasGraphendpoint(self, option:bool=False, message=\"must provide graphendpoint\") -&gt; bool:\n     \"\"\" if option is not true, so if summon only, then empty is graphendpoint is ok\n        \"\"\"\n     if    not option and is_empty(self.graphendpoint) :\n         log.fatal(message)\n         raise Exception(message)\n     return True</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.gleanerio","title":"<code>gleanerio</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.gleanerio.gleaner","title":"<code>gleaner</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.gleanerio.gleaner.getGleanerFromFile","title":"<code>getGleanerFromFile(cfgfile_obj)</code>","text":"<p>read a file from argparse when reading args where a file is returned, use this one</p> <p>Parameters:</p> Name Type Description Default <code>cfgfile_obj</code> <p>an argparse.FileType, maybe an io.FileIo</p> required Source code in <code>earthcube_utilities/src/ec/gleanerio/gleaner.py</code> <pre><code>def getGleanerFromFile( cfgfile_obj):\n    \"\"\" read a file from argparse\n    when reading args where a file is returned, use this one\n\n\n    Parameters:\n        cfgfile_obj: an argparse.FileType, maybe an io.FileIo\n    \"\"\"\n    cfg = yaml.safe_load(cfgfile_obj)\n    bucket = cfg['minio']['bucket']\n    s3endpoint = cfg['minio']['address']\n    return s3endpoint,  bucket, cfg</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.gleanerio.gleaner.getNabuFromFile","title":"<code>getNabuFromFile(cfgfile_obj)</code>","text":"<p>read a file from argparse when reading args where a file is returned, use this one</p> <p>Parameters:</p> Name Type Description Default <code>cfgfile_obj</code> <p>an argparse.FileType, maybe an io.FileIo</p> required Source code in <code>earthcube_utilities/src/ec/gleanerio/gleaner.py</code> <pre><code>def getNabuFromFile( cfgfile_obj) -&gt; Tuple[str, object]:\n    \"\"\" read a file from argparse\n    when reading args where a file is returned, use this one\n\n\n    Parameters:\n        cfgfile_obj: an argparse.FileType, maybe an io.FileIo\n\"\"\"\n\n    \"\"\" when reading args where a file is returned, use this one\"\"\"\n    cfg = yaml.safe_load(cfgfile_obj)\n    endpoint = cfg['sparql']['endpoint']\n    #     endpoint = cfg['sparql']['endpoint']\n    return endpoint, cfg</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph","title":"<code>graph</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph","title":"<code>manageGraph</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageBlazegraph","title":"<code>ManageBlazegraph</code>","text":"<p>               Bases: <code>ManageGraph</code></p> <p>Manages a blazegraph instance for a single namespace implements needed portions of the blazegraph rest API</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>class ManageBlazegraph(ManageGraph):\n    \"\"\" Manages a blazegraph instance for a single namespace\n    implements needed portions of the blazegraph rest API\"\"\"\n\n    createTemplateQuad =\"\"\"com.bigdata.namespace.fffff.spo.com.bigdata.btree.BTree.branchingFactor=1024\ncom.bigdata.rdf.store.AbstractTripleStore.textIndex=true\ncom.bigdata.namespace.fffff.lex.com.bigdata.btree.BTree.branchingFactor=400\ncom.bigdata.rdf.store.AbstractTripleStore.axiomsClass=com.bigdata.rdf.axioms.NoAxioms\ncom.bigdata.rdf.sail.isolatableIndices=false\ncom.bigdata.rdf.sail.truthMaintenance=false\ncom.bigdata.rdf.store.AbstractTripleStore.justify=false\ncom.bigdata.rdf.store.AbstractTripleStore.quads=true\ncom.bigdata.journal.Journal.groupCommit=false\ncom.bigdata.rdf.store.AbstractTripleStore.geoSpatial=false\ncom.bigdata.rdf.store.AbstractTripleStore.statementIdentifiers=false\n\"\"\"\n\n    createTemplateTriples = \"\"\"com.bigdata.namespace.fffff.spo.com.bigdata.btree.BTree.branchingFactor=1024\ncom.bigdata.rdf.store.AbstractTripleStore.textIndex=true\ncom.bigdata.namespace.fffff.lex.com.bigdata.btree.BTree.branchingFactor=400\ncom.bigdata.rdf.store.AbstractTripleStore.axiomsClass=com.bigdata.rdf.axioms.NoAxioms\ncom.bigdata.rdf.sail.isolatableIndices=false\ncom.bigdata.rdf.sail.truthMaintenance=false\ncom.bigdata.rdf.store.AbstractTripleStore.justify=false\ncom.bigdata.rdf.sail.namespace=fffff\ncom.bigdata.rdf.store.AbstractTripleStore.quads=false\ncom.bigdata.journal.Journal.groupCommit=false\ncom.bigdata.rdf.store.AbstractTripleStore.geoSpatial=false\ncom.bigdata.rdf.store.AbstractTripleStore.statementIdentifiers=false\n\"\"\"\n    #init w/namespace\n\n    def createNamespace(self, quads=True):\n        \"\"\" Creates a new namespace\"\"\"\n        # POST / bigdata / namespace\n        # ...\n        # Content - Type\n        # ...\n        # BODY\n       # add this to the createTemplates\n        # # com.bigdata.rdf.sail.namespace = {namespace}\n        if quads:\n            template = self.createTemplateQuad\n        else:\n            template = self.createTemplateTriples\n        template = template + f\"com.bigdata.rdf.sail.namespace = {self.namespace}\\n\"\n        url = f\"{self.baseurl}/namespace\"\n        headers = {\"Content-Type\": \"text/plain\"}\n        r = requests.post(url,data=template, headers=headers)\n        if r.status_code==201:\n            return \"Created\"\n        elif  r.status_code==409:\n            return \"Exists\"\n        else:\n            raise Exception(f\"Create Failed. Status code: {r.status_code} {r.reason}\")\n\n\n    def deleteNamespace(self):\n        \"\"\" deletes a blazegraph namespace\"\"\"\n        # DELETE /bigdata/namespace/NAMESPACE\n        url = f\"{self.baseurl}/namespace/{self.namespace}\"\n        headers = {\"Content-Type\": \"text/plain\"}\n        r = requests.delete(url, headers=headers)\n        if r.status_code == 200:\n            return \"Deleted\"\n        else:\n            raise Exception(\"Delete Failed.\")\n\n\n    def insert(self, data, content_type=\"text/x-nquads\"):\n        \"\"\"inserts data into a blazegraph namespace\"\"\"\n        # rdf datatypes: https://github.com/blazegraph/database/wiki/REST_API#rdf-data\n        # insert: https://github.com/blazegraph/database/wiki/REST_API#insert\n       #url = f\"{self.baseurl}/namespace/{self.namespace}{self.sparql}\"\n       #could call insure final slash\n        url = f\"{self.baseurl}/namespace/{self.namespace}/{self.sparql}\"\n        log.info(f'insert to {url} ')\n        headers = {\"Content-Type\": f\"{content_type}\"}\n        r = requests.post(url,data=data, headers=headers)\n        log.debug(f' status:{r.status_code}') #status:404\n        log.info(f' status:{r.status_code}') #status:404\n        if r.status_code == 200:\n            # '&lt;?xml version=\"1.0\"?&gt;&lt;data modified=\"0\" milliseconds=\"7\"/&gt;'\n            if 'data modified=\"0\"'  in r.text:\n                raise Exception(\"No Data Added: \" + r.text)\n            return True\n        else:\n            return False\n\n    #have upload methods here\n    #have graph instance:&lt;manageGraph.ManageBlazegraph object at ..&gt;, for url:https://graph.geocodes.ncsa.illinois.edu/blazegraph\n    #tmp_endpoint=f'https://graph.geocodes.ncsa.illinois.edu/blazegraph/namespace/{repo}/sparql'\n\n    def upload_file(self, filename, content_type=\"text/x-nquads\"):\n        \"to temp namespace or final one if given\"\n        log.debug(f'upload_file:{filename}')\n        log.info(f'upload_file:{filename}')\n        #open file and insert data\n        data = open(filename, 'rb').read()\n        log.debug(f'insert:{filename}')\n        log.info(f'insert:{filename}')\n        self.insert(data, content_type)\n\n    def upload_nq_file(self, fn=None):\n        \"will default to ns.nq\"\n        if fn:\n            filename=fn\n        else:\n            filename=self.namespace + \".nq\"\n        self.upload_file(filename)\n\n    def upload_ttl_file(self, fn=None):\n        \"will default to ns.ttl\"\n        if fn: #will want to upload ns=repo.ttl to ns=summary in the end\n            filename=fn\n        else:\n            filename=self.namespace + \".ttl\"\n        self.upload_file(filename, 'Content-Type:text/x-turtle')</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageBlazegraph.createNamespace","title":"<code>createNamespace(quads=True)</code>","text":"<p>Creates a new namespace</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>def createNamespace(self, quads=True):\n    \"\"\" Creates a new namespace\"\"\"\n    # POST / bigdata / namespace\n    # ...\n    # Content - Type\n    # ...\n    # BODY\n   # add this to the createTemplates\n    # # com.bigdata.rdf.sail.namespace = {namespace}\n    if quads:\n        template = self.createTemplateQuad\n    else:\n        template = self.createTemplateTriples\n    template = template + f\"com.bigdata.rdf.sail.namespace = {self.namespace}\\n\"\n    url = f\"{self.baseurl}/namespace\"\n    headers = {\"Content-Type\": \"text/plain\"}\n    r = requests.post(url,data=template, headers=headers)\n    if r.status_code==201:\n        return \"Created\"\n    elif  r.status_code==409:\n        return \"Exists\"\n    else:\n        raise Exception(f\"Create Failed. Status code: {r.status_code} {r.reason}\")</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageBlazegraph.deleteNamespace","title":"<code>deleteNamespace()</code>","text":"<p>deletes a blazegraph namespace</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>def deleteNamespace(self):\n    \"\"\" deletes a blazegraph namespace\"\"\"\n    # DELETE /bigdata/namespace/NAMESPACE\n    url = f\"{self.baseurl}/namespace/{self.namespace}\"\n    headers = {\"Content-Type\": \"text/plain\"}\n    r = requests.delete(url, headers=headers)\n    if r.status_code == 200:\n        return \"Deleted\"\n    else:\n        raise Exception(\"Delete Failed.\")</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageBlazegraph.insert","title":"<code>insert(data, content_type='text/x-nquads')</code>","text":"<p>inserts data into a blazegraph namespace</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>def insert(self, data, content_type=\"text/x-nquads\"):\n    \"\"\"inserts data into a blazegraph namespace\"\"\"\n    # rdf datatypes: https://github.com/blazegraph/database/wiki/REST_API#rdf-data\n    # insert: https://github.com/blazegraph/database/wiki/REST_API#insert\n   #url = f\"{self.baseurl}/namespace/{self.namespace}{self.sparql}\"\n   #could call insure final slash\n    url = f\"{self.baseurl}/namespace/{self.namespace}/{self.sparql}\"\n    log.info(f'insert to {url} ')\n    headers = {\"Content-Type\": f\"{content_type}\"}\n    r = requests.post(url,data=data, headers=headers)\n    log.debug(f' status:{r.status_code}') #status:404\n    log.info(f' status:{r.status_code}') #status:404\n    if r.status_code == 200:\n        # '&lt;?xml version=\"1.0\"?&gt;&lt;data modified=\"0\" milliseconds=\"7\"/&gt;'\n        if 'data modified=\"0\"'  in r.text:\n            raise Exception(\"No Data Added: \" + r.text)\n        return True\n    else:\n        return False</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageBlazegraph.upload_file","title":"<code>upload_file(filename, content_type='text/x-nquads')</code>","text":"<p>to temp namespace or final one if given</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>def upload_file(self, filename, content_type=\"text/x-nquads\"):\n    \"to temp namespace or final one if given\"\n    log.debug(f'upload_file:{filename}')\n    log.info(f'upload_file:{filename}')\n    #open file and insert data\n    data = open(filename, 'rb').read()\n    log.debug(f'insert:{filename}')\n    log.info(f'insert:{filename}')\n    self.insert(data, content_type)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageBlazegraph.upload_nq_file","title":"<code>upload_nq_file(fn=None)</code>","text":"<p>will default to ns.nq</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>def upload_nq_file(self, fn=None):\n    \"will default to ns.nq\"\n    if fn:\n        filename=fn\n    else:\n        filename=self.namespace + \".nq\"\n    self.upload_file(filename)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageBlazegraph.upload_ttl_file","title":"<code>upload_ttl_file(fn=None)</code>","text":"<p>will default to ns.ttl</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>def upload_ttl_file(self, fn=None):\n    \"will default to ns.ttl\"\n    if fn: #will want to upload ns=repo.ttl to ns=summary in the end\n        filename=fn\n    else:\n        filename=self.namespace + \".ttl\"\n    self.upload_file(filename, 'Content-Type:text/x-turtle')</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageGraph","title":"<code>ManageGraph</code>","text":"<p>Abstract class for managing a Graph Store instance for a single namespace</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>class ManageGraph: #really a manage graph namespace, bc a graph has several of them, &amp;this represents only one\n    \"\"\" Abstract class for managing a Graph Store\n     instance for a single namespace\n    \"\"\"\n    baseurl = \"http://localhost:3030\" # basically fuskei\n    namespace = \"temp_summary\"\n    path = \"namespace\"\n    sparql = \"/sparql\" # blazegraph uses sparql after namespace, but let's not assume this\n\n    def __init__(self, graphurl: str, namespace :str) :\n        \"\"\"initialize a class for a namespace\"\"\"\n        self.baseurl = graphurl\n        self.namespace = namespace\n    def graphFromEndpoint(endpoint: str) -&gt; str:\n        paths = endpoint.split('/')\n        paths = paths[0:len(paths) -3]\n        newurl = '/'.join(paths)\n        return newurl\n    def createNamespace(self, quads=True):\n        \"\"\" create a new namespace\"\"\"\n        pass\n\n    def deleteNamespace(self):\n        \"\"\"delete a namespace\"\"\"\n        pass</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageGraph.__init__","title":"<code>__init__(graphurl, namespace)</code>","text":"<p>initialize a class for a namespace</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>def __init__(self, graphurl: str, namespace :str) :\n    \"\"\"initialize a class for a namespace\"\"\"\n    self.baseurl = graphurl\n    self.namespace = namespace</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageGraph.createNamespace","title":"<code>createNamespace(quads=True)</code>","text":"<p>create a new namespace</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>def createNamespace(self, quads=True):\n    \"\"\" create a new namespace\"\"\"\n    pass</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.manageGraph.ManageGraph.deleteNamespace","title":"<code>deleteNamespace()</code>","text":"<p>delete a namespace</p> Source code in <code>earthcube_utilities/src/ec/graph/manageGraph.py</code> <pre><code>def deleteNamespace(self):\n    \"\"\"delete a namespace\"\"\"\n    pass</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.sparql_query","title":"<code>sparql_query</code>","text":"<p>This basically wraps sparqldataframe, and contains a way to get resources that are the sparql queries, and few helpers to basic queries</p>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.sparql_query._getSparqlFileFromResources","title":"<code>_getSparqlFileFromResources(filename)</code>","text":"<p>retrieves sparql file from the sparql_files folder when in a package</p> Source code in <code>earthcube_utilities/src/ec/graph/sparql_query.py</code> <pre><code>def _getSparqlFileFromResources(filename) -&gt; str:\n    \"\"\" retrieves sparql file from the sparql_files folder when in a package\"\"\"\n    resourcename = f\"{filename}.sparql\"\n    resource = pkg_resources.read_text(sparqlfiles, resourcename)\n    return resource</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.sparql_query.getAGraph","title":"<code>getAGraph(g, endpoint)</code>","text":"<p>Query a SPARQL endpoint and return a Pandas Dataframe for a geocodes object</p> Source code in <code>earthcube_utilities/src/ec/graph/sparql_query.py</code> <pre><code>def getAGraph(  g, endpoint: str) -&gt; pandas.DataFrame:\n    \"\"\"Query a SPARQL endpoint and return a Pandas Dataframe for a geocodes object\"\"\"\n    query = _getSparqlFileFromResources('urn_triples_for_a_graph')\n    q_template = Template(query)\n    thsGraphQuery = q_template.substitute(urn=g)\n    g_df = sparqldataframe.query(endpoint, thsGraphQuery)\n\n    return g_df</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.sparql_query.listSparqlFilesFromResources","title":"<code>listSparqlFilesFromResources()</code>","text":"<p>retrieves sparql file from the sparql_files folder when in a package</p> Source code in <code>earthcube_utilities/src/ec/graph/sparql_query.py</code> <pre><code>def listSparqlFilesFromResources() -&gt; str:\n    \"\"\" retrieves sparql file from the sparql_files folder when in a package\"\"\"\n    resource = pkg_resources.contents(sparqlfiles)\n    files = filter( lambda f: ends_with(f, \".sparql\"), resource)\n    files = map(lambda f: replace_end(f,\".sparql\",\"\"), files)\n    files = sort(list(files))\n    return files</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.graph.sparql_query.queryWithSparql","title":"<code>queryWithSparql(template_name, endpoint, parameters={})</code>","text":"<p>Query a SPARQL endpoint, and return a Pandas Dataframe</p> <p>Parameters:</p> Name Type Description Default <code>template_name</code> <code>str</code> <p>name of templates in the ec.graph.sparql_files directory</p> required <code>endpoint</code> <code>str</code> <p>SPARQL endpoint url</p> required <code>parameters</code> <code>object</code> <p>object with the names to fill in template eg {\"repo\": \"reponame\"}</p> <code>{}</code> Source code in <code>earthcube_utilities/src/ec/graph/sparql_query.py</code> <pre><code>def queryWithSparql( template_name : str, endpoint : str,parameters:object={}) -&gt; pandas.DataFrame:\n    \"\"\" Query a SPARQL endpoint, and return a Pandas Dataframe\n\n    Parameters:\n       template_name: name of templates in the ec.graph.sparql_files directory\n       endpoint: SPARQL endpoint url\n       parameters: object with the names to fill in template eg {\"repo\": \"reponame\"}\n    \"\"\"\n    query = _getSparqlFileFromResources(f\"{template_name}\")\n    q_template = Template(query)\n    thsGraphQuery = q_template.substitute(parameters)\n    q_df = sparqldataframe.query(endpoint, thsGraphQuery)\n    return q_df</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.logger","title":"<code>logger</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.logger.log","title":"<code>log</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.logger.log.config_app","title":"<code>config_app()</code>","text":"<p>Use this to set default logging behavior while in a console application</p> Source code in <code>earthcube_utilities/src/ec/logger/log.py</code> <pre><code>def config_app() -&gt; Logger:\n    \"\"\"Use this to set default logging behavior while in a console application\"\"\"\n    logging.basicConfig(format='%(levelname)s : %(message)s', level=os.environ.get(\"LOGLEVEL\", \"INFO\"),\n                        stream=sys.stdout)\n    log = logging.getLogger()\n    return log</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.logger.log.config_notebook","title":"<code>config_notebook()</code>","text":"<p>Use this to get default logging behavior while in a notebook</p> Source code in <code>earthcube_utilities/src/ec/logger/log.py</code> <pre><code>def config_notebook() -&gt; Logger:\n    \"\"\"Use this to get default logging behavior while in a notebook\"\"\"\n    logging.basicConfig(format='%(levelname)s : %(message)s', level=os.environ.get(\"LOGLEVEL\", \"INFO\"),\n                        stream=sys.stdout)\n    log = logging.getLogger()\n    return log</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.logger.log.config_web","title":"<code>config_web()</code>","text":"<p>Use this to set default logging behavior a web applicaiton like Flask</p> Source code in <code>earthcube_utilities/src/ec/logger/log.py</code> <pre><code>def config_web() -&gt; Logger:\n    \"\"\"Use this to set default logging behavior a web applicaiton like Flask\"\"\"\n    logging.basicConfig(format='%(levelname)s : %(message)s', level=os.environ.get(\"LOGLEVEL\", \"INFO\"),\n                        stream=sys.stdout)\n    log = logging.getLogger()\n    return log</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.missing_report","title":"<code>missing_report</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.missing_report.start","title":"<code>start()</code>","text":"<p>Run the write_missing_report program. Get a list of active repositories from the gleaner file. For each repository, generate missing reports and write these information to a json file and upload it to s3. Arguments:     args: Arguments passed from the command line. Returns:     An exit code.</p> Source code in <code>earthcube_utilities/src/ec/missing_report.py</code> <pre><code>def start():\n    \"\"\"\n        Run the write_missing_report program.\n        Get a list of active repositories from the gleaner file.\n        For each repository, generate missing reports and write these information to a json file and upload it to s3.\n        Arguments:\n            args: Arguments passed from the command line.\n        Returns:\n            An exit code.\n    \"\"\"\n    exitcode = writeMissingReport()\n    sys.exit(exitcode)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook","title":"<code>notebook</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.get_from_geocodes","title":"<code>get_from_geocodes(urn, endpoint='https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql')</code>","text":"<p>get an object from the graph, and return as a JSON-LD file</p> Source code in <code>earthcube_utilities/src/ec/notebook/geocodes.py</code> <pre><code>def get_from_geocodes(urn, endpoint=\"https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\"):\n    \"\"\"get an object from the graph, and return as a JSON-LD file \"\"\"\n    manager = EcObjectManager(endpoint, None, None)\n    return manager.getFromStore(urn, source='graph', form='jsonld')</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.get_jsfile2dict","title":"<code>get_jsfile2dict(fn)</code>","text":"<p>get jsonfile as dict</p> Source code in <code>earthcube_utilities/src/ec/notebook/utils.py</code> <pre><code>def get_jsfile2dict(fn):\n    \"get jsonfile as dict\"\n    #s=get_txtfile(fn)\n    #return json.loads(s)\n    with open(fn, \"r\") as f:\n        return json.load(f)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.get_txtfile","title":"<code>get_txtfile(fn)</code>","text":"<p>ret str from file</p> Source code in <code>earthcube_utilities/src/ec/notebook/utils.py</code> <pre><code>def get_txtfile(fn):\n    \"ret str from file\"\n    with open(fn, \"r\") as f:\n        return f.read()</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.inCollab","title":"<code>inCollab()</code>","text":"<p>Is this notebook being run in Goolge Collab</p> Source code in <code>earthcube_utilities/src/ec/notebook/utils.py</code> <pre><code>def inCollab():\n    \"\"\" Is this notebook being run in Goolge Collab\"\"\"\n    return  'google.colab' in sys.modules</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.put_txtfile","title":"<code>put_txtfile(fn, s, wa='w')</code>","text":"<p>filename to dump string to</p> Source code in <code>earthcube_utilities/src/ec/notebook/utils.py</code> <pre><code>def put_txtfile(fn,s,wa=\"w\"):\n    \"filename to dump string to\"\n    #with open(fn, \"w\") as f:\n    with open(fn, \"a\") as f:\n        return f.write(s)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.geocodes","title":"<code>geocodes</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.geocodes.get_from_geocodes","title":"<code>get_from_geocodes(urn, endpoint='https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql')</code>","text":"<p>get an object from the graph, and return as a JSON-LD file</p> Source code in <code>earthcube_utilities/src/ec/notebook/geocodes.py</code> <pre><code>def get_from_geocodes(urn, endpoint=\"https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\"):\n    \"\"\"get an object from the graph, and return as a JSON-LD file \"\"\"\n    manager = EcObjectManager(endpoint, None, None)\n    return manager.getFromStore(urn, source='graph', form='jsonld')</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.utils","title":"<code>utils</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.utils.get_jsfile2dict","title":"<code>get_jsfile2dict(fn)</code>","text":"<p>get jsonfile as dict</p> Source code in <code>earthcube_utilities/src/ec/notebook/utils.py</code> <pre><code>def get_jsfile2dict(fn):\n    \"get jsonfile as dict\"\n    #s=get_txtfile(fn)\n    #return json.loads(s)\n    with open(fn, \"r\") as f:\n        return json.load(f)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.utils.get_txtfile","title":"<code>get_txtfile(fn)</code>","text":"<p>ret str from file</p> Source code in <code>earthcube_utilities/src/ec/notebook/utils.py</code> <pre><code>def get_txtfile(fn):\n    \"ret str from file\"\n    with open(fn, \"r\") as f:\n        return f.read()</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.utils.inCollab","title":"<code>inCollab()</code>","text":"<p>Is this notebook being run in Goolge Collab</p> Source code in <code>earthcube_utilities/src/ec/notebook/utils.py</code> <pre><code>def inCollab():\n    \"\"\" Is this notebook being run in Goolge Collab\"\"\"\n    return  'google.colab' in sys.modules</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.notebook.utils.put_txtfile","title":"<code>put_txtfile(fn, s, wa='w')</code>","text":"<p>filename to dump string to</p> Source code in <code>earthcube_utilities/src/ec/notebook/utils.py</code> <pre><code>def put_txtfile(fn,s,wa=\"w\"):\n    \"filename to dump string to\"\n    #with open(fn, \"w\") as f:\n    with open(fn, \"a\") as f:\n        return f.write(s)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.reporting","title":"<code>reporting</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.reporting.report","title":"<code>report</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.reporting.report.compareSummoned2Graph","title":"<code>compareSummoned2Graph(bucket, repo, datastore, graphendpoint)</code>","text":"<p>return list of missing . we do not alway generate a milled.</p> Source code in <code>earthcube_utilities/src/ec/reporting/report.py</code> <pre><code>def compareSummoned2Graph(bucket, repo, datastore: bucketDatastore, graphendpoint):\n    \"\"\" return list of missing .\n    we do not alway generate a milled.\n    \"\"\"\n    # compare using s3, listJsonld(bucket, repo) to queryWithSparql(\"repo_select_graphs\", release_file)\n    summoned_list = datastore.listSummonedSha(bucket, repo)\n    graph_urns = ec.graph.sparql_query.queryWithSparql(\"repo_select_graphs\",graphendpoint,{\"repo\":repo})\n    graph_shas = list(map(lambda u: pydash.strings.substr_right_end(u, \":\"), graph_urns['g']))\n    difference = pydash.arrays.difference(summoned_list, graph_shas)\n    return {\n        \"summoned_count\": pydash.collections.size(summoned_list),\n        \"milled_count\": pydash.collections.size(graph_shas),\n            \"missing\": difference\n            }</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.reporting.report.compareSummoned2Milled","title":"<code>compareSummoned2Milled(bucket, repo, datastore)</code>","text":"<p>return list of missing urns/urls Generating milled will be good to catch such errors</p> Source code in <code>earthcube_utilities/src/ec/reporting/report.py</code> <pre><code>def compareSummoned2Milled(bucket, repo, datastore: bucketDatastore):\n    \"\"\" return list of missing urns/urls\n    Generating milled will be good to catch such errors\"\"\"\n    # compare using s3, listJsonld(bucket, repo) to  listMilledRdf(bucket, repo)\n    summoned_list = datastore.listSummonedSha(bucket, repo)\n    milled_list = datastore.listMilledSha(bucket, repo)\n    difference = pydash.arrays.difference(summoned_list, milled_list)\n    return {\n        \"summoned_count\": pydash.collections.size(summoned_list),\n        \"milled_count\": pydash.collections.size(milled_list),\n            \"missing\": difference\n            }</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.reporting.report.getGraphReportsLatestRepoReports","title":"<code>getGraphReportsLatestRepoReports(repo, datastore)</code>","text":"<p>get the latest for a dashboard</p> Source code in <code>earthcube_utilities/src/ec/reporting/report.py</code> <pre><code>def getGraphReportsLatestRepoReports(repo,  datastore: bucketDatastore):\n    \"\"\"get the latest for a dashboard\"\"\"\n    date=\"latest\"\n    path = f\"{datastore.paths['reports']}/{repo}/{date}/sparql.json\"\n    filelist = datastore.getReportFile(datastore.default_bucket, repo, path)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.reporting.report.get_url_from_sha_list","title":"<code>get_url_from_sha_list(shas, bucket, repo, datastore)</code>","text":"<p>if a repo has issues with milling or loading to a graph, then get the summoned file</p> Source code in <code>earthcube_utilities/src/ec/reporting/report.py</code> <pre><code>def get_url_from_sha_list(shas: list,  bucket, repo, datastore: bucketDatastore):\n    \"\"\"if a repo has issues with milling or loading to a graph, then get the summoned file\"\"\"\n\n    pass</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.reporting.report.listGraphReportDates4Repo","title":"<code>listGraphReportDates4Repo(repo, datastore)</code>","text":"<p>get the latest for a dashboard</p> Source code in <code>earthcube_utilities/src/ec/reporting/report.py</code> <pre><code>def listGraphReportDates4Repo(repo,  datastore: bucketDatastore):\n    \"\"\"get the latest for a dashboard\"\"\"\n    path = f\"{datastore.paths['reports']}/{repo}/\"\n    filelist = datastore.listPath(path)\n    return filelist</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sitemap","title":"<code>sitemap</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sitemap.sitemap","title":"<code>sitemap</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sitemap.sitemap.GleanerioSourceSitemap","title":"<code>GleanerioSourceSitemap</code>","text":"<p>               Bases: <code>Sitemap</code></p> <p>For a provided GleanerIO source record, create a Sitemap object to utilize</p> Source code in <code>earthcube_utilities/src/ec/sitemap/sitemap.py</code> <pre><code>class GleanerioSourceSitemap(Sitemap):\n    \"\"\" For a provided GleanerIO source record, create a Sitemap object to utilize\"\"\"\n    source= None\n\n    def __init__(self, source):\n        self.self.source = source\n        if source[\"sourcetype\"] !=  \"sitemap\":\n            return Exception(\"source is Not a sitemap\")\n        super.__init__(source.url, reponame= source.name)\n\n    def get_status(self):\n        s = self.source\n        return  { 'name': s.name , 'code': _urlResponse( s.get(\"url\") ), 'description': \"res\", 'url': s.get(\"description\"), 'type': s.get(\"sourcetype\")}</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sitemap.sitemap.Sitemap","title":"<code>Sitemap</code>","text":"<p>This holds information about a sitemap, and allows for testing of the URLS in the sitemap to see if they exist. If the sitemap URL does not exist, then it will return a Sitemap, with an len(errors) &gt; 0.</p> Source code in <code>earthcube_utilities/src/ec/sitemap/sitemap.py</code> <pre><code>class Sitemap():\n    \"\"\" This holds information about a sitemap, and allows for testing of the\n    URLS in the sitemap to see if they exist.\n    If the sitemap URL does not exist, then it will return a Sitemap, with an\n    len(errors) &gt; 0.\n\n    \"\"\"\n    sitemapurl = None\n    _validSitemap = True\n    sitemap_df = pandas.DataFrame()\n    _checkedUrls=False\n    def __init__(self, sitemapurl, repoName=\"\", no_progress_bar=False):\n        self.errors=[]\n        self.sitemapurl = sitemapurl\n        self.no_progress_bar = no_progress_bar\n        if _urlExists(sitemapurl):\n            self.sitemap_df = adv.sitemap_to_df(sitemapurl)\n            self._validSitemap = True\n        else:\n            self.errors.append(f\"sitemap url invalid: {sitemapurl}\")\n            self._validSitemap = False\n            # the other option is to return None.\n\n    def validUrl(self) -&gt; bool:\n        \"\"\" does the provided sitemap URL exist.\"\"\"\n        return self._validSitemap\n\n    def errors(self):\n        return self.errors\n    def uniqueItems(self):\n        \"\"\"list of unqiue sitemaps records\"\"\"\n        return self.sitemap_df.sitemap.unique().tolist()\n\n    def uniqueUrls(self) :\n        \"\"\"Returns a pandas series of the URLS'\"\"\"\n        return self.sitemap_df[\"loc\"].unique().tolist()\n\n    def check_urls(self) -&gt; DataFrame:\n        \"\"\"This will run head on the list of url's in the pandas dataframe.\n        It will append columns (\"url_response\",\"content_type\") to the sitemap_df\n        \"\"\"\n\n        # add columns to the dataframe. clear out any previous values\n        df = self.sitemap_df\n        df[\"url_response\"]=None\n        df[\"content_type\"]=None\n\n        if not self._validSitemap:\n            self._checkedUrls = True\n            return df\n\n        pandarallel.initialize(progress_bar=self.no_progress_bar)\n        df[\"url_response\"],df[\"content_type\"]=  zip(*df.parallel_apply(lambda row:\n                           _urlResponse(row.get('loc')),\n                           axis=1))\n        self._checkedUrls =True\n        return df\n\n    def get_url_report(self) -&gt; str:\n        \"\"\"returns a csv of the 'loc','lastmod','url_response', 'content_type' \"\"\"\n        if not self._checkedUrls:\n            self.check_urls()\n        out= StringIO()\n        self.sitemap_df.to_csv(out, index=False, quoting=csv.QUOTE_ALL,\n                columns=['loc','lastmod','url_response', 'content_type']\n              #                 columns=['loc','lastmod',\"url_response/content_type\"]\n                               )\n        return out.getvalue()</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sitemap.sitemap.Sitemap.get_url_report","title":"<code>get_url_report()</code>","text":"<p>returns a csv of the 'loc','lastmod','url_response', 'content_type'</p> Source code in <code>earthcube_utilities/src/ec/sitemap/sitemap.py</code> <pre><code>def get_url_report(self) -&gt; str:\n    \"\"\"returns a csv of the 'loc','lastmod','url_response', 'content_type' \"\"\"\n    if not self._checkedUrls:\n        self.check_urls()\n    out= StringIO()\n    self.sitemap_df.to_csv(out, index=False, quoting=csv.QUOTE_ALL,\n            columns=['loc','lastmod','url_response', 'content_type']\n          #                 columns=['loc','lastmod',\"url_response/content_type\"]\n                           )\n    return out.getvalue()</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sitemap.sitemap.Sitemap.uniqueItems","title":"<code>uniqueItems()</code>","text":"<p>list of unqiue sitemaps records</p> Source code in <code>earthcube_utilities/src/ec/sitemap/sitemap.py</code> <pre><code>def uniqueItems(self):\n    \"\"\"list of unqiue sitemaps records\"\"\"\n    return self.sitemap_df.sitemap.unique().tolist()</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sitemap.sitemap.Sitemap.uniqueUrls","title":"<code>uniqueUrls()</code>","text":"<p>Returns a pandas series of the URLS'</p> Source code in <code>earthcube_utilities/src/ec/sitemap/sitemap.py</code> <pre><code>def uniqueUrls(self) :\n    \"\"\"Returns a pandas series of the URLS'\"\"\"\n    return self.sitemap_df[\"loc\"].unique().tolist()</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sitemap.sitemap.Sitemap.validUrl","title":"<code>validUrl()</code>","text":"<p>does the provided sitemap URL exist.</p> Source code in <code>earthcube_utilities/src/ec/sitemap/sitemap.py</code> <pre><code>def validUrl(self) -&gt; bool:\n    \"\"\" does the provided sitemap URL exist.\"\"\"\n    return self._validSitemap</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sitemap.sitemap._urlResponse","title":"<code>_urlResponse(item_loc)</code>","text":"<p>return response code, content type,</p> Source code in <code>earthcube_utilities/src/ec/sitemap/sitemap.py</code> <pre><code>def _urlResponse(item_loc: str):\n    \"\"\"return response code, content type, \"\"\"\n    try:\n        r = requests.head(item_loc)\n        if r.status_code != 200:\n            return [r.status_code, None]\n        else:\n            content_type = r.headers.get(\"Content-Type\")\n            return [r.status_code, content_type]\n    except:\n        return [400, None]</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json","title":"<code>sos_json</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.rdf","title":"<code>rdf</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.rdf.createRDFNode","title":"<code>createRDFNode(nodeValue)</code>","text":"<p>fix_url and quote otherwise</p> Source code in <code>earthcube_utilities/src/ec/sos_json/rdf.py</code> <pre><code>def createRDFNode(nodeValue: str) -&gt; Union[Literal, BNode ,URIRef]:\n    \"fix_url and quote otherwise\"\n    if not isinstance(nodeValue,str):\n        if  (nodeValue is None) or  (pandas.isnull(nodeValue)):\n            return Literal(\"\")\n        return Literal(nodeValue)\n    else:\n        if nodeValue.startswith(\"&lt;ht\"):\n            return URIRef(nodeValue)\n        elif nodeValue.startswith(\"_:B\"):\n            return BNode(nodeValue.replace(\"_:B\", \"B\"))\n        elif nodeValue.startswith(\"t1\"):\n            return BNode(nodeValue.replace(\"t1\", \"Bt1\"))\n        elif is_http(nodeValue):\n            return URIRef(nodeValue)\n        elif nodeValue.startswith(\"doi:\"):\n            return URIRef(nodeValue)\n        elif nodeValue.startswith(\"DOI:\"):\n            return URIRef(nodeValue)\n        #elif obj:\n        elif nodeValue is None:\n            return Literal(\"\")\n        elif pandas.isnull(nodeValue):\n            return Literal(\"\")\n        else:\n            # import json\n            # return json.dumps(url)\n           return Literal(nodeValue)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.rdf.df2rdfgraph","title":"<code>df2rdfgraph(df)</code>","text":"<p>print out df as .nt file</p> Source code in <code>earthcube_utilities/src/ec/sos_json/rdf.py</code> <pre><code>def df2rdfgraph(df: pandas.DataFrame):\n    \"print out df as .nt file\"\n\n    g = Graph()\n    g.bind(\"schema\", \"https://schema.org/\")\n    for index, row in df.iterrows():\n        s=df[\"s\"][index]\n        s=createRDFNode(s)\n        p=df[\"p\"][index]\n        p=createRDFNode(p)\n        o=df[\"o\"][index]\n        o=createRDFNode(o)\n        g.add((s, p, o))\n\n        #need to finish up w/dumping to a file\n    return  g</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.rdf.get_graph2jsonld","title":"<code>get_graph2jsonld(urn, endpoint, form='compact', schemaType='Dataset')</code>","text":"<p>get jsonld from endpoint</p> <p>Parameters:</p> Name Type Description Default <code>urn</code> <code>str</code> <p>?g from sparql query. URN of the graph eg. urn:gleaner.io:earthcube:geocodes_demo_datasets:257108e0760f96ef7a480e1d357bcf8720cd11e4</p> required <code>form</code> <p>jsonld| compact, frame</p> <code>'compact'</code> <code>schemaType</code> <p>if form=frame then this type is passed to the frame</p> <code>'Dataset'</code> <code>endpoint</code> <code>str</code> <p>sparql endpoint</p> required Source code in <code>earthcube_utilities/src/ec/sos_json/rdf.py</code> <pre><code>def get_graph2jsonld(urn: str, endpoint:str, form=\"compact\", schemaType=\"Dataset\") -&gt; str:\n    \"\"\"get jsonld from endpoint\n\n    Parameters:\n        urn: ?g from sparql query. URN of the graph eg. urn:gleaner.io:earthcube:geocodes_demo_datasets:257108e0760f96ef7a480e1d357bcf8720cd11e4\n        form: jsonld| compact, frame\n        schemaType: if form=frame then this type is passed to the frame\n        endpoint: sparql endpoint\n\n    \"\"\"\n    g = get_rdfgraph(urn, endpoint)\n\n    return graph2jsonld(g, form=form, schemaType=schemaType)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.rdf.get_rdf2jld_str","title":"<code>get_rdf2jld_str(urn, endpoint)</code>","text":"<p>get jsonld from endpoint</p> Source code in <code>earthcube_utilities/src/ec/sos_json/rdf.py</code> <pre><code>def get_rdf2jld_str(urn: str, endpoint:str) -&gt; str:\n    \"get jsonld from endpoint\"\n    g= get_rdfgraph(urn, endpoint)\n    jld_str = g.serialize(format=\"json-ld\")\n    return compact_jld_str(jld_str)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.rdf.graph2jsonld","title":"<code>graph2jsonld(g, form='jsonld', schemaType='Dataset')</code>","text":"<p>get jsonld from endpoint</p> <p>Parameters:</p> Name Type Description Default <code>g</code> <p>?g from sparql query. URN of the graph eg. urn:gleaner.io:earthcube:geocodes_demo_datasets:257108e0760f96ef7a480e1d357bcf8720cd11e4</p> required <code>form</code> <p>jsonld| compact, frame</p> <code>'jsonld'</code> <code>schemaType</code> <p>if form=frame then this type is passed to the frame</p> <code>'Dataset'</code> Source code in <code>earthcube_utilities/src/ec/sos_json/rdf.py</code> <pre><code>def graph2jsonld(g, form=\"jsonld\", schemaType=\"Dataset\") -&gt; str:\n    \"\"\"get jsonld from endpoint\n\n    Parameters:\n        g: ?g from sparql query. URN of the graph eg. urn:gleaner.io:earthcube:geocodes_demo_datasets:257108e0760f96ef7a480e1d357bcf8720cd11e4\n        form: jsonld| compact, frame\n        schemaType: if form=frame then this type is passed to the frame\n    \"\"\"\n    # auto_compact=False might change\n    jld_str = g.serialize(format=\"json-ld\")\n\n    return formatted_jsonld(jld_str, form=form, schemaType=schemaType)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.rdf.load_release","title":"<code>load_release(releaseurl)</code>","text":"<p>retrieves a release from the url</p> Source code in <code>earthcube_utilities/src/ec/sos_json/rdf.py</code> <pre><code>def load_release(releaseurl:str) -&gt; Graph:\n    \"\"\"retrieves a release from the url\"\"\"\n    g= Dataset()\n    g.parse(releaseurl, format='nquads')\n    return g</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.utils","title":"<code>utils</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.utils._getSchemaFromResources","title":"<code>_getSchemaFromResources(filename)</code>","text":"<p>retrieves json schema file from the  ec.sos_json.schemas directory when in a package</p> Source code in <code>earthcube_utilities/src/ec/sos_json/utils.py</code> <pre><code>def _getSchemaFromResources(filename) :\n    \"\"\" retrieves json schema file from the  ec.sos_json.schemas directory when in a package\"\"\"\n    resourcename = f\"{filename}.json\"\n    resource = pkg_resources.read_text(schemafiles, resourcename)\n    schema = json.loads(resource)\n    return schema</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.utils.compact_jld_str","title":"<code>compact_jld_str(jld_str)</code>","text":"<p>depreciated use formatted_jsonld</p> Source code in <code>earthcube_utilities/src/ec/sos_json/utils.py</code> <pre><code>def compact_jld_str(jld_str: str) -&gt; str:\n    \"\"\"\n       depreciated use formatted_jsonld\n\n    \"\"\"\n    doc = json.loads(jld_str)\n    compacted = jsonld.compact(doc, jsonld_context)\n    r = json.dumps(compacted, indent=2)\n    return r</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.utils.formatted_jsonld","title":"<code>formatted_jsonld(jld_str, form='compact', schemaType='Dataset')</code>","text":"<p>returns a formatted JSONLD string based on the input  Parameters:      jld_str: jsonld string returns the string      form: 'compact'--compact form,             'frome'--framed usngin schemaType,             'jsonld' - passed form      schemaType: type of the schema default=\"Dataset\"  Returns:      string.</p> Source code in <code>earthcube_utilities/src/ec/sos_json/utils.py</code> <pre><code>def formatted_jsonld(jld_str: str, form=\"compact\", schemaType=\"Dataset\") -&gt; str:\n    \"\"\"\n       returns a formatted JSONLD string based on the input\n        Parameters:\n            jld_str: jsonld string returns the string\n            form: 'compact'--compact form,\n                   'frome'--framed usngin schemaType,\n                   'jsonld' - passed form\n            schemaType: type of the schema default=\"Dataset\"\n        Returns:\n            string.\n\n    \"\"\"\n    if (form == 'jsonld'):\n        return jld_str\n\n    elif (form == \"frame\"):\n        frame = (' {\\n'\n                 '              \"@context\": {\\n'\n                 '                \"@vocab\": \"https://schema.org/\",\\n'\n                 '                    \"rdf\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\\n'\n                 '                    \"rdfs\": \"http://www.w3.org/2000/01/rdf-schema#\",\\n'\n                 '                    \"schema\": \"https://schema.org/\",\\n'\n                 '                    \"xsd\": \"http://www.w3.org/2001/XMLSchema#\"\\n'\n                 '              },\\n'\n                 '              \"@type\": \"schema:${schemaType}\"\\n'\n                 '  }\\n '\n                 )\n        f_template = Template(frame)\n        thsGraphQuery = f_template.substitute(schemaType=schemaType)\n\n        frame_doc = json.loads(thsGraphQuery)\n        doc = json.loads(jld_str)\n #       doc = jsonld.flatten(doc)\n        # issue where @id on Dataset and @id on Dataset.identifer were the same. If so\n        # @type = ['Dataset', 'PropertyValue'] was the conversion.\n\n        framed = jsonld.frame(doc, frame_doc)\n\n        r = json.dumps(framed, indent=2)\n        return r\n    else:  # compact\n        doc = json.loads(jld_str)\n        compacted = jsonld.compact(doc, jsonld_context)\n        r = json.dumps(compacted, indent=2)\n        return r</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.utils.isValidJSON","title":"<code>isValidJSON(jsonData)</code>","text":"<p>Is Json Valid</p> Source code in <code>earthcube_utilities/src/ec/sos_json/utils.py</code> <pre><code>def isValidJSON(jsonData: str)-&gt; bool:\n    \"\"\"Is Json Valid\"\"\"\n    try:\n        json.loads(jsonData)\n    except ValueError as err:\n        return False\n    return True</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.utils.listSchemaFilesFromResources","title":"<code>listSchemaFilesFromResources()</code>","text":"<p>retrieves schema file from the schemas folder when in a package</p> Source code in <code>earthcube_utilities/src/ec/sos_json/utils.py</code> <pre><code>def listSchemaFilesFromResources() -&gt; str:\n    \"\"\" retrieves schema file from the schemas folder when in a package\"\"\"\n    resource = pkg_resources.contents(schemafiles)\n    files = filter( lambda f: ends_with(f, \".json\"), resource)\n    files = sort(list(files))\n    return files</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.utils.validateEcrrTool","title":"<code>validateEcrrTool(jsonData)</code>","text":"<p>validated  Parameters:      jsonData: jsonld string returns the string      form: 'compact'--compact form,             'frome'--framed usngin schemaType,             'jsonld' - passed form      schemaType: type of the schema default=\"Dataset\"  Returns:      bool.</p> Source code in <code>earthcube_utilities/src/ec/sos_json/utils.py</code> <pre><code>def validateEcrrTool(jsonData : str) -&gt; bool:\n    \"\"\"\n       validated\n        Parameters:\n            jsonData: jsonld string returns the string\n            form: 'compact'--compact form,\n                   'frome'--framed usngin schemaType,\n                   'jsonld' - passed form\n            schemaType: type of the schema default=\"Dataset\"\n        Returns:\n            bool.\n\n    \"\"\"\n    try:\n        json_data = json.loads(jsonData)\n    except ValueError as err:\n        return False\n    valid2_schema, err = validateJson2Schema(jsonData, schemaname=\"GeoCodes-ECRR-DatasetSchema.json\")\n    return valid2_schema</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.utils.validateJson2Schema","title":"<code>validateJson2Schema(json_data, schemaname='GeoCodes-DatasetSchema')</code>","text":"<p>REF: https://json-schema.org/</p> <p>Parameters:</p> Name Type Description Default <code>json_data</code> <code>Any</code> <p>JSON Object (aka json.loads())</p> required <code>schemaname</code> <p>name of schem in ec.sos_json.shcemas: (GeoCodes-DatasetSchema or GeoCodes-ECRR-DatasetSchema.json)</p> <code>'GeoCodes-DatasetSchema'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>is valid</p> <code>message</code> <code>str</code> <p>\"Given JSON data is Valid\" or \"Given JSON data is InValid\"</p> Source code in <code>earthcube_utilities/src/ec/sos_json/utils.py</code> <pre><code>def validateJson2Schema(json_data: Any, schemaname='GeoCodes-DatasetSchema') -&gt; Tuple[bool,str]:\n    \"\"\"REF: https://json-schema.org/\n\n    Parameters:\n        json_data: JSON Object (aka json.loads())\n        schemaname: name of schem in ec.sos_json.shcemas: (GeoCodes-DatasetSchema or GeoCodes-ECRR-DatasetSchema.json)\n\n    Returns:\n        bool: is valid\n        message: \"Given JSON data is Valid\" or \"Given JSON data is InValid\"\n        \"\"\"\n    # Describe what kind of json you expect.\n    execute_api_schema = _getSchemaFromResources(schemaname)\n\n    try:\n        validate(instance=json_data, schema=execute_api_schema)\n    except ValidationError as err:\n        logging.error(err)\n        err = f\"Given JSON data is InValid {err}\"\n        return False, err\n\n    message = \"Given JSON data is Valid\"\n    return True, message</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.sos_json.utils.validateSosDataset","title":"<code>validateSosDataset(jsonData)</code>","text":"<p>validates a SOS Dataset</p> Source code in <code>earthcube_utilities/src/ec/sos_json/utils.py</code> <pre><code>def validateSosDataset(jsonData: str) -&gt; bool:\n    \"\"\" validates a SOS Dataset\"\"\"\n    try:\n        json_data = json.loads(jsonData)\n    except ValueError as err:\n        return False\n    valid2_schema, err = validateJson2Schema(jsonData)\n    return valid2_schema</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.summarize","title":"<code>summarize</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.summarize.summarize_materializedview","title":"<code>summarize_materializedview</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.summarize.summarize_materializedview.context","title":"<code>context = f'@prefix : &lt;{BASE_SHCEMA_ORG}&gt; .'</code>  <code>module-attribute</code>","text":"<p>original fetch all from temporary namespace</p>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.summarize.summarize_materializedview.summaryDF2ttl","title":"<code>summaryDF2ttl(df, repo, from_release=False)</code>","text":"<p>summarize sparql query returns turtle string and rdf lib Graph</p> Source code in <code>earthcube_utilities/src/ec/summarize/summarize_materializedview.py</code> <pre><code>def summaryDF2ttl(df: pandas.DataFrame, repo: str, from_release=False) -&gt; tuple[ Union[str,bytes], Graph]:\n    \"summarize sparql query returns turtle string and rdf lib Graph\"\n    urns = {}\n    def is_str(v):\n        return type(v) is str\n    g = Graph()\n    ## ##########\n    # Not officially a standard schema format.\n    # we might want to use our own namespace in the future\n    ###########\n    g.bind(\"ecsummary\", BASE_SHCEMA_ORG)\n    ecsummary = Namespace(BASE_SHCEMA_ORG)\n    sosschema = Namespace(BASE_SHCEMA_ORG)\n\n    for index, row in df.iterrows():\n        logging.debug(f'dbg:{row}')\n        gu=row[\"g\"]\n\n        graph_subject = URIRef(gu)\n        #skip the small %of dups, that even new get_summary.txt * has\n        if not urns.get(gu):\n            urns[gu]=1\n        else:\n            #print(f'already:{there},so would break loop')\n            continue #from loop\n\n\n        rt_=row.get('resourceType')\n        rt=rt_.replace(\"https://schema.org/\",\"\")\n        logging.debug(f'rt:{rt}')\n\n        name=json.dumps(row.get('name')) #check for NaN/fix\n        if not name:\n            name=f'\"\"'\n        if not is_str(name):\n            name=f'\"{name}\"'\n        if name==\"NaN\": #this works, but might use NA\n            name=f'\"{name}\"'\n# description\n        description=row['description']\n        if is_str(description):\n            sdes=json.dumps(description)\n            #sdes=description.replace(' / ',' \\/ ').replace('\"','\\\"')\n            #sdes=sdes.replace(' / ',' \\/ ').replace('\"','\\\"')\n          # sdes=sdes.replace('\"','\\\"')\n        else:\n            sdes=f'\"{description}\"'\n# keywords\n        kw_=row['kw']\n        if is_str(kw_):\n            kw=json.dumps(kw_)\n        else:\n            kw=f'\"{kw_}\"'\n# publisher\n        pubname=row['pubname']\n        if   pandas.isna(pubname) or  pubname==\"No Publisher\":\n            pubname = repo\n        #if no publisher urn.split(':')\n        #to use:repo in: ['urn', 'gleaner', 'summoned', 'opentopography', '58048498c7c26c7ab253519efc16df237866e8fe']\n        #as of the last runs, this was being done per repo, which comes in on the CLI, so could just use that too*\n\n\n# date\n        datep=row['datep']\n        if datep == \"No datePublished\":\n            datep=pandas.NA\n        # Query should not return \"No datePublished\" is not a valid Date \"YYYY-MM-DD\" so\n        # UI Date Select failed, because it expects an actual date\n        #   Empty values might be handled in the UI...,\n        #   or the repository valiation reporting\n\n\n        placename=row['placenames']\n\n\n        ##############\n        # output\n        # write to f StringIO()\n        # for RDF graph, using sub, verp object\n        ###############\n        s=row['subj']\n# RDF.TYPE\n\n#         if rt == \"tool\":\n#             g.add((graph_subject,RDF.type, sosschema.SoftwareApplication) )\n#         else:\n#             g.add((graph_subject, RDF.type, sosschema.Dataset))\n\n        # original aummary query wrote out strings, then converted back to schema uri... just skip that step\n        # RDF.TYPE\n        rt = row['sosType']\n        g.add((graph_subject, RDF.type, URIRef(rt)))\n\n# ecsummary.name\n        if (pandas.isnull( row.get('name'))):\n            g.add((graph_subject, ecsummary.name, Literal(\"\")))\n        else:\n            g.add( (graph_subject, ecsummary.name, Literal( row.get('name') ) ) )\n\n# ecsummary.description\n        g.add((graph_subject, ecsummary.description, Literal(description)))\n\n# ecsummary.keywords\n        g.add((graph_subject, ecsummary.keywords, Literal(kw_)))\n# ecsummary.publisher\n        if pandas.notna(pubname):\n            g.add((graph_subject, ecsummary.publisher, Literal(pubname)))\n# ecsummary.place\n        g.add((graph_subject, ecsummary.place, Literal(placename)))\n# ecsummary date\n        if pandas.notna(datep):\n            #might be: \"No datePublished\" ;should change in qry, for dv's lack of checking\n            # Query should not return \"No datePublished\" is not a valid Date \"YYYY-MM-DD\" so\n            # UI Date Select failed, because it expects an actual date\n            #   Empty values might be handled in the UI...,\n            #   or the repository valiation reporting\n            g.add((graph_subject, ecsummary.date, Literal(datep)))\n# ecsummary subjectOf\n        g.add((graph_subject, ecsummary.subjectOf, URIRef(s)))\n\n# ecsummary.distribution\n        du= row.get('url') # check now/not yet\n        if is_str(du):\n            g.add((graph_subject, ecsummary.distribution, URIRef(s)))\n# spatial\n\n# ecsummary.latitude\n        mlat= row.get('maxlat') # check now/not yet\n        if is_str(mlat):\n            g.add((graph_subject, ecsummary.latitude, Literal(mlat)))\n        mlon= row.get('maxlon') # check now/not yet\n        if is_str(mlon):\n            g.add((graph_subject, ecsummary.longitude, Literal(mlon)))\n\n# ecsummary.encodingFormat\n        encodingFormat= row.get('encodingFormat') # check now/not yet\n        if is_str(encodingFormat):\n            g.add((graph_subject, ecsummary.encodingFormat, Literal(encodingFormat)))\n        #see abt defaults from qry or here, think dv needs date as NA or blank/check\n        #old:\n        #got a bad:         :subjectOf &lt;metadata-doi:10.17882/42182&gt; .\n        #incl original subj, just in case for now\n        #lat/lon not in present ui, but in earlier version\n\n        #### end for ####\n    return g.serialize(format='longturtle'), g</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.summarize_from_graph_namespace","title":"<code>summarize_from_graph_namespace</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.summarize_from_graph_namespace.summarizeGraphOnly","title":"<code>summarizeGraphOnly()</code>","text":"<p>Summarize directly from a namespace, upload to provided summarize namespace</p> Description <ul> <li> <p>query for repository graphs using defined repo parameter,</p> </li> <li> <p>build summary triples</p> </li> <li> <p>loading to a summarized namespace</p> </li> </ul> Source code in <code>earthcube_utilities/src/ec/summarize_from_graph_namespace.py</code> <pre><code>def summarizeGraphOnly():\n    \"\"\" Summarize directly from a namespace, upload to provided summarize namespace\n\n    Description:\n        * query for repository graphs using defined repo parameter,\n\n        * build summary triples\n\n        * loading to a summarized namespace\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--repo\", dest='repo', help='repo name used in the  urn')\n\n    parser.add_argument('--graphendpoint', dest='graphendpoint',\n                        help='graph endpoint with namespace',\n                        default=\"https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\"\n                        , required=True)\n\n    parser.add_argument('--nographsummary', action='store_true', dest='nographsummary',\n                        help='send triples to file', default=False)\n    parser.add_argument('--summary_namespace', dest='summary_namespace',\n                        help='summary_namespace  just the namspace. defaults to \"repo_summary\"',\n                        )\n    args = parser.parse_args()\n    repo = args.repo\n\n    if args.summary_namespace:\n        if isValidURL(args.summary_namespace):\n            msg = 'For summary_namespace, Please enter the namespace only.'\n            print(msg)\n            logging.error(msg)\n            return 1\n        summary = args.summary_namespace\n    else:\n        summary = f\"{repo}_summary\"\n\n    endpoint= args.graphendpoint\n    graphendpoint = mg.graphFromEndpoint(endpoint)\n\n    try:\n\n        sumnsgraph = mg(graphendpoint, summary)\n\n        summaryendpoint =endpointUpdateNamespace(endpoint,summary)\n\n        if repo is not None:\n            summarydf = get_summary4repoSubset(endpoint, repo)\n        else:\n            # this really needs to be paged ;)\n            summarydf = get_summary4graph(endpoint)\n            repo = \"\"\n\n        nt,g = summaryDF2ttl(summarydf,repo) # let's try the new generator\n\n        summaryttl = g.serialize(format='longturtle')\n        # write to s3  in future\n        # with open(os.path.join(\"output\",f\"{repo}.ttl\"), 'w') as f:\n        #      f.write(summaryttl)\n        if not args.nographsummary:\n            inserted = sumnsgraph.insert(bytes(summaryttl, 'utf-8'),content_type=\"application/x-turtle\" )\n            if inserted:\n                logging.info(f\"Inserted into graph store{sumnsgraph.namespace}\" )\n            else:\n                logging.error(f\" dumping file {repo}.ttl  Repo {repo} not inserted into {sumnsgraph.namespace}\")\n                dumpToFile(repo, summaryttl)\n                return 1\n        else:\n            logging.info(f\" dumping file {repo}.ttl  nographsummary: {args.nographsummary} \")\n\n            dumpToFile(repo, summaryttl)\n    except Exception as ex:\n        logging.error(f\"error {ex}\")\n        print(f\"Error: {ex}\")\n        return 1</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.summarize_identifier_metadata","title":"<code>summarize_identifier_metadata</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.summarize_identifier_metadata.start","title":"<code>start()</code>","text":"<p>Run the summarize_identifier_metadata program. Get a list of active repositories from the gleaner file. For each repository, summarize identifierTypes and matchedPath, then write these information to a file and upload it to s3. Arguments:     args: Arguments passed from the command line. Returns:     An exit code.</p> Source code in <code>earthcube_utilities/src/ec/summarize_identifier_metadata.py</code> <pre><code>def start():\n    \"\"\"\n        Run the summarize_identifier_metadata program.\n        Get a list of active repositories from the gleaner file.\n        For each repository, summarize identifierTypes and matchedPath, then write these information to a file and upload it to s3.\n        Arguments:\n            args: Arguments passed from the command line.\n        Returns:\n            An exit code.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    # source of sources, and default s3 store.\n    #   at present, graph endpoint is no longer in gleaner\n    parser.add_argument('--cfgfile', dest='cfgfile',\n                        help='gleaner config file')\n    # no default for s3 parameters here. read from gleaner. if provided, these override the gleaner config\n    parser.add_argument('--s3', dest='s3server',\n                        help='s3 server address ')\n    parser.add_argument('--s3bucket', dest='s3bucket',\n                        help='s3 bucket ')\n    parser.add_argument('--no_upload', dest='no_upload', action='store_true', default=False,\n                        help='do not upload to s3 bucket ')\n    parser.add_argument('--output', default='identifier_metadata_summary.csv',\n                        dest='output', help='dump to file')\n    parser.add_argument('--json', dest='json', action='store_false', default=True,\n                        help='output json format')\n    parser.add_argument('--source', action='append', help=\"one or more repositories (--source a --source b)\")\n\n    args = parser.parse_args()\n    exitcode = summarizeIdentifierMetadata(args)\n    sys.exit(exitcode)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.summarize_repo","title":"<code>summarize_repo</code>","text":""},{"location":"earthcube_utilities/earthcube_utilities_code/#ec.summarize_repo.summarizeRepo","title":"<code>summarizeRepo()</code>","text":"<p>Summarize a repository using a temporary graph namespace</p> Description <ul> <li> <p>read nabu config,</p> </li> <li> <p>uploading to a graph namespace</p> </li> <li> <p>building summarize triples</p> </li> <li> <p>loading to a summarized namespace</p> </li> </ul> Source code in <code>earthcube_utilities/src/ec/summarize_repo.py</code> <pre><code>def summarizeRepo():\n    \"\"\" Summarize a repository using a temporary graph namespace\n\n    Description:\n        * read nabu config,\n\n        * uploading to a graph namespace\n\n        * building summarize triples\n\n        * loading to a summarized namespace\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"repo\", help='repository name')\n    parser.add_argument('nabufile', type=argparse.FileType('r'),\n                        help='nabu configuration file')\n    parser.add_argument('--graphendpoint', dest='graphendpoint',\n                        help='use this endpoint (full url:https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/sparql\"). overrides nabu endpoint')\n    parser.add_argument('--glcon', dest='glcon',\n                        help='override path to glcon', default=\"~/indexing/glcon\")\n    parser.add_argument('--nographsummary', action='store_true', dest='nographsummary',\n                        help='send triples to file', default=False)\n    parser.add_argument('--keeptemp', dest='graphtemp',\n                        help='do not delete the temp namespace. a namespace \"{repo}_temp\" will be created', default=True)\n    parser.add_argument('--summary_namespace', dest='summary_namespace',\n                        help='summary_namespace. just the namepsace defaults to \"{repo}_temp_summary\"')\n    args = parser.parse_args()\n\n    repo = args.repo\n    if args.summary_namespace:\n        if isValidURL(args.summary_namespace):\n            msg = 'For summary_namespace, Please enter the namespace only.'\n            print(msg)\n            logging.error(msg)\n            return 1\n        summary = args.summary_namespace\n    else:\n        summary = f\"{repo}__temp_summary\"\n    nabucfg = args.nabufile\n    endpoint, cfg = getNabuFromFile(nabucfg)\n    graphendpoint = mg.graphFromEndpoint(endpoint)\n    tempnsgraph = mg(graphendpoint, f'{repo}_temp')\n    try:  # temp has been created\n        created = tempnsgraph.createNamespace()\n        if ( created=='Failed'):\n            logging.fatal(\"coould not create namespace\")\n        sumnsgraph = mg(graphendpoint, summary)\n        created = sumnsgraph.createNamespace()\n        if ( created=='Failed'):\n            logging.fatal(\"coould not create summary namespace\")\n        # endpoints for file\n        tempendpoint =endpointUpdateNamespace(endpoint,f\"{repo}_temp\")\n        summaryendpoint =endpointUpdateNamespace(endpoint,summary)\n        newNabucfg = reviseNabuConfGraph(cfg, tempendpoint)\n        runNabu(newNabucfg,repo, args.glcon )\n\n        summarydf = get_summary4repo(tempendpoint)\n        nt,g = summaryDF2ttl(summarydf,repo) # let's try the new generator\n        summaryttl = g.serialize(format='longturtle')\n        # write to s3  in future\n        dumpToFile(repo, summaryttl)\n        if not args.nographsummary:\n            inserted = sumnsgraph.insert(bytes(summaryttl, 'utf-8'),content_type=\"application/x-turtle\" )\n            if inserted:\n                logging.info(f\"Inserted into graph store{sumnsgraph.namespace}\" )\n            else:\n                logging.error(f\" dumping file {repo}_dumped.ttl  Repo {repo} not inserted into {sumnsgraph.namespace}\")\n                dumpToFile(f\"{repo}_dumped\", summaryttl)\n                return 1\n    except Exception as ex:\n        logging.error(f\"error {ex}\")\n        print(f\"Error: {ex}\")\n        return 1\n    finally:\n        # need to figure out is this is run after return, I think it is.\n        if not args.graphtemp :\n            logging.debug(f\"Deleting Temp namespace {tempnsgraph.namespace}\")\n            deleted = tempnsgraph.deleteNamespace()\n        return 0</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/","title":"Earthcube Utilities Queries","text":""},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#query_graph","title":"query_graph","text":"<p><code>query_graph SPARQL_RESOURCE --graphendpoint https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/</code></p> <pre><code>usage: query_graph.py [-h] [--graphendpoint GRAPHENDPOINT] [--output OUTPUT]\n                      query\n\nA tool to use the queries in the earthcube utilities\n\npositional arguments:\n  query                 select_one\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --graphendpoint GRAPHENDPOINT\n                        graph endpoint\n  --output OUTPUT       output file\n\nQueries \nall_count_datasets \nall_count_keywords\nall_count_multiple_versioned_datasets \nall_count_triples all_count_types\nall_count_variablename \nall_repo_count_datasets \nall_repo_count_graphs\nall_repo_count_keywords \nall_repo_count_versioned_datasets\nall_repo_with_keywords \nall_select_datasets \nall_select_graphs \nall_summary_query\nall_versioned_datasets_multiple_versions \nget_triples_for_a_graph\nrepo_count_datasets \nrepo_count_graph_triples \nrepo_count_graphs\nrepo_count_keywords \nrepo_count_multi_versioned_datasets \nrepo_count_triples\nrepo_count_types \nrepo_count_variablename \nrepo_graphs_startwith\nrepo_select_datasets \nrepo_select_graphs \nrepo_summary_query \nselect_one</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#sparql-resources","title":"SPARQL RESOURCES","text":"<p>The naming pattern is if the query starts with all_ then it applies to the overall graph store, and requires no parameters</p> <p>if the naming pattern includes a repo or a urn then those are the parameters</p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#basic-queries","title":"Basic Queries","text":""},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#select_one","title":"select_one","text":"<p>test query, returns one triple <pre><code>SELECT * { ?s ?p ?o } LIMIT 1</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#urn_triples_for_a_graph","title":"urn_triples_for_a_graph","text":"<p>parameter:  urn - the urn identifier of the graph to be retrieved.</p> <p>returns a list of triples which represents one JSONLD file, aka graph <pre><code># template substitute ${urn}\nSELECT ?s ?p ?o ?g\nWHERE     {\n    GRAPH &lt;${urn}&gt; {?s ?p ?o .\n    bind(&lt;${urn}&gt; as ?g)\n}\n\n}</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#information-about-the-overall-graph","title":"Information about the overall graph","text":"<p>These may be used to collect information on the overall graph.</p> <p>In addition, when onboarding a community, these can be used to assess the evaluation the information for  repository containing a single communities data.</p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#top-level-types","title":"Top Level types","text":""},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_count_types_top_level","title":"all_count_types_top_level","text":"<pre><code># returns count of top level types\n\nprefix schema: &lt;https://schema.org/&gt;\nSELECT  ?type  (count(distinct ?s ) as ?scount)\nWHERE {\nGraph ?g {\n\n       ?s a ?type .\nFILTER ( !isBLANK(?s) )  .\n       }\n}\n\nGROUP By ?type\nORDER By DESC(?scount)\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_count_types_top_level","title":"repo_count_types_top_level","text":"<pre><code># returns count of top level types for repo\n\nprefix schema: &lt;https://schema.org/&gt;\nSELECT  ?type  (count(distinct ?s ) as ?scount)\nWHERE {\nGraph ?g {\n\n       ?s a ?type .\nFILTER ( !isBLANK(?s) )  .\nFILTER( CONTAINS(str(?g), \"${repo}\"))\n       }\n}\n\nGROUP By ?type\nORDER By DESC(?scount)\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_urn_w_types_toplevel","title":"repo_urn_w_types_toplevel","text":"<pre><code># this returns the URN's and the types of non blank nodes\nprefix schema: &lt;https://schema.org/&gt;\nSELECT ?s ?type\nWHERE {\nGraph ?g {\n\n       ?s a ?type .\n        ?s schema:identifier ?id .\nFILTER ( !isBLANK(?s) )  .\nFILTER( CONTAINS(str(?g), \"${repo}\"))\n       }\n}\n\nGROUP By ?type ?s\nORDER By DESC(?scount)\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_urn_w_types_toplevel","title":"all_urn_w_types_toplevel","text":"<pre><code># returns the urn and the type of the\nprefix schema: &lt;https://schema.org/&gt;\nSELECT ?s ?type\nWHERE {\nGraph ?g {\n       ?s a ?type .\n        FILTER ( !isBLANK(?s) )  .\n\n       }\n}\n\nGROUP By ?type ?s\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_count_datasets","title":"all_count_datasets","text":"<pre><code>SELECT (count(?g ) as ?datasetcount)\nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}}</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_count_keywords","title":"all_count_keywords","text":"<pre><code># needs work... keywords can be an array.\nprefix schema: &lt;https://schema.org/&gt;\nSELECT  ?keyword (count(distinct ?s) as ?scount)\nWHERE {\n  {\n    ?s schema:keywords ?keyword .\n   }\n}\nGROUP By ?keyword\nORDER By DESC(?scount)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_count_multiple_versioned_datasets","title":"all_count_multiple_versioned_datasets","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?version (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:version ?version .\n\n       }\n}\nGROUP By ?version\nHAVING(count(distinct ?s) &gt;1)\nORDER By DESC(?scount)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_count_triples","title":"all_count_triples","text":"<pre><code>SELECT (count(*) as ?tripelcount) { ?s ?p ?o }</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_graph_sizes","title":"all_graph_sizes","text":"<p>Returns aggregate of the size of the generated  triples for each loaded JSON-LD objects,</p> <p>High triple counts for a triple size may indicate a conversion issue</p> <pre><code>## WARNING Not bad performance for all\n## if the SAME TRIPLE COUNT APPEARS it may be a loading issue\n\nSELECT  ?triple_per_jsonld (count(?triplesize) as ?count)\nWHERE     {\n    SELECT  ?g (count(?s) as ?triple_per_jsonld)\n    WHERE     {\n         GRAPH ?g {?s ?p ?o .}\n\n    }\n    GROUP BY ?g\n         }\nGROUP BY ?triple_per_jsonld\nORDER BY DESC(?count)\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_count_types","title":"all_count_types","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?type  (count(distinct ?s ) as ?scount)\nWHERE {\n{\n\n       ?s a ?type .\n\n       }\n}\n\nGROUP By ?type\nORDER By DESC(?scount)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_count_variablename","title":"all_count_variablename","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?variableName (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:variableMeasured ?variableMeasured .\n    ?variableMeasured schema:name ?variableName\n\n       }\n}\nGROUP By ?variableName\nORDER By DESC(?scount)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_repo_count_datasets","title":"all_repo_count_datasets","text":"<pre><code>## Graphs do not always equal datasets. Especially in non-Earthcube graph stores\n## double dollar signes are to escape to a single dollar sign when using string template\n\nSELECT   ?repo (COUNT(distinct ?g) as ?graphs) (COUNT(*) as ?datasets)\nWHERE     {\n  GRAPH ?g {\n      ?s a &lt;https://schema.org/Dataset&gt; .\n      bind(  REPLACE(REPLACE( STR(?g) ,\"urn:(?:\\\\w+):\" ,\"\" ), \":\\\\w+$$\" , \"\")   as ?repo )\n   }\n}\ngroup by ?repo\n\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_repo_count_graphs","title":"all_repo_count_graphs","text":"<p>returns a count of inserted JSON-LD files, where each graph is a file. <pre><code>## double dollar signes are to escape to a single dollar sign when using string template\n## Graphs do not always equal datasets. Especially in non-Earthcube graph stores\nSELECT   ?repo (COUNT(distinct ?g) as ?graphs) (COUNT(*) as ?triples)\nWHERE     {\n  GRAPH ?g {\n      ?s ?p ?o .\n       bind(  REPLACE(REPLACE( STR(?g) ,\"urn:(?:\\\\w+):\" ,\"\" ), \":\\\\w+$$\" , \"\")   as ?repo )\n   }\n}\ngroup by ?repo\n\n## the regesx replace grabs the first portioon, replaces it with nothing,\n## then grabs the last part and replaces it with nothing,\n## NOTE: double \\\\ the escape sequeqnce akd \\w+ becomes \\\\w+\n#((\\w+):(\\w+))$$\n#(?2)(:(\\w+))\n\n##&lt;urn:gleaner:summoned:opentopography:0024e35144d902d8b413ffd400ede6a27efe2146&gt;\n# urn:\\w+:summoned:(\\w+)\n#urn:(?:\\w+):summoned:(\\w+)\n</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_repo_count_keywords","title":"all_repo_count_keywords","text":"<pre><code># needs work... keywords can be an array.\n## double dollar signes are to escape to a single dollar sign when using string template\nprefix schema: &lt;https://schema.org/&gt;\nSELECT  ?keyword (count(distinct ?s) as ?scount) ?repo\nWHERE {\n GRAPH ?g {\n           ?s schema:keywords ?keyword .\n           bind(  REPLACE(REPLACE( STR(?g) ,\"urn:(?:\\\\w+):\" ,\"\" ), \":\\\\w+$$\" , \"\")   as ?repo ) .\n           }\n      }\nGROUP By ?keyword ?repo\nORDER By DESC(?scount)\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_repo_count_versioned_datasets","title":"all_repo_count_versioned_datasets","text":"<p>returns a count of graphs including the version field. <pre><code>## double dollar signes are to escape to a single dollar sign when using string template\nprefix schema: &lt;https://schema.org/&gt;\nSELECT  ?repo (count(distinct ?s) as ?versionscount)\nWHERE {\n  GRAPH ?g {\n\n       ?s schema:version ?version .\n\n       } bind(  REPLACE(REPLACE( STR(?g) ,\"urn:(?:\\\\w+):\" ,\"\" ), \":\\\\w+$$\" , \"\")   as ?repo ) .\n}\nGROUP By ?repo\nORDER By DESC(?versionscount)\n</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_repo_with_keywords","title":"all_repo_with_keywords","text":"<p>Returns rows with a repo brief name, and the count of the keywords in that repo <pre><code># needs work... keywords can be an array.\n## double dollar signes are to escape to a single dollar sign when using string template\nprefix schema: &lt;https://schema.org/&gt;\nSELECT  ?repo (count(distinct ?s) as ?kwcount)\nWHERE {\n GRAPH ?g {\n           ?s schema:keywords ?keyword .\n           bind(  REPLACE(REPLACE( STR(?g) ,\"urn:(?:\\\\w+):\" ,\"\" ), \":\\\\w+$$\" , \"\")  as ?repo ) .\n           }\n      }\nGROUP By  ?repo\nORDER By DESC(?kwcount)\n</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_select_datasets","title":"all_select_datasets","text":"<p>returns a list of Schema.org/Dataset <pre><code>SELECT ?g\nWHERE     {     GRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}}</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_select_graphs","title":"all_select_graphs","text":"<p>returns a list of graphs, which are the urn's of the JSONLD files <pre><code>SELECT distinct ?g\nWHERE     {     GRAPH ?g {?s ?p ?o} }\n\n</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_summary_query","title":"all_summary_query","text":"<p>Return records with the fields as a 'summary ' which is used to materialize a view for performance</p> <p>this will need to be rewritten to use a LIMIT and OFFSET <pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?g ?resourceType ?name ?description  ?pubname\n        (GROUP_CONCAT(DISTINCT ?placename; SEPARATOR=\", \") AS ?placenames)\n        (GROUP_CONCAT(DISTINCT ?kwu; SEPARATOR=\", \") AS ?kw) ?datep ?sosType\n        ?maxdepth ?minDepth\n        #(GROUP_CONCAT(DISTINCT ?url; SEPARATOR=\", \") AS ?disurl)\n        WHERE {\n          graph ?g {\n           values ?sosType {\n               schema:Dataset\n#               schema:DataCatalog\n            }\n              ?subj    a    ?sosType .\n             ?subj schema:name ?name .\n             ?subj schema:description ?description .\n            Minus {?subj a schema:Person } .\n BIND (IF (exists {?subj a schema:Dataset .} ||exists{?subj a schema:DataCatalog .} , \"data\", \"tool\") AS ?resourceType).\n\n            optional {?subj schema:distribution/schema:url|schema:subjectOf/schema:url ?url1 .}\n            OPTIONAL {?subj schema:datePublished ?date_p .}\n            OPTIONAL {?subj schema:publisher/schema:name|schema:sdPublisher|schema:provider/schema:name ?pub_name .}\n            OPTIONAL {?subj schema:spatialCoverage/schema:name ?place_name .}\n            OPTIONAL {?subj schema:keywords ?kw1 .}\n            OPTIONAL {?subj schema:variableMeasured ?vm .\n               ?vm a  schema:PropertyValue  .\n               ?vm schema:name \"depth\" .\n                    ?vm schema:maxValue ?maxdepth .\n                  ?vm schema:minValue ?minDepth\n           }\n            # need these to get rdflib to work\n            bind ( COALESCE(?kw1, \"\") As ?kwu)\n            bind ( COALESCE(?url1, \"\") As ?url)\n           # Query should not return \"No datePublished\" is not a valid Date \"YYYY-MM-DD\" so\n            # UI Date Select failed, because it expects an actual date\n           # BIND ( IF ( BOUND(?date_p), ?date_p, \"No datePublished\") as ?datep ) .\n             BIND ( IF ( BOUND(?date_p), ?date_p, \"1900-01-01\") as ?datep ) .\n            BIND ( IF ( BOUND(?pub_name), ?pub_name, \"No Publisher\") as ?pubname ) .\n            BIND ( IF ( BOUND(?place_name), ?place_name, \"No spatialCoverage\") as ?placename ) .\n             }\n        }\n        GROUP BY ?subj ?pubname ?placenames ?kw ?datep   ?name ?description  ?resourceType ?sosType ?maxdepth ?minDepth ?g\n</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#all_versioned_datasets_multiple_versions","title":"all_versioned_datasets_multiple_versions","text":"<p>returns Schema.org/Dataset with more than one version.</p> <p>this might need to be a UNION with using SameAs, or another field... someone uses a different pattern <pre><code>prefix schema: &lt;http://schema.org/&gt;\nprefix sschema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?sameAs ?version ?url where {\n    {SELECT distinct  ?sameAs (MAX(?version2) as ?version  )\n    where {\n       ?subj schema:sameAs|sschema:sameAs ?sameAs .\n        ?subj schema:version|sschema:version ?version2 .\n    filter (?version2 &gt;1)\n    }\n        GROUP BY ?sameAs\n}\n        ?subj schema:identifier|sschema:identifier ?url .\n        ?subj schema:version|sschema:version ?version .\n        ?subj schema:sameAs|sschema:sameAs ?sameAs .\n    }\n    GROUP BY ?sameAs ?version  ?subj  ?url\norder by ?sameAs ?version\n</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#information-for-an-individual-repository","title":"Information for an individual repository","text":"<p>These queries take a parameter 'repo'</p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_count_datasets","title":"repo_count_datasets","text":"<p>Returns count of Schema.org/Dataset <pre><code>SELECT (count(?g ) as ?datasetcount)\nWHERE     {\n   GRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}\n    FILTER( CONTAINS(str(?g), \"${repo}\") )\n}</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_count_triples_by_graph","title":"repo_count_triples_by_graph","text":"<p>Returns count of triples for each loaded JSON-LD objects,</p> <p>If the count is small, and the same number, then it is possible that the conversion from JSON-LD to RDF is broken for this repository.</p> <pre><code>## WARNING THIS IS  A SLOW QUERY\nSELECT  ?g (count(?s) as ?count)\nWHERE     {\n     GRAPH ?g {?s ?p ?o .}\n    FILTER( CONTAINS(str(?g), \"${repo}\") )\n}\nGROUP BY ?g</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_graph_sizes","title":"repo_graph_sizes","text":"<p>Returns aggregate of the size of the generated  triples for each loaded JSON-LD objects,</p> <p>If the count is small, and the same number, then it is possible that the conversion from JSON-LD to RDF is broken for this repository. <pre><code>## WARNING SLOW FOR A REPO...\n## if the SAME TRIPLE COUNT APPEARS it may be a loading issue\n\nSELECT  ?triple_per_jsonld (count(?triple_per_jsonld) as ?count)\nWHERE     {\n    SELECT  ?g (count(?s) as ?triple_per_jsonld)\n    WHERE     {\n         GRAPH ?g {?s ?p ?o .}\n         FILTER( CONTAINS(str(?g), \"${repo}\") )\n    }\n    GROUP BY ?g\n         }\nGROUP BY ?triple_per_jsonld\nORDER BY DESC(?count)\n</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_count_graphs","title":"repo_count_graphs","text":"<p>Count of loaded JSON-LD files <pre><code>SELECT ?repo (count(distinct ?g) as ?graphcount)\nWHERE     {\n     GRAPH ?g {?s ?p ?o .}\n    FILTER( CONTAINS(str(?g), \"${repo}\") )\n}</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_count_keywords","title":"repo_count_keywords","text":"<pre><code># needs work... keywords can be an array.\nprefix schema: &lt;https://schema.org/&gt;\nSELECT  ?keyword (count(distinct ?s) as ?scount)\nWHERE {\n    GRAPH ?g {?s schema:keywords ?keyword . }\n     FILTER( CONTAINS(str(?g), \"${repo}\") )\n}\nGROUP By ?keyword\nORDER By DESC(?scount)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_count_multi_versioned_datasets","title":"repo_count_multi_versioned_datasets","text":"<p>Does this repo have multiple dataset versions, if so, what is the count.</p> <pre><code>## double dollar signes are to escape to a single dollar sign when using string template\nprefix schema: &lt;https://schema.org/&gt;\nSELECT  ?g (count(distinct ?s) as ?versionscount)\nWHERE {\n  GRAPH ?g {\n\n       ?s schema:version ?version .\n\n       }  FILTER( CONTAINS(str(?g), \"${repo}\")) .\n}\nGROUP By ?g\nHAVING(count(distinct ?s) &gt;1 )\nORDER By DESC(?versionscount)\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_count_triples","title":"repo_count_triples","text":"<pre><code>SELECT (count(*) as ?tripelcount)\nWHERE     {\n     GRAPH ?g {?s ?p ?o .}\n    FILTER( CONTAINS(str(?g), \"${repo}\") )\n}</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_count_types","title":"repo_count_types","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?type  (count(distinct ?s ) as ?scount)\nWHERE {\nGRAPH ?g  {\n\n       ?s a ?type .\n\n       }\n       FILTER( CONTAINS(str(?g), \"${repo}\"))\n}\n\nGROUP By ?type\nORDER By DESC(?scount)\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_count_variablename","title":"repo_count_variablename","text":"<pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT  ?variableName (count(distinct ?s) as ?scount)\nWHERE {\n  {\n\n       ?s schema:variableMeasured ?variableMeasured .\n    ?variableMeasured schema:name ?variableName\n\n       } FILTER( CONTAINS(str(?g), \"${repo}\"))\n}\nGROUP By ?variableName\nORDER By DESC(?scount)\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_graphs_startwith","title":"repo_graphs_startwith","text":"<pre><code>SELECT distinct ?g\nWHERE     {\n     GRAPH ?g {?s ?p ?o .}\n    FILTER( CONTAINS(str(?g), \"${repo}\") )\n}\n</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_select_datasets","title":"repo_select_datasets","text":"<p>return urns/graph of Schema.org/Dataset <pre><code>SELECT ?g\nWHERE     {\nGRAPH ?g {?s a &lt;https://schema.org/Dataset&gt;}\n #FILTER( CONTAINS(str(?g), \"${repo}\") )\n FILTER( CONTAINS(str(?g), \"${repo}\"))\n}\n</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_select_graphs","title":"repo_select_graphs","text":"<p>return urns/graph of all loaded information <pre><code>SELECT distinct ?g\nWHERE     {\n     GRAPH ?g {?s ?p ?o .}\n    # FILTER( CONTAINS(str(?g), \"${repo}\") )\n    FILTER( CONTAINS(str(?g), \"${repo}\"))\n}\n</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#repo_summary_query","title":"repo_summary_query","text":"<p>Return records with the fields as a 'summary ' which is used to materialize a view for performance</p> <p>this will need to be rewritten to use a LIMIT and OFFSET <pre><code>prefix schema: &lt;https://schema.org/&gt;\nSELECT distinct ?subj ?g ?resourceType ?name ?description  ?pubname\n        (GROUP_CONCAT(DISTINCT ?placename; SEPARATOR=\", \") AS ?placenames)\n        (GROUP_CONCAT(DISTINCT ?kwu; SEPARATOR=\", \") AS ?kw) ?datep ?sosType\n        ?maxdepth ?minDepth\n        #(GROUP_CONCAT(DISTINCT ?url; SEPARATOR=\", \") AS ?disurl)\n        WHERE {\n          graph ?g {\n            values ?sosType {\n               schema:Dataset\n#               schema:DataCatalog\n            }\n              ?subj    a    ?sosType .\n             ?subj schema:name ?name .\n             ?subj schema:description ?description .\n\n            #?subj a schema:Dataset .\n           #  ?subj a ?sosType .\n            Minus {?subj a schema:Person } .\n BIND (IF (exists {?subj a schema:Dataset .} ||exists{?subj a schema:DataCatalog .} , \"data\", \"tool\") AS ?resourceType).\n\n optional {?subj schema:distribution/schema:url|schema:subjectOf/schema:url ?url1 .}\n            OPTIONAL {?subj schema:datePublished ?date_p .}\n            OPTIONAL {?subj schema:publisher/schema:name|schema:sdPublisher|schema:provider/schema:name ?pub_name .}\n            OPTIONAL {?subj schema:spatialCoverage/schema:name ?place_name .}\n            OPTIONAL {?subj schema:keywords ?kw1 .}\n            OPTIONAL {?subj schema:variableMeasured ?vm .\n               ?vm a  schema:PropertyValue  .\n               ?vm schema:name \"depth\" .\n                    ?vm schema:maxValue ?maxdepth .\n                  ?vm schema:minValue ?minDepth\n           }\n            # need these to get rdflib to work\n            bind ( COALESCE(?kw1, \"\") As ?kwu)\n            bind ( COALESCE(?url1, \"\") As ?url)\n           # Query should not return \"No datePublished\" is not a valid Date \"YYYY-MM-DD\" so\n            # UI Date Select failed, because it expects an actual date\n           # BIND ( IF ( BOUND(?date_p), ?date_p, \"No datePublished\") as ?datep ) .\n             BIND ( IF ( BOUND(?date_p), ?date_p, \"1900-01-01\") as ?datep ) .\n            BIND ( IF ( BOUND(?pub_name), ?pub_name, \"No Publisher\") as ?pubname ) .\n            BIND ( IF ( BOUND(?place_name), ?place_name, \"No spatialCoverage\") as ?placename ) .\n             }\n        FILTER( CONTAINS(str(?g), \"${repo}\") )\n        }\n        GROUP BY ?subj ?pubname ?placenames ?kw ?datep   ?name ?description  ?resourceType ?sosType ?maxdepth ?minDepth ?g\n</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#code","title":"Code","text":""},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#ec.generate_graph_stats.graphStats","title":"<code>graphStats(args)</code>","text":"<p>query an endpoint, store results as a json file in an s3 store</p> Source code in <code>earthcube_utilities/src/ec/generate_graph_stats.py</code> <pre><code>def graphStats(args):\n    \"\"\"query an endpoint, store results as a json file in an s3 store\"\"\"\n    if args.release is not None:\n        log.info(f\"Using  {args.release} for graph statisitcs  \")\n        if (args.detailed):\n            report_json = generateGraphReportsRelease(args.source, args.release,reportList=reportTypes[\"repo_detailed\"] )\n        else:\n            report_json = generateGraphReportsRelease(args.source,\n                                                       args.release, reportList=reportTypes[\"repo\"] )\n    else:\n        log.info(f\"Querying {args.graphendpoint} for graph statisitcs  \")\n    ### more work needed before detailed works\n        if args.source == \"all\":\n             # report_json = generateGraphReportsRepo(\"all\",\n             #      args.graphendpoint, reportTypes=reportTypes)\n\n            if (args.detailed):\n                report_json = generateGraphReportsRepo(\"all\", args.graphendpoint, reportList=reportTypes[\"all_detailed\"] )\n            else:\n                report_json = generateGraphReportsRepo(\"all\",\n                                                           args.graphendpoint,reportList=reportTypes[\"all\"])\n        else:\n            # report_json = generateGraphReportsRepo(args.repo,\n            #   args.graphendpoint,reportTypes=reportTypes)\n\n            if (args.detailed):\n                report_json = generateGraphReportsRepo(args.source, args.graphendpoint,reportList=reportTypes[\"repo_detailed\"] )\n            else:\n                report_json = generateGraphReportsRepo(args.source,\n                                                           args.graphendpoint, reportList=reportTypes[\"repo\"] )\n\n    #data = f.getvalue()\n\n    if (args.output):  # just append the json files to one filem, for now.\n        logging.info(f\" report for {args.source} appended to file\")\n        args.output.write(report_json)\n    if not args.no_upload:\n        s3Minio = s3.MinioDatastore(args.s3server, None)\n        bucketname, objectname = s3Minio.putReportFile(args.s3bucket,args.source,\"graph_stats.json\",report_json)\n    return 0</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_graph_query_sparql/#ec.generate_graph_stats.start","title":"<code>start()</code>","text":"<p>Run the generate_repo_stats program. query an endpoint, store results as a json file in an s3 store. Arguments:     args: Arguments passed from the command line. Returns:     An exit code.</p> Source code in <code>earthcube_utilities/src/ec/generate_graph_stats.py</code> <pre><code>def start():\n    \"\"\"\n        Run the generate_repo_stats program.\n        query an endpoint, store results as a json file in an s3 store.\n        Arguments:\n            args: Arguments passed from the command line.\n        Returns:\n            An exit code.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--release', dest='release',\n                        help='run over release file: try: https://oss.geocodes-aws.earthcube.org/earthcube/graphs/latest/iris_release.nq'\n\n                        )\n\n    parser.add_argument('--graphendpoint', dest='graphendpoint',\n                        help='graph endpoint' ,default=\"https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/\")\n    parser.add_argument('--s3', dest='s3server',\n                        help='s3 server address (localhost:9000)', default='localhost:9000')\n    parser.add_argument('--s3bucket', dest='s3bucket',\n                        help='s3 server address (localhost:9000)', default='gleaner')\n    parser.add_argument('--source', dest='source',\n                        help='repository', default='all')\n\n    parser.add_argument(\"--detailed\",action='store_true',\n                        dest=\"detailed\" ,help='run the detailed version of the reports', default=False)\n    parser.add_argument('--no-upload', dest = 'no_upload',action='store_true', default=False,\n                        help = 'do not upload to s3 bucket ')\n    parser.add_argument('--output',  type=argparse.FileType('w'), dest=\"output\", help=\"dump to file\")\n\n    args = parser.parse_args()\n\n    exitcode = graphStats(args)\n    exit(exitcode)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/","title":"Earthcube Utilities Code Documentation","text":"<p>Note</p> <p>this does not work with multirepo plug, so will need to be linked </p>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/#scripts","title":"Scripts","text":""},{"location":"earthcube_utilities/earthcube_utilities_scripts/#ec_reports","title":"ec_reports","text":"<p>```Usage: ec_reports.py [OPTIONS] COMMAND [ARGS]... Options:   --cfgfile PATH          gleaner config file   --s3server TEXT         s3 server address   --s3bucket TEXT         s3 bucket   --graphendpoint TEXT    graph endpoint   --upload / --no-upload  upload to s3 bucket   --output FILENAME       dump to file   --debug / --no-debug   --help                  Show this message and exit.</p> <p>Commands:   graph-stats   missing-report   identifier-stats  Usage: ec_reports.py missing-report [OPTIONS]</p> <p>Options:   --source TEXT           gone or more repositories (--source a --source b)   --milled / --no-milled  include milled   --summon / --no-sommon  check summon only   --cfgfile PATH          gleaner config file   --s3server TEXT         s3 server address   --s3bucket TEXT         s3 bucket   --graphendpoint TEXT    graph endpoint   --upload / --no-upload  upload to s3 bucket   --output FILENAME       dump to file   --debug / --no-debug   --help                  Show this message and exit.</p> <p> Usage: ec_reports.py graph-stats [OPTIONS]</p> <p>Options:   --source TEXT           One or more repositories (--source a --source b)   --detailed              run the detailed version of the reports   --cfgfile PATH          gleaner config file   --s3server TEXT         s3 server address   --s3bucket TEXT         s3 bucket   --graphendpoint TEXT    graph endpoint   --upload / --no-upload  upload to s3 bucket   --output FILENAME       dump to file   --debug / --no-debug    run the detailed version of the reports   --help                  Show this message and exit.</p> <p> Usage: ec_reports.py identifier-stats [OPTIONS]</p> <p>Options:   --source TEXT           One or more repositories (--source a --source b)   --detailed              run the detailed version of the reports   --cfgfile PATH          gleaner config file   --s3server TEXT         s3 server address   --s3bucket TEXT         s3 bucket   --graphendpoint TEXT    graph endpoint   --upload / --no-upload  upload to s3 bucket   --output FILENAME       dump to file   --json boolean          output json format (default True for json, False for csv)   --help                  Show this message and exit.</p> <p><pre><code>\n## **bucketutil**\n```Usage: bucketutil.py [OPTIONS] COMMAND [ARGS]...\nOptions:\n  --cfgfile PATH          Gleaner config file\n  --s3server TEXT         S3 server address\n  --s3bucket TEXT         S3 bucket\n  --upload Boolean        Upload to s3 bucket. default True\n  --output FILENAME       Dump to file\n  --help                  Show this message and exit.\n\nCommands:\n  count\n  urls\n  download\n  sourceurl\n  duplicates\n  stats\n  cull</code></pre> <pre><code>Usage: bucketutil.py count [OPTIONS]\n\nOptions:\n  --path TEXT             Path to source, e.g. summoned/iris\n  --milled Boolean        Include milled, default False\n  --summon Boolean        Check summon only, default True\n  --cfgfile PATH          Gleaner config file\n  --s3server TEXT         S3 server address\n  --s3bucket TEXT         S3 bucket\n  --upload Boolean        Upload to s3 bucket\n  --output FILENAME       dump to file\n  --help                  Show this message and exit.</code></pre> <pre><code>Usage: bucketutil.py urls [OPTIONS]\n\nOptions:\n  --source TEXT           A repository\n  --cfgfile PATH          Gleaner config file\n  --s3server TEXT         S3 server address\n  --s3bucket TEXT         S3 bucket\n  --graphendpoint TEXT    Graph endpoint\n  --upload Boolean        Upload to s3 bucket, default True\n  --output FILENAME       Dump to file\n  --help                  Show this message and exit.</code></pre> <pre><code>Usage: bucketutil.py download [OPTIONS]\n\nOptions:\n  --urn TEXT              One or more urns (--urn a --urn b)\n  --cfgfile PATH          Gleaner config file\n  --s3server TEXT         S3 server address\n  --s3bucket TEXT         S3 bucket\n  --upload Boolean        Upload to s3 bucket, default True\n  --output FILENAME       Dump to file\n  --help                  Show this message and exit.</code></pre> <pre><code>Usage: bucketutil.py sourceurl [OPTIONS]\n\nOptions:\n  --url TEXT              The X-Amz-Meta-Url in metadata\n  --milled Boolean        Include milled, default False\n  --summon Boolean        Check summon only, default True\n  --cfgfile PATH          Gleaner config file\n  --s3server TEXT         S3 server address\n  --s3bucket TEXT         S3 bucket\n  --upload Boolean        Upload to s3 bucket, default True\n  --output FILENAME       Dump to file\n  --help                  Show this message and exit.</code></pre> <pre><code>Usage: bucketutil.py duplicateurls [OPTIONS]\n\nOptions:\n  --source TEXT           One or more sources (--source a --source b)\n  --cfgfile PATH          Gleaner config file\n  --s3server TEXT         S3 server address\n  --s3bucket TEXT         S3 bucket\n  --upload Boolean        Upload to s3 bucket, default True\n  --output FILENAME       Dump to file\n  --help                  Show this message and exit.</code></pre> <pre><code>Usage: bucketutil.py stats [OPTIONS]\n\nOptions:\n  --source TEXT           One or more sources (--source a --source b)\n  --cfgfile PATH          Gleaner config file\n  --s3server TEXT         S3 server address\n  --s3bucket TEXT         S3 bucket\n  --upload Boolean        Upload to s3 bucket, default True\n  --output FILENAME       Dump to file\n  --help                  Show this message and exit.</code></pre> <pre><code>Usage: bucketutil.py cullurls [OPTIONS]\n\nOptions:\n  --source TEXT           One or more sources (--source a --source b)\n  --cfgfile PATH          Gleaner config file\n  --s3server TEXT         S3 server address\n  --s3bucket TEXT         S3 bucket\n  --graphendpoint TEXT    Graph endpoint\n  --upload Boolean        Upload to s3 bucket, default True\n  --output FILENAME       Dump to file\n  --help                  Show this message and exit.</code></pre></p>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/#query_graph","title":"query_graph","text":"<p>Separate Document with Sparql Queries <code>query_graph SPARQL_FILE --graphendpoint https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/</code></p> <pre><code>usage: query_graph.py [-h] [--graphendpoint GRAPHENDPOINT] [--output OUTPUT]\n                      query\n\nA tool to use the queries in the earthcube utilities\n\npositional arguments:\n  query                 select_one\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --graphendpoint GRAPHENDPOINT\n                        graph endpoint\n  --output OUTPUT       output file</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/#check_sitemap","title":"check_sitemap","text":"<p>`check_sitemap SITEMAP_URL --output FILE --no-check-url </p> <pre><code>usage: check_sitemap.py [-h] [--output OUTPUT] [--no-url-check] [--no-progress] sitemapurl\n\npositional arguments:\n  sitemapurl       sitemapurl\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --output OUTPUT  output file\n  --no-url-check   output file\n  --no-progress    no progress bar</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/#ec.check_sitemap.start","title":"<code>start()</code>","text":"<p>Run the sitemap_checker program. Sitemap checker. Default option  checks if url is sitemap exist. Arguments:     args: Arguments passed from the command line. Returns:     result of check as csv.</p> Source code in <code>earthcube_utilities/src/ec/check_sitemap.py</code> <pre><code>def start():\n    \"\"\"\n        Run the sitemap_checker program.\n        Sitemap checker. Default option  checks if url is sitemap exist.\n        Arguments:\n            args: Arguments passed from the command line.\n        Returns:\n            result of check as csv.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"sitemapurl\", help='sitemapurl')\n    parser.add_argument(\"--output\", type=argparse.FileType('w'), help='output file')\n    parser.add_argument(\"--no-url-check\",action='store_true', dest=\"nocheck\" ,help='output file', default=False)\n    parser.add_argument(\"--no-progress\", action='store_false',   dest=\"no_progress\" ,help='no progress bar', default=True)\n    args = parser.parse_args()\n\n    result = sitemap_checker(args)\n    if args.output:\n        log.info(\" Sitemap written to file\"  )\n        args.output.write(result)\n    else:\n        print(result)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/#generategrapstats","title":"generategrapstats","text":"<p><code>generategrapstats --graphendpoint https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/ -s3 localhost:9000 --s3bucket gleaner</code></p> <pre><code>usage: generate_graph_stats.py [-h] [--graphendpoint GRAPHENDPOINT]\n                               [--s3 S3SERVER] [--s3bucket S3BUCKET]\n                               [--repo REPO] [--detailed]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --graphendpoint GRAPHENDPOINT\n                        graph endpoint\n  --s3 S3SERVER         s3 server address (localhost:9000)\n  --s3bucket S3BUCKET   s3 bucket name\n  --repo REPO           repository\n  --detailed            run the detailed version of the reports</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/#ec.generate_graph_stats.graphStats","title":"<code>graphStats(args)</code>","text":"<p>query an endpoint, store results as a json file in an s3 store</p> Source code in <code>earthcube_utilities/src/ec/generate_graph_stats.py</code> <pre><code>def graphStats(args):\n    \"\"\"query an endpoint, store results as a json file in an s3 store\"\"\"\n    if args.release is not None:\n        log.info(f\"Using  {args.release} for graph statisitcs  \")\n        if (args.detailed):\n            report_json = generateGraphReportsRelease(args.source, args.release,reportList=reportTypes[\"repo_detailed\"] )\n        else:\n            report_json = generateGraphReportsRelease(args.source,\n                                                       args.release, reportList=reportTypes[\"repo\"] )\n    else:\n        log.info(f\"Querying {args.graphendpoint} for graph statisitcs  \")\n    ### more work needed before detailed works\n        if args.source == \"all\":\n             # report_json = generateGraphReportsRepo(\"all\",\n             #      args.graphendpoint, reportTypes=reportTypes)\n\n            if (args.detailed):\n                report_json = generateGraphReportsRepo(\"all\", args.graphendpoint, reportList=reportTypes[\"all_detailed\"] )\n            else:\n                report_json = generateGraphReportsRepo(\"all\",\n                                                           args.graphendpoint,reportList=reportTypes[\"all\"])\n        else:\n            # report_json = generateGraphReportsRepo(args.repo,\n            #   args.graphendpoint,reportTypes=reportTypes)\n\n            if (args.detailed):\n                report_json = generateGraphReportsRepo(args.source, args.graphendpoint,reportList=reportTypes[\"repo_detailed\"] )\n            else:\n                report_json = generateGraphReportsRepo(args.source,\n                                                           args.graphendpoint, reportList=reportTypes[\"repo\"] )\n\n    #data = f.getvalue()\n\n    if (args.output):  # just append the json files to one filem, for now.\n        logging.info(f\" report for {args.source} appended to file\")\n        args.output.write(report_json)\n    if not args.no_upload:\n        s3Minio = s3.MinioDatastore(args.s3server, None)\n        bucketname, objectname = s3Minio.putReportFile(args.s3bucket,args.source,\"graph_stats.json\",report_json)\n    return 0</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/#ec.generate_graph_stats.start","title":"<code>start()</code>","text":"<p>Run the generate_repo_stats program. query an endpoint, store results as a json file in an s3 store. Arguments:     args: Arguments passed from the command line. Returns:     An exit code.</p> Source code in <code>earthcube_utilities/src/ec/generate_graph_stats.py</code> <pre><code>def start():\n    \"\"\"\n        Run the generate_repo_stats program.\n        query an endpoint, store results as a json file in an s3 store.\n        Arguments:\n            args: Arguments passed from the command line.\n        Returns:\n            An exit code.\n\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--release', dest='release',\n                        help='run over release file: try: https://oss.geocodes-aws.earthcube.org/earthcube/graphs/latest/iris_release.nq'\n\n                        )\n\n    parser.add_argument('--graphendpoint', dest='graphendpoint',\n                        help='graph endpoint' ,default=\"https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/\")\n    parser.add_argument('--s3', dest='s3server',\n                        help='s3 server address (localhost:9000)', default='localhost:9000')\n    parser.add_argument('--s3bucket', dest='s3bucket',\n                        help='s3 server address (localhost:9000)', default='gleaner')\n    parser.add_argument('--source', dest='source',\n                        help='repository', default='all')\n\n    parser.add_argument(\"--detailed\",action='store_true',\n                        dest=\"detailed\" ,help='run the detailed version of the reports', default=False)\n    parser.add_argument('--no-upload', dest = 'no_upload',action='store_true', default=False,\n                        help = 'do not upload to s3 bucket ')\n    parser.add_argument('--output',  type=argparse.FileType('w'), dest=\"output\", help=\"dump to file\")\n\n    args = parser.parse_args()\n\n    exitcode = graphStats(args)\n    exit(exitcode)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/#missing_report","title":"missing_report","text":"<p><code>missing_report --graphendpoint https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/  --s3 localhost:9000 --s3bucket gleaner --cfgfile gleaner --no_upload True --output output.json</code></p> <pre><code>usage: missing_report.py [-h] [--graphendpoint GRAPHENDPOINT]\n                              [--s3 S3SERVER] [--s3bucket S3BUCKET]\n                              [--cfgfile] [--no_upload] [--output]\n                              [--source] [--milled] [--summon]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --graphendpoint GRAPHENDPOINT\n                        graph endpoint\n  --s3 S3SERVER         s3 server address (localhost:9000)\n  --s3bucket S3BUCKET   s3 bucket name\n  --cfgfile             a gleaner config file\n  --no_upload           whether to write the missing report to s3 or not (True/False)\n  --output              if no_upload is True, dump the missing report to output\n  --source              one or more repositories (--source a --source b)\n  --milled              include milled\n  --summon              check summon only</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/#ec.missing_report.start","title":"<code>start()</code>","text":"<p>Run the write_missing_report program. Get a list of active repositories from the gleaner file. For each repository, generate missing reports and write these information to a json file and upload it to s3. Arguments:     args: Arguments passed from the command line. Returns:     An exit code.</p> Source code in <code>earthcube_utilities/src/ec/missing_report.py</code> <pre><code>def start():\n    \"\"\"\n        Run the write_missing_report program.\n        Get a list of active repositories from the gleaner file.\n        For each repository, generate missing reports and write these information to a json file and upload it to s3.\n        Arguments:\n            args: Arguments passed from the command line.\n        Returns:\n            An exit code.\n    \"\"\"\n    exitcode = writeMissingReport()\n    sys.exit(exitcode)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilities_scripts/#summarize_identifier_metadata","title":"summarize_identifier_metadata","text":"<p><code>summarize_identifier_metadata --graphendpoint https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/earthcube/  --s3 localhost:9000 --s3bucket gleaner --cfgfile gleaner --no_upload True  --output output.json --json False --source Iris</code></p> <pre><code>usage: summarize_identifier_metadata.py [-h] [--graphendpoint GRAPHENDPOINT]\n                              [--s3 S3SERVER] [--s3bucket S3BUCKET]\n                              [--cfgfile] [--no_upload] [--output]\n                              [--json] [--source]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --graphendpoint GRAPHENDPOINT\n                        graph endpoint\n  --s3 S3SERVER         s3 server address (localhost:9000)\n  --s3bucket S3BUCKET   s3 bucket name\n  --cfgfile             a gleaner config file\n  --no_upload           whether to write the missing report to s3 or not (True/False)\n  --output              if no_upload is True, dump the report to output\n  --json                output as json format, otherwise csv format (True/False)\n  --source              one or more repositories (--source a --source b)</code></pre>"},{"location":"earthcube_utilities/earthcube_utilties_functionality/","title":"Functionality of the Earthcube Utilities","text":""},{"location":"earthcube_utilities/earthcube_utilties_functionality/#earthcube-utilities-functionality-docments","title":"Earthcube Utilities Functionality Docments:","text":"<ul> <li>Functionality Breakdown </li> <li>~~Mini-base~~</li> <li> X(./ec_query.md)</li> <li>[ X]  RDF to Quads in sos_json/rdf</li> <li>Validation<ul> <li>[ X]  Validating Sitemaps [ ]  Validating JSONLD [x ]  Validating Graph Loading initially use reporting/get graph reports</li> <li> (./validate_distribution_links.md)</li> </ul> </li> <li> X(./validation_test_data.md) in Geocodes_Metadata/Integration_testing</li> <li>X (./repository_reporting.md)]</li> <li>[ ]  RoCrate, aka collection</li> <li>[ X]  JSONLD Utils sos_json/utils</li> <li>Notebooks<ul> <li>Visualization</li> </ul> </li> </ul>"},{"location":"earthcube_utilities/earthcube_utilties_functionality/#goal","title":"Goal","text":"<p>We want earthcube_utilities to be a pypi package... </p> <p>The present codebase needs to reimplemented, and documented so that it can be a package, and can be used by others.</p> <p>The present earthcube_utilities.py  in the top level directory is large (3700 lines of code) and contains a lot of dead code that makes it difficult to  comprehend. It needs to be redone. </p> <p>Note, this directory contains an older version, and so it can be deleted when developement of  the revised code begins.</p> <p>Reimplementation will need to be done with planning, and a clean room approach to modularization. Ideally, code should be reimplemented, cleanly with logical function names, and not just copied. The implementation should include unit testing, if appropriate. Especially, if such tests help demonstrate functionality.</p>"},{"location":"earthcube_utilities/earthcube_utilties_functionality/#steps","title":"Steps","text":"<ul> <li>Plan: Document what functionality that you think needs to exist in the package.</li> <li>functionality, basically, what are the 'modules'</li> <li>Document, what methods that you expect users to call to get access to the functionality of the modules.</li> <li>What are the base modules that need to be implemented first.</li> <li>Implementation Implement functionality in modular chunks.</li> <li>Pick a 'feature'/functionality,</li> <li>Write  tests, and code.  </li> <li>Revise documentation, or just incorporate documentation into the code, and generate automatically</li> </ul> <p>Know that by utilizing  unit tests, you  demonstrate how some of the functionality is utilized, and reduce cruft in the actual code.</p> <p>You can test pandas dataframes by reading in data dumped for pandas dataframes.</p> <p>For dataframe testing, you  load data from a csv using load pandas.read_csv...</p> <p>To get data for testing, I suggest using pycharm, you can dump a dataframe using the export when viewing in debuging... Caveat: name with a .csv extension.... and trim testing data it down to a row or two.  </p>"},{"location":"earthcube_utilities/reporting/","title":"Using the Reports scripts","text":"<p>The utilities include a set of scripts that can generate Q/A and other details about data summoned and loaded to a graph. When installed as a python package (pip3 install earthcube_utilities)  or editable developer package (cd earthcube_utilties; pip3 install -e .)</p> <p>In the longer term, this will provide a record of how repostories grow/shrink etc.</p> <p>These scripts are installed.</p> <ul> <li><code>check_sitemap</code> - given a sitemap, checks to see if the url work</li> <li><code>missing_report</code> = looks a data loss sitemap&gt;summon&gt;graph</li> <li><code>generate_graph_stats</code> = provides a summary of the infromation loaded into a graph</li> <li><code>summarize_identifier_metadata</code> - For each repo, provides summary on how sha identifers were generated.`</li> <li><code>query_graph</code> = tool, not a report. It can be used to run a specific sqarql query from the command line</li> </ul>"},{"location":"earthcube_utilities/reporting/#load-process","title":"Load process","text":"<ol> <li>check sitemap, see that url work</li> <li>run glcon gleaner batch</li> <li>run a missing_report  --summon to see what is possibly missing.<ul> <li>for a slow load, you can run this and see about possible issues with particular url patterns in the sitemap</li> </ul> </li> <li>run <code>summarize_identifier_metadata</code> - Do the identifiers seem reasonable</li> <li>run glcon nabu prefix (and glcon nabu prune, if updating)</li> <li>run missing_report</li> <li>run generate_graph_stats</li> </ol>"},{"location":"earthcube_utilities/reporting/#check_sitemap","title":"check_sitemap","text":"<p>This read sitemap, and writes to a file (needs to be pushed to s3 with the sitemaps?) It uses a head call to see if url is not 404. Some webistes return 200 for all calls, so this will not be accurate, but</p>"},{"location":"earthcube_utilities/reporting/#missing-report","title":"missing report","text":"<p>Counts</p> <ul> <li>sitemap how many urls in a sitemap</li> <li>summon how many made it into the graphstore. This may be larger if multiple JSONLD's are in a page</li> <li>graph_urn_count how many JSONLD loaded into the graph</li> </ul> <p>Records</p> <ul> <li>Missing_sitemap_Summon == Files w/o jsonld, or not a url</li> <li>\"missing_summon_graph\" URN's of files that did not make it into graph.</li> </ul> <pre><code>{\n  \"source\": \"cchdo\",\n  \"graph\": \"https://graph.geocodes-dev.earthcube.org/blazegraph/namespace/deepoceans/sparql\",\n  \"sitemap\": \"https://cchdo.ucsd.edu/sitemap.xml\",\n  \"date\": \"2023-04-19\",\n  \"bucket\": \"deepoceans\",\n  \"s3store\": \"oss.geocodes-dev.earthcube.org\",\n  \"sitemap_count\": 2527,\n  \"summoned_count\": 2520,\n  \"missing_sitemap_summon\": [\n    \"http://cchdo.ucsd.edu/search/map\",\n    \"http://cchdo.ucsd.edu/search/advanced\",\n    \"http://cchdo.ucsd.edu/contact\",\n    \"http://cchdo.ucsd.edu/citation\",\n    \"http://cchdo.ucsd.edu/policy\",\n    \"http://cchdo.ucsd.edu/formats\",\n    \"http://cchdo.ucsd.edu/cruise/FKAD20230131\"\n  ],\n  \"graph_urn_count\": 2521,\n  \"missing_summon_graph\": []\n}</code></pre>"},{"location":"earthcube_utilities/reporting/#identifier-report","title":"Identifier Report","text":"<p>This summarizes which 'identifiers' were used to generate the unique identifier (sha hash)</p> <pre><code>[\n  {\n    \"Identifiertype\":\"identifiersha\",\n    \"Matchedpath\":\"$.url\",\n    \"Uniqueid\":1\n  },\n  {\n    \"Identifiertype\":\"identifiersha\",\n    \"Matchedpath\":\"$['@id']\",\n    \"Uniqueid\":2519\n  }\n]</code></pre>"},{"location":"earthcube_utilities/reporting/#graph-report","title":"graph report","text":"<p>This queries the loaded graphstore. There are two possibilties, all and per-source (repository)</p> <p>All sources in a gleaner file will have one record, the org record.</p> <p>A large difference between the JSONLD count and the Loaded graph count should show up in the missing_report</p> <p>This tells you what Schema.org types are loaded from a source, how many datasets there are, and the diversity of keywords  and variable naming. How many ways to say latitude, longitude... date, Day, etc </p> <p>While for variable names we may not need to address this, in loading since they describe the dataset, for  discovery, we will need to well, use the graph for what the graph is good at.</p> <pre><code>{\n    \"version\": 0,\n    \"repo\": \"all\",\n    \"date\": \"2023-04-19\",\n    \"reports\": [\n        {\n            \"report\": \"triple_count\",\n            \"data\": [\n                {\n                    \"tripelcount\": \"2346399\"\n                }\n            ]\n        },\n        {\n            \"report\": \"graph_count_by_repo\",\n            \"data\": [\n                {\n                    \"repo\": \"bco-dmo\",\n                    \"graphs\": \"825\",\n                    \"triples\": \"76096\"\n                },\n                {\n                    \"repo\": \"cchdo\",\n                    \"graphs\": \"2520\",\n                    \"triples\": \"818876\"\n                },\n                {\n                    \"repo\": \"r2r\",\n                    \"graphs\": \"11105\",\n                    \"triples\": \"613434\"\n                },\n                {\n                    \"repo\": \"ssdb.iodp\",\n                    \"graphs\": \"26155\",\n                    \"triples\": \"837958\"\n                },\n                {\n                    \"repo\": \"orgs:bco-dmo\",\n                    \"graphs\": \"1\",\n                    \"triples\": \"8\"\n                },\n                {\n                    \"repo\": \"orgs\",\n                    \"graphs\": \"2\",\n                    \"triples\": \"16\"\n                },\n             ]\n        },\n        {\n            \"report\": \"dataset_count\",\n            \"data\": [\n                {\n                    \"datasetcount\": \"41137\"\n                }\n            ]\n        },\n        {\n            \"report\": \"dataset_count_by_repo\",\n            \"data\": [\n                {\n                    \"repo\": \"bco-dmo\",\n                    \"graphs\": \"460\",\n                    \"datasets\": \"998\"\n                },\n                {\n                    \"repo\": \"cchdo\",\n                    \"graphs\": \"2041\",\n                    \"datasets\": \"2880\"\n                },\n                {\n                    \"repo\": \"r2r\",\n                    \"graphs\": \"11104\",\n                    \"datasets\": \"11104\"\n                },\n                {\n                    \"repo\": \"ssdb.iodp\",\n                    \"graphs\": \"26155\",\n                    \"datasets\": \"26155\"\n                }\n            ]\n        },\n        {\n            \"report\": \"mutilple_version_count\",\n            \"data\": [\n                {\n                    \"version\": \"1\",\n                    \"scount\": \"69\"\n                },\n                {\n                    \"version\": \"2\",\n                    \"scount\": \"11\"\n                },\n                {\n                    \"version\": \"09 September 2014\",\n                    \"scount\": \"11\"\n                },\n\n            ]\n        },\n        {\n            \"report\": \"mutilple_version_count_by_repo\",\n            \"data\": [\n                {\n                    \"repo\": \"bco-dmo\",\n                    \"versionscount\": \"259\"\n                }\n            ]\n        },\n        {\n            \"report\": \"keywords_counts_by_repo\",\n            \"data\": [\n                {\n                    \"keyword\": \"ctd\",\n                    \"scount\": \"1808\",\n                    \"repo\": \"cchdo\"\n                },\n                {\n                    \"keyword\": \"bottle\",\n                    \"scount\": \"1146\",\n                    \"repo\": \"cchdo\"\n                },\n                {\n                    \"keyword\": \"Timeseries\",\n                    \"scount\": \"1001\",\n                    \"repo\": \"cchdo\"\n                },\n\n            ]\n        },\n        {\n            \"report\": \"types_count\",\n            \"data\": [\n                {\n                    \"type\": \"https://schema.org/GeoCoordinates\",\n                    \"scount\": \"143170\"\n                },\n                {\n                    \"type\": \"https://schema.org/DataDownload\",\n                    \"scount\": \"57770\"\n                },\n                {\n                    \"type\": \"https://schema.org/Dataset\",\n                    \"scount\": \"41137\"\n                },\n                {\n                    \"type\": \"https://schema.org/Place\",\n                    \"scount\": \"39621\"\n                },\n                {\n                    \"type\": \"https://schema.org/Person\",\n                    \"scount\": \"38268\"\n                },\n                {\n                    \"type\": \"https://schema.org/GeoShape\",\n                    \"scount\": \"35022\"\n                },\n                {\n                    \"type\": \"https://schema.org/CreativeWork\",\n                    \"scount\": \"33312\"\n                },\n                {\n                    \"type\": \"https://schema.org/Organization\",\n                    \"scount\": \"22595\"\n                },\n                {\n                    \"type\": \"https://schema.org/PropertyValue\",\n                    \"scount\": \"17119\"\n                },\n                {\n                    \"type\": \"https://schema.org/DataCatalog\",\n                    \"scount\": \"2520\"\n                },\n                {\n                    \"type\": \"https://schema.org/DigitalDocument\",\n                    \"scount\": \"1095\"\n                },\n                {\n                    \"type\": \"https://schema.org/FundingAgency\",\n                    \"scount\": \"607\"\n                },\n                {\n                    \"type\": \"https://schema.org/MonetaryGrant\",\n                    \"scount\": \"607\"\n                },\n                {\n                    \"type\": \"https://schema.org/ResearchProject\",\n                    \"scount\": \"439\"\n                },\n                {\n                    \"type\": \"http://spdx.org/rdf/terms#Checksum\",\n                    \"scount\": \"282\"\n                },\n                {\n                    \"type\": \"https://schema.org/Event\",\n                    \"scount\": \"200\"\n                },\n                {\n                    \"type\": \"https://schema.org/Service\",\n                    \"scount\": \"5\"\n                },\n                {\n                    \"type\": \"https://schema.org/SearchAction\",\n                    \"scount\": \"4\"\n                },\n                {\n                    \"type\": \"https://schema.org/ServiceChannel\",\n                    \"scount\": \"4\"\n                },\n                {\n                    \"type\": \"https://schema.org/EntryPoint\",\n                    \"scount\": \"3\"\n                },\n                {\n                    \"type\": \"https://schema.org/OfferCatalog\",\n                    \"scount\": \"3\"\n                },\n                {\n                    \"type\": \"https://schema.org/WebSite\",\n                    \"scount\": \"2\"\n                },\n                {\n                    \"type\": \"https://schema.org/PostalAddress\",\n                    \"scount\": \"2\"\n                },\n                {\n                    \"type\": \"https://schema.org/ContactPoint\",\n                    \"scount\": \"1\"\n                },\n                {\n                    \"type\": \"https://schema.org/ImageObject\",\n                    \"scount\": \"1\"\n                },\n                {\n                    \"type\": \"https://schema.org/thing\",\n                    \"scount\": \"1\"\n                },\n                {\n                    \"type\": \"https://schema.org/Thing\",\n                    \"scount\": \"1\"\n                },\n                {\n                    \"type\": \"https://schema.org/WebPage\",\n                    \"scount\": \"1\"\n                }\n            ]\n        },\n        {\n            \"report\": \"keywords_count\",\n            \"data\": [\n                {\n                    \"keyword\": \"ctd\",\n                    \"scount\": \"1808\"\n                },\n                 ]\n        },\n        {\n            \"report\": \"variablename_count\",\n            \"data\": [\n                {\n                    \"variableName\": \"lat\",\n                    \"scount\": \"116\"\n                },\n                {\n                    \"variableName\": \"lon\",\n                    \"scount\": \"112\"\n                },\n                {\n                    \"variableName\": \"year\",\n                    \"scount\": \"57\"\n                },\n                {\n                    \"variableName\": \"date\",\n                    \"scount\": \"56\"\n                },\n                {\n                    \"variableName\": \"depth\",\n                    \"scount\": \"43\"\n                },\n                {\n                    \"variableName\": \"ISO_DateTime_UTC\",\n                    \"scount\": \"38\"\n                },\n                {\n                    \"variableName\": \"day\",\n                    \"scount\": \"37\"\n                },\n                {\n                    \"variableName\": \"time\",\n                    \"scount\": \"34\"\n                },\n                {\n                    \"variableName\": \"temp\",\n                    \"scount\": \"34\"\n                },\n\n\n                {\n                    \"variableName\": \"Latitude\",\n                    \"scount\": \"28\"\n                },\n                {\n                    \"variableName\": \"Longitude\",\n                    \"scount\": \"28\"\n                },\n                {\n                    \"variableName\": \"event\",\n                    \"scount\": \"26\"\n                },\n                {\n                    \"variableName\": \"month\",\n                    \"scount\": \"26\"\n                },\n                {\n                    \"variableName\": \"station\",\n                    \"scount\": \"25\"\n                },\n\n                {\n                    \"variableName\": \"site\",\n                    \"scount\": \"20\"\n                },\n                {\n                    \"variableName\": \"LATITUDE\",\n                    \"scount\": \"19\"\n                },\n                {\n                    \"variableName\": \"LONGITUDE\",\n                    \"scount\": \"19\"\n                },\n                {\n                    \"variableName\": \"time_local\",\n                    \"scount\": \"19\"\n                },\n\n                {\n                    \"variableName\": \"Date\",\n                    \"scount\": \"17\"\n                },\n                {\n                    \"variableName\": \"Station\",\n                    \"scount\": \"16\"\n                },\n                {\n                    \"variableName\": \"Time\",\n                    \"scount\": \"15\"\n                },\n                {\n                    \"variableName\": \"Lat\",\n                    \"scount\": \"15\"\n                },\n                {\n                    \"variableName\": \"Lon\",\n                    \"scount\": \"15\"\n                },\n\n            ]\n        },\n        {\n            \"report\": \"graph_sizes\",\n            \"data\": [\n                {\n                    \"triplesize\": \"31\",\n                    \"triplecount\": \"12181\"\n                },\n                {\n                    \"triplesize\": \"54\",\n                    \"triplecount\": \"6414\"\n                },\n\n                {\n                    \"triplesize\": \"1029\",\n                    \"triplecount\": \"1\"\n                },\n                {\n                    \"triplesize\": \"3\",\n                    \"triplecount\": \"1\"\n                }\n            ]\n        }\n    ]\n}</code></pre>"},{"location":"earthcube_utilities/setup_testing/","title":"testing","text":"<p>To create test data, we will need to setup some test datasets.</p> <p>Sometime we will need to setup an s3 mock to do this.</p> <p>Right now, we can rclone. * bucket testdata is a source DO NOT RUN TESTS OVER TESTDATA * bucker test this is the data that should be run over Rclone only supports a one-time sync of metadata. This means that metadata will be synced from the source object to the destination object only when the source object has changed and needs to be re-uploaded. If the metadata subsequently changes on the source object without changing the object itself then it won't be synced to the destination object. This is in line with the way rclone syncs Content-Type without the --metadata flag.</p> <p>Log onto geocodes-aws-dev.earthcube.org Use rclone to 'sync'  rclone delete awsdev:test/summoned/geocodes_demo_datasets  rclone delete  awsdev:test/summoned/iris  rclone delete  awsdev:test/milled/geocodes_demo_datasets  rclone delete  awsdev:test/milled/iris</p> <p>rclone copy -M  awsdev:testdata/summoned/geocodes_demo_datasets awsdev:test/summoned/geocodes_demo_datasets   rclone copy -M  awsdev:testdata/summoned/geocodes_demo_datasets awsdev:test/summoned/geocodes_demo_datasets_set2  rclone copy -M  awsdev:testdata/summoned/iris awsdev:test/summoned/iris  rclone copy -M  awsdev:testdata/milled/geocodes_demo_datasets awsdev:test/milled/geocodes_demo_datasets  rclone copy -M  awsdev:testdata/milled/iris awsdev:test/milled/iris</p> <p>big clone  rclone copy -M  awsdev:testdata/summoned/magic awsdev:test/summoned/magic</p> <p>rclone copy -M awsdev:testdata/summoned/geocodes_demo_datasets awsdev:test/summoned/geocodes_demo_datasets  rclone copy -M production:gleaner-wf/summoned/iris awsdev:testdata/summoned/iris   rclone copy -M production:gleaner-wf/milled/iris awsdev:testdata/milled/iris earthcube@ip-1</p>"},{"location":"earthcube_utilities/sqarql_notes/","title":"notes on developing new sparql queries","text":"<p>graph: top level subj: https://portal.opentopography.org/raster?opentopoID=OTSDEM.052015.26910.1 urn all triples in graph: 88 <pre><code>SELECT distinct ?s ?p ?o ?g\nWHERE     {\n    GRAPH &lt;urn:gleaner:summoned:opentopography:237ccc9501fc0e0349c8643e61e9db1e82dbd9ac&gt; {?s ?p ?o .\n    bind(&lt;urn:gleaner:summoned:opentopography:237ccc9501fc0e0349c8643e61e9db1e82dbd9ac&gt;   as ?g)\n} }</code></pre> <p>subj_construct with blank nodes: 69</p> <pre><code>CONSTRUCT {?x ?y ?z . ?z ?w ?v } WHERE\n  {\n    &lt;https://portal.opentopography.org/raster?opentopoID=OTSDEM.052015.26910.1&gt;  ?y ?z .\n    BIND(&lt;https://portal.opentopography.org/raster?opentopoID=OTSDEM.052015.26910.1&gt;  as ?x)\n    OPTIONAL {\n      ?z ?w ?v\n      FILTER (isBlank(?z) &amp;&amp; !isBlank(?v))\n     }\n        OPTIONAL {\n      ?z ?w ?v\n      FILTER (!isBlank(?z) )\n    }\n\n}</code></pre> <p>V3 subj_construct with blank nodes: 82 mising the six  identifier triples https://doi.org/10.5069/G9J10130 <pre><code>CONSTRUCT {?x ?y ?z . ?z ?w ?v . ?z ?w1 ?v1 . ?v2 ?w3 ?v3} WHERE\n  {\n    &lt;https://portal.opentopography.org/raster?opentopoID=OTSDEM.052015.26910.1&gt;  ?y ?z .\n    BIND(&lt;https://portal.opentopography.org/raster?opentopoID=OTSDEM.052015.26910.1&gt;  as ?x)\n    OPTIONAL {\n      ?z ?w ?v\n      FILTER (isBlank(?z) &amp;&amp; !isBlank(?v))\n\n     }\n     OPTIONAL {\n      ?z ?w1 ?v1     \n    }\n    OPTIONAL {\n      ?z ?w2 ?v2\n      FILTER (isBlank(?z) &amp;&amp; isBlank(?v2))\n      ?v2 ?w3 ?v3\n     }\n\n}</code></pre></p>"},{"location":"earthcube_utilities/summarize/","title":"Summarize","text":""},{"location":"earthcube_utilities/summarize/#overview","title":"Overview","text":"<p>Because of the lack of standardardization on how Science on Schema is implemented, there are performance issues quering and retrieving information from the blazegraph graphstore. Because JSONLD fields can be strings/objects/arrays asking the graphstore to guess the best optimization for retrieval is haphazard. In order to work around this, we create a materilized view view. Materialization is common performance trick which basically brings all values into a single object so that retreival of what would be normalized information is flattened.  The JSONLD strings/objects/arrays single object to improve search and retrieval performance. </p> <p>Summarize tools create a 'materialized\" view of a repositories JSONLD as triple to improve performance a searches for geocodes. </p> <p>To do this, it reads the named graphs (aka quads) that nabu generates as part of its workflow, then  converts them into flattend triples in  'summary' graph namespace.</p>"},{"location":"earthcube_utilities/summarize/#workflow-for-summarize-a-part-of-qualifying-a-repostiory-for-loading-into-geocodes","title":"workflow for  Summarize a part of qualifying a repostiory for loading into geocodes","text":"<pre><code>sequenceDiagram\n\n    participant summarize_repo.py\n    participant manageGraph\n    participant nabu\n    participant summarize_materializedview.py\n    participant graphstore\n\n\n    summarize_repo.py-&gt;&gt;summarize_repo.py: load nabu configuration file \n    summarize_repo.py-&gt;&gt;manageGraph: call to create managegraph instance for repo_temp \n    summarize_repo.py-&gt;&gt;manageGraph: call to create namespace for repo_temp \n    manageGraph-&gt;&gt;graphstore: creates namespace  for repo_temp\n\n    summarize_repo.py-&gt;&gt;manageGraph: call to create managegraph instance for repo_summary \n    summarize_repo.py-&gt;&gt;manageGraph: call to create namespace for repo_summary\n\n    manageGraph-&gt;&gt;graphstore: creates namespace  for repo_summary, if it does not exist\n    summarize_repo.py-&gt;&gt;summarize_repo.py: create nabu config to load to temp  \n    summarize_repo.py-&gt;&gt;nabu:  execute nabu with modified config file\n    nabu-&gt;&gt;nabu:  reads configuration, files from s3, converts to quads \n    nabu-&gt;&gt;graphstore:  writes quads to repo_temp namespace\n    summarize_repo.py-&gt;&gt;summarize_materializedview.py: call materialize summary\n    graphstore-&gt;&gt;summarize_materializedview.py: query temp namespace in graph store\n    summarize_materializedview.py-&gt;&gt;summarize_materializedview.py: create summary triples\n    summarize_materializedview.py-&gt;&gt;summarize_repo.py: return summary triples\n    summarize_repo.py-&gt;&gt;manageGraph: if graphsummary:  insert summary triples as byte[] to blazegraph\n    manageGraph-&gt;&gt;graphstore: insert ttl to repo_summary namespace\n    summarize_repo.py-&gt;&gt;summarize_repo.py: if not graphsummary: write file repo.ttl\n    summarize_repo.py-&gt;&gt;manageGraph: delete repo_temp namespace\n\n</code></pre>"},{"location":"earthcube_utilities/summarize/#workflow-for-summarizing-from-an-existing-graph-namespace","title":"Workflow for Summarizing from an existing graph namespace","text":"<pre><code>sequenceDiagram\n\n    participant summarize_from_graph_names.py\n    participant manageGraph\n    participant summarize_materializedview.py\n    participant graphstore\n\n\n    summarize_from_graph_names.py-&gt;&gt;summarize_from_graph_names.py: load nabu configuration file \n\n    summarize_from_graph_names.py-&gt;&gt;summarize_materializedview.py: call materialize summary\n    graphstore-&gt;&gt;summarize_materializedview.py: query  namespace in graph store\n    summarize_materializedview.py-&gt;&gt;summarize_materializedview.py: create summary triples\n    summarize_materializedview.py-&gt;&gt;summarize_from_graph_names.py: return summary triples\n    summarize_from_graph_names.py-&gt;&gt;manageGraph: if graphsummary:  insert summary triples as byte[] to blazegraph\n    manageGraph-&gt;&gt;graphstore: insert ttl to repo_summary namespace\n    summarize_from_graph_names.py-&gt;&gt;summarize_from_graph_names.py: if not graphsummary: write file repo.ttl\n\n\n</code></pre>"},{"location":"earthcube_utilities/summarize/#dependencies","title":"Dependencies","text":"<ul> <li>Geocodes Stack</li> <li>glcon</li> <li>python &gt; 3.3</li> <li>the nabu configuration file that was used to load the data to the graphstore</li> </ul>"},{"location":"earthcube_utilities/summarize/#install","title":"INSTALL","text":""},{"location":"earthcube_utilities/summarize/#option-1-doing-developement","title":"option 1: doing developement","text":"<ul> <li>pull repository,<ul> <li><code>git clone https://github.com/earthcube/earthcube_utilities.git</code></li> </ul> </li> <li><code>cd earthcube_utilities/summarize</code></li> <li>install python dependencies<ul> <li><code>pip3 install requirements.txt</code></li> </ul> </li> <li>Run steps below</li> </ul>"},{"location":"earthcube_utilities/summarize/#option-2-use-package","title":"Option 2: use package","text":"<p>The code exists in a testing package <code>python3 -m pip install --index-url https://test.pypi.org/simple/ earthcube_summarize</code></p>"},{"location":"earthcube_utilities/summarize/#approaches","title":"Approaches","text":"<ul> <li>Summarize existing graph stores. AKA: data is loaded into a graph. The can and should be subsetted using  a repository name.</li> <li>Summarize as part of workflow, aka build summary when repo is loaded.    This is useful for onboarding a new repository    it loaded from s3 to a grpah using <code>glcon nabu</code> into a temporary repository,    queries, then pushes results to a summary namespace</li> <li>Build summary for a 'release' graph (coded, but untested/undocumented)</li> </ul>"},{"location":"earthcube_utilities/summarize/#summarize-an-existing-graph-stores","title":"Summarize an existing graph stores.","text":"<p>This can  summarize an entire graph store, or just process a msubset, eg, a recently updated repo.</p> <ul> <li>The information is in an existing blazegraph instance <ul> <li><code>glcon nabu prefix --cfgName {configdir}</code> </li> </ul> </li> <li>There exists a  summary namespace in a blazegraph instance</li> </ul> <pre><code>usage: summarize_from_graph_namespace.py [-h] [--repo REPO] --graphendpoint GRAPHENDPOINT [--graphsummary GRAPHSUMMARY] [--summary_namespace SUMMARY_NAMESPACE]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --repo REPO           repo name used in the urn\n  --graphendpoint GRAPHENDPOINT\n                        graph endpoint with namespace\n  --graphsummary GRAPHSUMMARY\n                        upload triples to graphsummary\n  --summary_namespace SUMMARY_NAMESPACE\n                        summary_namespace defaults to {repo_summary}\n</code></pre>"},{"location":"earthcube_utilities/summarize/#run-summarize_from_graph_namespace","title":"run summarize_from_graph_namespace","text":"<ol> <li>if you have not, changed to the summarize directory: <code>cd  earthcube_utilities/summarize</code></li> <li>run</li> </ol> <p>option 2 script installed <pre><code>summarize_from_graph --repo {repo} --graphendpoint {endppiont} --summary_namespace {earthcube_summary}\n</code></pre></p> <p>option1 from repository <pre><code>./src/ec_summarize/summarize_from_graph_namespace.py --repo {repo} --graphendpoint {endppiont} --summary_namespace {earthcube_summary}\n</code></pre> repo is optional. Without the  <code>--repo</code> the code will summarize all information</p>"},{"location":"earthcube_utilities/summarize/#summarize-a-part-of-qualifying-a-repostiory-for-loading-into-geocodes","title":"Summarize a part of qualifying a repostiory for loading into geocodes","text":"<p>As part of the qualifying of a repository for loading we want to load the data into separate instances This codebase will read a gleaner configuraiton, push the summoned information to a repository, and create a summary repository</p> <pre><code>usage: summarize_repo.py [-h] [--graphendpoint GRAPHENDPOINT] [--glcon GLCON] [--graphsummary GRAPHSUMMARY] [--keeptemp GRAPHTEMP]\n                         [--summary_namespace SUMMARY_NAMESPACE]\n                         repo nabufile\n\npositional arguments:\n  repo                  repository name\n  nabufile              nabu configuration file\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --graphendpoint GRAPHENDPOINT\n                        override nabu endpoint\n  --glcon GLCON         override path to glcon\n  --graphsummary GRAPHSUMMARY\n                        upload triples to graphsummary\n  --keeptemp GRAPHTEMP  do not delete the temp namespace. a namespace {repo}_temp will be created\n  --summary_namespace SUMMARY_NAMESPACE\n                        summary_namespace defaults to {repo}_temp_summary\n</code></pre>"},{"location":"earthcube_utilities/summarize/#run-summarize_repo","title":"run summarize_repo","text":"<p>option 2 script installed <pre><code>summarize_from_repo {repo} {path_to_nabu_config_file} --graphendpoint {endppiont} --summary_namespace {{repo}_temp_summary}\n</code></pre></p> <p>option1 from repository <pre><code>./src/ec_summarize/summarize_repo.py {repo} {path_to_nabu_config_file} --graphendpoint {endppiont} --summary_namespace {{repo}_temp_summary}\n</code></pre></p>"},{"location":"earthcube_utilities/summarize/#developement","title":"Developement","text":"<p>create a virutal env</p> <p><code>source {envname}/bin/activate</code></p> <p>use editable install</p> <pre><code>cd summarize\npip3 install -e .</code></pre> <p>If you edit the pyproject.toml and want to test an added script,  <pre><code>cd summarize\npip3 uninstall -e earthcube-summarize\npip3 install -e .</code></pre></p>"},{"location":"earthcube_utilities/design/breakdown/","title":"earthcube_utliites wiki","text":"<p>Originally used for notebooks only, but getting more reuse in the rest of the workflow  this lead to a natural breakdown of submodules, driven by need, in the processing pipeline</p> <pre><code>flowchart TD;\nEC[ec.py] -- broke_out --&gt; T[testing:1475];\nEC -- leaving --&gt; ec2[ec main:2248];\nec2 -- back2early --&gt; U[earthcube_utilities:518]  -- has --&gt; R[a bit for opening data in notebooks];\nU -- loads --&gt; TR[the rest]\nTR -- import --&gt; MB[min-base:690];\nTR -- import --&gt; query[SPARQL queries from git:175];\nTR -- import --&gt; R2[rdf2nq:146];\nU -- could_load --&gt; TR2[more];\nTR2 ----&gt; RD[related datasets:145];\nTR2 ----&gt; RC[RO-CRATE creation:107];\nTR2 -- leaving --&gt; o[other:718]\n</code></pre> <p>This is more how the functionality has been broken down, to help with the ec_utils plan, a more simple view can be further devloped to  better show a potiential user about the Functionality of the  libraries modules</p>"},{"location":"earthcube_utilities/design/breakdown/#earthcube_utilities-breakdown","title":"earthcube_utilities breakdown","text":"<p>the file itself can keep some of the notebook specific code or that can go into a notebook/data_download sub modules</p> <p>If it can be that is good, but this is about clean coding. So how does importing a single undocumented  module help a user utilize it.</p> <p>Functionality: * mini-base * query * rdf to triples * RO-Crate * Additional Modules</p>"},{"location":"earthcube_utilities/design/breakdown/#minibase","title":"Minibase","text":"<p>mb.py mini-base small util functions that sometimes end up getting copied into the other places they are needed now  some of this could be replaced by libs like: pydash</p>"},{"location":"earthcube_utilities/design/breakdown/#query","title":"Query","text":"<p>query.py  is can do all the SPARQL queries the UI can do.</p> <p>Query returns a pandas dataframe using the sparqldataframe library. A set of queries based on the the facetserch UI sparql queries and other queries utilized for data exploration and data validation are included.</p> <p>LIST QUERIES: This is the detailed functionality</p> <p>(I am going to suggest that a list of queries, and functionality, and some details be incorporated into an assets list. The list should probably be a file in YAML or JSON format in the resources directory.)</p> short_name file detailed description summary ./resources/sparql/summary.txt takes query term, <code>q</code>, and returns a summary of the matched resoruces sbj2urn ./resources/sparql/sbj2urn.txt returns the urn of the graph  for a given graph <code>g</code> <p>This is the proposed implementation stratedgy...  it is setup to add one get_{qry_name}_txt  function to get the txt of the query, usually from raw git  then a function: {qry_name} that calls one fuction with {qry_name} as the arg, and maybe a variable it will get the txt from the 1st function, and replace the var w/in the template txt, run the query and return a DF</p> <p>(above  reads  like a possible security hole I am going to run a method with a dynamically defined name. Implementation seems overly complex, </p> <p>Suggest that queries be embedded in resources. which would make things clearer than many courtesy methods. something simpler, like:</p> <pre><code>def QUERY(endpoint)\n# validate that endpoint exists.\ndef listQueries() \n# return list of queries\ndef query(self, short_name=summary, options)\n# where options would be an object containing passed parameters {\"q\":\"steens\"}\ndef queryFromURL(self, url=\"https://raw.githubusercontent.com/MBcode/ec/master/NoteBook/sparql-query.txt\", options)</code></pre> <pre><code>q = QUERY(\"http:example.com\")\ndataframe = q.query(\"sub2urn\",options={\"g\":\"somegraph\"} )\ndataframe2 = q.queryFromURL( url=\"https://raw.githubusercontent.com/MBcode/ec/master/NoteBook/sparql-query.txt\", options={\"q\":\"steens\"})\n</code></pre>"},{"location":"earthcube_utilities/design/breakdown/#usages","title":"Usages:","text":"<p>many.  * It can be used to get information to validated informaiton from the graph, * it can be used in notebooks to run queries.</p> <p>Also, suggest jinja2, or the standard python3 templates for replacing parameters in templates.</p>"},{"location":"earthcube_utilities/design/breakdown/#rdf-to-triples","title":"RDF to triples","text":"<p>At present, gleaner generates triples, and nabu loads them into the graph as quads. The summary code needs to know the expected <code>graph</code> so this module creates quads from gleaner/milled Basically, rdf2nq.py takes one form of rdf triples,  and adds the filename of the file as the last column in its nquads output * if it is .ntriples, then you just add a column * if it is another format like jsonld, then it runs jena's riot RDF I/O technology (RIOT) on it, right now <pre><code>rdf2nq.py nabuconfig repository --path (default=milled)</code></pre> The config contains the service secrets, and more importantly the source 'repository' will be in the file. <pre><code>def s3client(config)\ndef s3GetFile(s3client, file )\ndef read_files(s3client, bucket, repo)\ndef triplesToQuads(triples, graphname)\ndef appeandToOutput(quads)\ndef pathToUrn(file) \n   # the path should be summoned/repo/urn.jsonld\n   # remove json or jsonld from s3 path\n   # returns urn in for urn:repo:file sans extenstion\ndef __main__\n  s3 = s3Client(config)\n  files = read_files(s3, config.s3.bucket, repo)\n  foreach file in files:\n     urn = pathToUrn(file)\n     data = s3GetFile(s3client, file.path) # whatever it needs\n     quads = triplesToQuads(data, urn)\n     appendToOutput(quads)\n  outputfile.write() </code></pre> <p>Some related could probably be handled by kglab now</p>"},{"location":"earthcube_utilities/design/breakdown/#ro-crate-creation","title":"RO-CRATE creation","text":"<p>RO-Crate issue decribes some of the steps needed</p> <p>The Notebook Dispatch describes the possible functionality </p> <p>The Decoder Sprint Document includes a diagram of how we might immplement RO-Crate workflows</p> <pre><code>def s3client() # public bucket\ndef s3getFile(s3client, path to rocrate)\ndef s3PathToRoctate(roCrateQueryParameter)\ndef roCrateRender(rocrate)\n</code></pre> <pre><code>roCrateQueryParameter = request.params.rocrate\ns3 = s3client()\npathToCrate = s3PathToRocrate(roCrateQueryParameter)\nrocrate = s3getFile(s3,pathToCrate)\nprint(roCrateRender(rocrate))</code></pre>"},{"location":"earthcube_utilities/design/breakdown/#misc-notes","title":"Misc notes:","text":"<p>could be integrated with the new gleaner-logging.worklfow.</p>"},{"location":"earthcube_utilities/design/breakdown/#could-be-loading","title":"Could be loading:","text":""},{"location":"earthcube_utilities/design/breakdown/#related-data-using-sklearn-here","title":"related-data using sklearn here","text":""},{"location":"earthcube_utilities/design/breakdown/#ro-crate-metadata-creation-that-dv-asked-for","title":"RO-CRATE metadata creation, that dv asked for","text":""},{"location":"earthcube_utilities/design/ec_query/","title":"Ec query","text":""},{"location":"earthcube_utilities/design/ec_query/#earthcube-query","title":"Earthcube Query","text":"<p>Implemented in ec/graph</p> <p>manageGraph/ManageBlazegraph</p> <p>query.py  is can do all the SPARQL queries the UI can do.</p> <p>Query returns a pandas dataframe using the sparqldataframe library. A set of queries based on the the facetserch UI sparql queries and other queries utilized for data exploration and data validation are included.</p> <p>LIST QUERIES: This is the detailed functionality</p> <p>(I am going to suggest that a list of queries, and functionality, and some details be incorporated into an assets list. The list should probably be a file in YAML or JSON format in the resources directory.)</p> short_name file detailed description summary ./resources/sparql/summary.txt takes query term, <code>q</code>, and returns a summary of the matched resoruces sbj2urn ./resources/sparql/sbj2urn.txt returns the urn of the graph  for a given graph <code>g</code> <p>This is the proposed implementation stratedgy...  it is setup to add one get_{qry_name}_txt  function to get the txt of the query, usually from raw git  then a function: {qry_name} that calls one fuction with {qry_name} as the arg, and maybe a variable it will get the txt from the 1st function, and replace the var w/in the template txt, run the query and return a DF</p> <p>(above  reads  like a possible security hole I am going to run a method with a dynamically defined name. Implementation seems overly complex, </p> <p>Suggest that queries be embedded in resources. which would make things clearer than many courtesy methods. something simpler, like:</p> <pre><code>def QUERY(endpoint)\n# validate that endpoint exists.\ndef listQueries() \n# return list of queries\ndef query(self, short_name=summary, options)\n# where options would be an object containing passed parameters {\"q\":\"steens\"}\ndef queryFromURL(self, url=\"https://raw.githubusercontent.com/MBcode/ec/master/NoteBook/sparql-query.txt\", options)</code></pre> <pre><code>q = QUERY(\"http:example.com\")\ndataframe = q.query(\"sub2urn\",options={\"g\":\"somegraph\"} )\ndataframe2 = q.queryFromURL( url=\"https://raw.githubusercontent.com/MBcode/ec/master/NoteBook/sparql-query.txt\", options={\"q\":\"steens\"})\n</code></pre>"},{"location":"earthcube_utilities/design/ec_query/#usages","title":"Usages:","text":"<p>many.  * It can be used to get information to validated informaiton from the graph, * it can be used in notebooks to run queries.</p> <p>Also, suggest jinja2, or the standard python3 templates for replacing parameters in templates.</p>"},{"location":"earthcube_utilities/design/mini-base/","title":"Minibase","text":"<p>mb.py mini-base small util functions that sometimes end up getting copied into the other places they are needed now  some of this could be replaced by libs like: pydash</p> <p>Functionality:</p>"},{"location":"earthcube_utilities/design/rdf2triples/","title":"Rdf2triples","text":""},{"location":"earthcube_utilities/design/rdf2triples/#rdf-to-triples","title":"RDF to triples","text":"<p>working: earthcube_utilities/earthcube_utilities/src/ec/sos_json/rdf.py</p>"},{"location":"earthcube_utilities/design/rdf2triples/#original-spec","title":"original spec","text":"<p>At present, gleaner generates triples, and nabu loads them into the graph as quads. The summary code needs to know the expected <code>graph</code> so this module creates quads from gleaner/milled Basically, rdf2nq.py takes one form of rdf triples,  and adds the filename of the file as the last column in its nquads output * if it is .ntriples, then you just add a column * if it is another format like jsonld, then it runs jena's riot RDF I/O technology (RIOT) on it, right now <pre><code>rdf2nq.py nabuconfig repository --path (default=milled)</code></pre> The config contains the service secrets, and more importantly the source 'repository' will be in the file. <pre><code>def s3client(config)\ndef s3GetFile(s3client, file )\ndef read_files(s3client, bucket, repo)\ndef triplesToQuads(triples, graphname)\ndef appeandToOutput(quads)\ndef pathToUrn(file) \n   # the path should be summoned/repo/urn.jsonld\n   # remove json or jsonld from s3 path\n   # returns urn in for urn:repo:file sans extenstion\ndef __main__\n  s3 = s3Client(config)\n  files = read_files(s3, config.s3.bucket, repo)\n  foreach file in files:\n     urn = pathToUrn(file)\n     data = s3GetFile(s3client, file.path) # whatever it needs\n     quads = triplesToQuads(data, urn)\n     appendToOutput(quads)\n  outputfile.write() </code></pre> <p>Some related could probably be handled by kglab now</p>"},{"location":"earthcube_utilities/design/repository_reporting/","title":"Reporting to repositories","text":"<p>This can happen with queries that can be run. </p> <p>earthcube_utilities/src/ec/reporting</p> <p><code>queryWithSparql( template_name, endpoint,parameters={}):</code></p>"},{"location":"earthcube_utilities/design/repository_reporting/#research","title":"Research","text":"<p>https://github.com/gleanerio/notebooks/tree/master/notebooks/validation</p> <p>Some testing may be mostly standardized can we do a pattern like Approval Testing of Repository Information</p> <p>Old mockup</p> <p>Need papermill code to autogenerate a report.</p> <p>There is a sitemap assay notebook. Is this useful?</p>"},{"location":"earthcube_utilities/design/ro_crate/","title":"Ro crate","text":""},{"location":"earthcube_utilities/design/ro_crate/#ro-crate-creation","title":"RO-CRATE creation","text":"<p>RO-Crate issue decribes some of the steps needed</p> <p>Development happening in: https://github.com/earthcube/earthcube_utilities/tree/dev_16_nb</p> <p>The Notebook Dispatch describes the possible functionality </p> <p>The Decoder Sprint Document includes a diagram of how we might immplement RO-Crate workflows</p> <pre><code>def s3client(): # public bucket\ndef s3getFile(s3client, pathToCrate):\ndef s3PathToRoctate(roCrateQueryParameter):\ndef roCrateRender(rocrate):\ndef roCrateGetCollection(rocrate)\ndef roCrateGetObjectFromJsonLdStore(s3clinet, collectionItem)\ndef roCrateGetObject(graphstore, collectionItem)</code></pre> <pre><code>roCrateQueryParameter = request.params.rocrate\ns3 = s3client()\npathToCrate = s3PathToRocrate(roCrateQueryParameter)\nrocrate = s3getFile(s3,pathToCrate)\nprint(roCrateRender(rocrate))</code></pre>"},{"location":"earthcube_utilities/design/sos_jsonld_utils/","title":"utlities to manipulate Science on Schema JSONLD","text":"<p>implemented in sos_json/utils</p> <p>the basic info and distibutions should be implemented in objects.</p>"},{"location":"earthcube_utilities/design/sos_jsonld_utils/#functionality","title":"functionality","text":""},{"location":"earthcube_utilities/design/sos_jsonld_utils/#methods","title":"Methods","text":""},{"location":"earthcube_utilities/design/sos_jsonld_utils/#manipulate-jsonld","title":"manipulate jsonld","text":"<pre><code>def validJson(jsonld):\ndef validToSpec(jsonld, spec=\"ec_sos\"):\ndef basicInfo(jsonld): # name, identifier\ndef distributions(jsonld, formats=[\"all\"]): # return a link</code></pre>"},{"location":"earthcube_utilities/design/sos_jsonld_utils/#from-graphstore","title":"From Graphstore:","text":"<p>is this a good idea <pre><code>class EC_Graphstore:\n    def __init__(self, config):\n    def getObject(self, urn): # object as defined by gleanerio, aka loaded rdf\n    def getDistributions(self, urn, format=[\"all\"]):\n    def getIdentifiers(self, urn, type=None): # none return all\ndef validToSpec(jsonld, spec=\"ec_sos\"):\ndef basicInfo(jsonld): # name, identifier\ndef distributions(jsonld, formats=[\"all\"]): # return a link</code></pre></p>"},{"location":"earthcube_utilities/design/submodules/","title":"Submodules","text":""},{"location":"earthcube_utilities/design/submodules/#earthcube_utliites","title":"earthcube_utliites","text":""},{"location":"earthcube_utilities/design/submodules/#originally-used-for-notebooks-only-but-getting-more-reuse-in-the-rest-of-the-workflow","title":"originally used for notebooks only, but getting more reuse in the rest of the workflow","text":""},{"location":"earthcube_utilities/design/submodules/#this-has-caused-a-natural-breakdown-of-sub-modules-that-can-be-used-as-needed","title":"this has caused a natural breakdown of Sub-Modules, that can be used as needed","text":"<pre><code>flowchart TD;\nU[earthcube_utilities]  -- has --&gt; R[a bit for opening data in notebooks];\nU -- loads --&gt; TR[the rest]\nTR -- import --&gt; MB[min-base];\nTR -- import --&gt; query[SPARQL queries from git];\nTR -- import --&gt; R2[rdf2nq];\nU -- could_load --&gt; TR2[more];\nTR2 ----&gt; RD[related-datasets];\nTR2 ----&gt; RC[RO-CRATE creation];\n</code></pre>"},{"location":"earthcube_utilities/design/submodules/#earthcube_utilities-breakdown","title":"earthcube_utilities breakdown","text":""},{"location":"earthcube_utilities/design/submodules/#the-file-itself-can-keep-some-of-the-notebook-specific-code-or-that-can-go-into-a-notebookdata_download-sub-modules","title":"the file itself can keep some of the notebook specific code or that can go into a notebook/data_download sub modules","text":""},{"location":"earthcube_utilities/design/submodules/#mbpy-mini-base-small-util-functions-that-sometimes-end-up-getting-copied-into-the-other-places-they-are-needed-now","title":"mb.py mini-base small util functions that sometimes end up getting copied into the other places they are needed now","text":""},{"location":"earthcube_utilities/design/submodules/#some-of-this-could-be-replaced-by-libs-like-pydash","title":"some of this could be replaced by libs like: pydash","text":""},{"location":"earthcube_utilities/design/submodules/#querypy-is-can-do-all-the-sparql-queries-the-ui-can-do","title":"query.py is can do all the SPARQL queries the UI can do","text":""},{"location":"earthcube_utilities/design/submodules/#it-is-setup-to-add-one-get_qry_name_txt-function-to-get-the-txt-of-the-query-usually-from-raw-git","title":"it is setup to add one get_{qry_name}_txt  function to get the txt of the query, usually from raw git","text":""},{"location":"earthcube_utilities/design/submodules/#then-a-function-qry_name-that-calls-one-fuction-with-qry_name-as-the-arg-and-maybe-a-variable","title":"then a function: {qry_name} that calls one fuction with {qry_name} as the arg, and maybe a variable","text":""},{"location":"earthcube_utilities/design/submodules/#it-will-get-the-txt-from-the-1st-function-and-replace-the-var-win-the-template-txt-run-the-query-and-return-a-df","title":"it will get the txt from the 1st function, and replace the var w/in the template txt, run the query and return a DF","text":""},{"location":"earthcube_utilities/design/submodules/#rdf2nqpy-takes-one-form-of-rdf-triples-and-adds-the-filename-of-the-file-as-the-last-column-in-its-nquads-output","title":"rdf2nq.py takes one form of rdf triples, and adds the filename of the file as the last column in its nquads output","text":""},{"location":"earthcube_utilities/design/submodules/#if-it-is-ntriples-then-you-just-add-a-column","title":"if it is .ntriples, then you just add a column","text":""},{"location":"earthcube_utilities/design/submodules/#if-it-is-another-format-like-jsonld-then-it-runs-jenas-riot-rdf-io-technology-riot-on-it-right-now","title":"if it is another format like jsonld, then it runs jena's riot RDF I/O technology (RIOT) on it, right now","text":""},{"location":"earthcube_utilities/design/submodules/#some-related-could-probably-be-handled-by-kglab-now","title":"Some related could probably be handled by kglab now","text":""},{"location":"earthcube_utilities/design/submodules/#there-is-more-that-im-working-on-the-grouping-now","title":"There is more that I'm working on the grouping now","text":""},{"location":"earthcube_utilities/design/submodules/#to-include-some-high-level-descriptions-of-possible-groupingsoffunctionality","title":"to include some high level descriptions of possible groupings/(of)functionality","text":""},{"location":"earthcube_utilities/design/submodules/#and-there-is-the-original-third-sectioned-off-for-the-old-testing-that-parts-of-can-integrated-wthe-new-logging","title":"and there is the original third sectioned off for the old testing, that parts of can integrated w/the new logging..","text":""},{"location":"earthcube_utilities/design/submodules/#could-be-loading","title":"Could be loading:","text":""},{"location":"earthcube_utilities/design/submodules/#related-data-using-sklearn-here","title":"related-data using sklearn here","text":""},{"location":"earthcube_utilities/design/submodules/#ro-crate-creation-that-dv-asked-for","title":"RO-CRATE creation, that dv asked for","text":""},{"location":"earthcube_utilities/design/validate_distribution_links/","title":"Distribution Links","text":"<p>At some point, we want to know that we can get data and manipulate data</p> <p>we need to check to see that if we can directly access files, and if not, then we may need to work with repositories on  * improving links * proxying data access</p>"},{"location":"earthcube_utilities/design/validate_distribution_links/#methods","title":"Methods","text":"<pre><code>def isDistDirectAccess(): # need to learn how to guess this\ndef isDistCloudStorage():\ndef distPing(url): # does dataset page exist\ndef isAccessViaServices(): # not everything is a distribution\n</code></pre>"},{"location":"earthcube_utilities/design/validation_jsonld/","title":"Validation jsonld","text":"<p>https://earthcube.github.io/earthcube_utilities/earthcube_utilities/earthcube_utilities_code/#ec.sos_json.utils</p>"},{"location":"earthcube_utilities/design/validation_overall/","title":"Validation Overview","text":"<p>There are several parts to the system that need to be validated * Sitemap   * sitemap exist   * can we access the urls in the sitemap   * Do documents we 'summoned' have jsonld   * counts      * how many url's in sitemap      * how many not 200. * JSONLD   * Is jsonld valid json   * does it have correct context, etc (this is presently fixed by gleaner but needs to be reported, somehow)   * Is it a valid JSONLD Dataset? Use a shacl shape to test.   * Counts      * how many jsonld files made in into s3 * Graph  Converting to RDF and loading to graph * Did jsonld convert in ways we expected? This is tough, but we need to try to find a quick measure</p> <p>There also needs to be an implementaiton set of tests that works with a small set of known  data that we evaluation and know when something unexpected happens. We we get a quirky jsonld. we need to add it to the test dataset.</p>"},{"location":"earthcube_utilities/design/validation_overall/#some-methods","title":"Some methods:","text":"<p>This is just some overall brain dump of possible methods that might be useful. There should be more details in each validation document.</p>"},{"location":"earthcube_utilities/design/validation_overall/#dataloading","title":"Dataloading","text":"<p>When gleaner pushes data into the s3 instance, then an identifier is created</p> <p><code>urn:repo:id</code></p> <p>Previous patterns included the bucket and the we avoid using slashes and hashes because that can be problematic with url parsing</p> <p><code>urn:bucket:path:repo:id</code></p> <p>eg</p> <p><code>urn:bucket:summoned:repo:id</code></p>"},{"location":"earthcube_utilities/design/validation_overall/#methods","title":"Methods","text":"<pre><code>def parseLogForErrors(file):</code></pre>"},{"location":"earthcube_utilities/design/validation_overall/#sitemaps","title":"Sitemaps","text":"<pre><code>class SiteMap():\n  def __init__(self, url):\n  def validSiteMap():\n  def siteMapCount():\n  def testUrls(sample=0): # 0 = all\n    return {\"count\": 0, \"status\":{\"200\":0, \"400\":0}}</code></pre>"},{"location":"earthcube_utilities/design/validation_overall/#s3","title":"s3","text":"<p><pre><code>class S3client():\n  def __init__(self, config):\n  def s3GetFile(self, file ):\n  def s3CountFilesInRepo(self, repo, base='summoned'):\n  def s3UrnFromPath(filepath):\n  def s3RepoFromPath(filepath):\n  def subSampleFileList(self, random=False, subsample=0): # return a subset \n  def  uploadFile(self, file, contenttype, etc):</code></pre> Do we want to classify the s3 functionality, and be able to use self?</p>"},{"location":"earthcube_utilities/design/validation_overall/#jsonld","title":"JSONLD","text":""},{"location":"earthcube_utilities/design/validation_overall/#methods_1","title":"Methods","text":"<pre><code>def repoCountJson(s3client, repo, base='summoned'): # courtsey function fo s3CountFilesInRepo\ndef possibleMissingJsonLoad(s3client, sitemap, repo):\ndef jsonldFormatGuess(file):  #is is expaded, flattend, compact\ndef validateJson( file):\ndef validatedContext( file):\ndef validateJsonSchema(file, schema=\"sos\":\ndef validateUsingShacl( file, shacl=\"sos_earthcube\"):</code></pre> <p>Need a listing of shacl shapes</p>"},{"location":"earthcube_utilities/design/validation_overall/#graph","title":"Graph","text":"<pre><code>class manageGraph():\n  def __init__(self, graphurl, namespace ):\n  def countGraphs(self, repo=None): # if not give entire count, otherwise fancy search for urn:repo\n  def getItem(self, urn):\n  def getItemsStats(self, urn): # return line count, and other information.\n  def compareGraph2s3Jsonstore(self, s3config, repo):  # nabu prune has example\n</code></pre>"},{"location":"earthcube_utilities/design/validation_overall/#save-to","title":"Save to","text":"<p>code to save results to someplace we can keep track of them.</p>"},{"location":"earthcube_utilities/design/validation_overall/#quick-validation","title":"Quick validation.","text":"<p>do numbers match</p> <p>jsonld&gt;graph count</p> <p>Store counts in github/repository/s3</p> <p>did counts change?</p> Sitemap JSON-LD Count Milled Count from s3 Graph Count from Triplestore count {sitemap:count} {summon:count} {nabu: milled} {graph: count} errors {sitemap:errors} {summon:errors} {nabu: errors} {graph: errors} loss"},{"location":"earthcube_utilities/design/validation_sitemap/","title":"Validating sitemaps","text":"<p>https://earthcube.github.io/earthcube_utilities/earthcube_utilities/earthcube_utilities_code/#ec.sitemap.sitemap</p> <p>There is a tool in development for validating sitemaps with a github action.</p> <p>workflow</p> <p>example action</p>"},{"location":"earthcube_utilities/design/validation_sitemap/#research","title":"research","text":"<p>are there other more complete libraries that might be integrated? There is a sitemap assay notebook. Is this useful?</p>"},{"location":"earthcube_utilities/design/validation_sitemap/#steps-that-will-be-needed","title":"Steps that will be needed:","text":"<ul> <li>extract code from oih to a gleanerio repository<ul> <li>fork into earthcube</li> </ul> </li> <li>test</li> <li> <p>write first documentation</p> </li> <li> <p>Add:</p> </li> <li>are the url's unique</li> <li>implement full ping on all urls, multithead if it's allowed in an action.</li> <li>Implement ability to trigger an email to a repository holder or at least a EC developer when <ul> <li>sitemap no longer exists (404)</li> <li>count of JSONLD extracted, or 404 errors changes significantly.</li> </ul> </li> <li>command line like</li> </ul>"},{"location":"earthcube_utilities/design/validation_sitemap/#functionality","title":"Functionality:","text":"<ul> <li>does sitemap exist (daily)</li> <li>send email (do we want this, can it be a github user name?)</li> <li>multithreaded check of url in sitemap (weekly or monthly?)</li> </ul>"},{"location":"earthcube_utilities/design/validation_test_data/","title":"Validate using test data","text":"<p>Test Level 1 and level 2, WORKING This is incorporated in the https://github.com/earthcube/GeoCODES-Metadata</p> <p>They utilize Approval testing,  if a result differs, then alert and ask if this is ok. If there is no change, then test passes.</p> <p>If a test fails look in integration_testing/approved_files for a file with a .received.txt extenstion look at it, rename it to approved if it looks good.</p> <p>Since the JSON testing is only line counts for now, you may need to debug to see the output.</p>"},{"location":"earthcube_utilities/design/validation_test_data/#approval-tests","title":"Approval Tests:","text":"<p>Logic, after files are approved, they should not change, so testing can be automated. using approvals tests approach</p>"},{"location":"earthcube_utilities/design/validation_test_data/#note","title":"NOTE","text":"<p>This is in the GeocodesMetadata reop, rather than earthcube utilities closer to the metadata is a good thing.</p> <p>Also, that would mean that a small portion of the EC utilities is packaged and documented. * graph (graph directory from  to  ec_utils) * datastore </p>"},{"location":"earthcube_utilities/design/validation_test_data/#test-1-basic-loading","title":"Test 1: basic loading","text":"<ul> <li>pre-setup</li> <li>create a config file, sans secrets. store.</li> <li>pass glcon env variables for secrets. (put in github, also)</li> <li>Test setup</li> <li>test  code creates a temp bucket </li> <li>test  code create a temp graph  (summary does this)</li> <li>Test code executes glcon command to run gleaner, and nabu using the </li> <li>testing</li> <li>Approvals Summon:<ul> <li>count in {bucket}/summoned/{repo}</li> </ul> </li> <li>Approvals JSONLD:<ul> <li>for each file in bucket, </li> <li>is file valid json,</li> <li>Approval of JSON LD loaded to s3</li> </ul> </li> <li>Approvals Graph<ul> <li>for each file {bucket}/milled/{repo}</li> <li>is file non-zero length</li> <li>Approval. Does this look like the loaded triples match the JSONLD</li> </ul> </li> <li>Approvals Nabu<ul> <li>does the count of triples equal the last time run</li> <li>for each urn (aka file in bucket) </li> <li>does the triples count for the graph match the count in the {bucket}/milled/{repo}<ul> <li>do we realy need to test to see they match if the counts are equal?</li> </ul> </li> </ul> </li> </ul>"},{"location":"earthcube_utilities/design/validation_test_data/#testing-2-test-for-duplicate-loads-and-pruning","title":"Testing 2, test for duplicate loads, and pruning.","text":"<p>This might be just added as a test with an additional load * Test setup   * test  code creates a temp bucket    * test  code create a temp graph  (summary does this)   * Test code executes glcon command to run gleaner, and nabu using the    * test code loads to nabu a SECOND time * testing   * Approvals Nabu     * Did the count of triples increase from the original load.         *  If so, fail. just use an approval to do this. run count if it changed, bad.     * for each urn (aka file in bucket)         * does the triples count for the graph match the count in the {bucket}/milled/{repo}          * do we realy need to test to see they match if the counts are equal?</p>"},{"location":"earthcube_utilities/design/validation_test_data/#has-the-config-file-changed","title":"Has the config file changed","text":"<p>except for the secrets, has the config file changed. This is just heads up warning</p>"},{"location":"earthcube_utilities/design/validation_test_data/#does-the-counts-match","title":"does the counts match","text":"<p>run count, capture to approval test. Should only fail when a file is added.</p>"},{"location":"earthcube_utilities/design/validation_test_data/#does-jsonld-jsonld","title":"Does JSONLD == JSONLD","text":"<p>does the JSONLD uploaded to s3 equal the last approval test result.  There may be changes over time as we refine the conversion... but that it what approval tests are for</p>"},{"location":"earthcube_utilities/design/validation_test_data/#does-the-graph-graph","title":"Does the Graph == graph","text":"<p>Does the uploaded and downloaded triples match</p>"},{"location":"earthcube_utilities/design/validation_test_data/#has-the-shacl-validation-result-changed-future","title":"Has the SHACL Validation result changed (future)","text":"<p>Capture SHACL Validation, and see if it has changed.</p>"},{"location":"earthcube_utilities/design/viz/","title":"visualizations for notebooks?","text":"<p>Do we need this, or is it rolled into some other part</p> <p>Some of the rdf functions (even in the much smaller earthcube_utilities) has a viz() function that will produce a graphviz plot of all the rdf that has been processes</p> <p>Other viz of earth-science data (v metadata) would be done by viz libs off the DFs</p>"},{"location":"notebook_proxy/","title":"Make Notebook Proxy","text":"<p>This proxy creates a notebook in Google Collab from a template. The parameters extracted from the Scicence on Schema JSON-LD files, and sent to the proxy /mknb</p> <p>For testing, running locally works well.</p> <p>In production, a container is utilized. This is created by a github workflow, (containerize.yaml)</p>"},{"location":"notebook_proxy/#calling-the-notebook-proxy","title":"Calling the Notebook Proxy","text":""},{"location":"notebook_proxy/#index","title":"Index","text":"<p>http://localhost:3031/ should return the index.html</p>"},{"location":"notebook_proxy/#is-it-alive","title":"is it alive","text":"<p>http://localhost:3031/alive/ will let you know it's working.</p>"},{"location":"notebook_proxy/#generate-a-notebook","title":"generate a notebook","text":"<p>http://localhost:3031/mknb/ This proxy creates a notebook in Google Collab from a template. The parameters extracted from the Scicence on Schema JSON-LD files, and sent to the proxy /mknb * url - url of the resource * ext - encoding format of the resource. * urn - urn geocodes urn.  * template -- name of the template file to render. (optional: default: template.ipynb)</p> <p>Note about url, urns, ext: A hash in the name of the item causes the rest of the url passed to be ignored. To avoid this, we custom encode '#' as &lt;hash&gt;</p> <p>so:</p> <p><code>ext=http://linked.earth/ontology/core/1.2.0/index-en.html#Dataset</code></p> <p>becomes:</p> <p><code>ext=http://linked.earth/ontology/core/1.2.0/index-en.html&lt;hash&gt;Dataset</code></p>"},{"location":"notebook_proxy/#generate-jsonld-from-a-graphstore","title":"generate jsonld from  a graphstore","text":"<p>A call like http://localhost:3031/get_graph/urn:gleaner:summoned:opentopography:0024e35144d902d8b413ffd400ede6a27efe2146/json</p> <p>several forms can be returned</p> <p>http://localhost:3031/get_graph/{URN}/jsonld</p> <p>http://localhost:3031/get_graph/{URN}/framed</p> <p>http://localhost:3031/get_graph/{URN}/compact</p> <p>http://localhost:3031/get_graph/{URN}/csv</p> <p>http://localhost:3031/get_graph/{URN}/tsv</p>"},{"location":"notebook_proxy/#development","title":"Development:","text":""},{"location":"notebook_proxy/#install-ec-utiltites-in-development-mode","title":"install ec utiltites in development mode","text":"<pre><code>python -m pip install -e ec ../earthcube_utilities\n</code></pre> <pre><code>python -m pip install -e bar @ git+https://github.com/earthcube/earthcube_utilities.git</code></pre> <pre><code>python -m pip install -e ec /path/to/earthcube_utilities\n</code></pre>"},{"location":"notebook_proxy/#start","title":"start","text":"<p>In order to use oauth, you need to setup a github app, and set the following environment variables</p> <p>GITHUB_OAUTHSECRET = GITHUB APP Secret GITHUB_OAUTHCLIENTID = GIHUB Client ID</p> <p><pre><code>cd src/notebook_proxy\nflask mknb</code></pre> this will start it at:  http://localhost:5000/</p> <p><pre><code>cd src/notebook_proxy\npython mknb.py</code></pre>  http://localhost:3031/</p>"},{"location":"notebook_proxy/#docker","title":"Docker","text":"<p>A docker container is built using github actions:</p> <p>https://hub.docker.com/repository/docker/nsfearthcube/mknb</p> <p>Note need to update to https://github.com/tiangolo/uwsgi-nginx-flask-docker</p>"},{"location":"notebook_proxy/#setting-up-a-developer-access-token","title":"Setting up a developer access token","text":"<p>before use you need a token: * go to settings&gt;developer settings. * personal access token * create a token with gist permissions.</p>"},{"location":"notebook_proxy/#command-to-install-a-container-for-local-use","title":"command to install a container for local use:","text":"<pre><code>docker pull nsfearthcube/mknb:latest\n\ndocker run -e GITHUB_CLIENTID={GITHUB OAUTH APP} -e GITHUB_SECRET={GITHUB OAUTH APP SECRET}  -p 127.0.0.1:3031:3031 nsfearthcube/mknb:latest</code></pre>"},{"location":"notebook_proxy/#for-docker-compose","title":"For docker compose:","text":"<p>Need notes here.</p>"},{"location":"notebook_proxy/#local-build","title":"Local Build","text":"<p>You can build locally from the earthcube_utilities directory</p> <p><code>%  docker build .</code> </p> <p><code>%  docker images</code> <code>% docker images REPOSITORY          TAG       IMAGE ID       CREATED          SIZE &lt;none&gt;              &lt;none&gt;    1f0ebd17e990   22 minutes ago   959MB</code>  image id is: 1f0ebd17e990</p> <p><code>docker run -e GIST_TOKEN={YOUR TOKEN} -e GIST_USERNAME={YOU USERNAME}  -p 127.0.0.1:3031:3031 {image id}</code> </p> <p>Proxy is now running at:  http://localhost:3031/</p> <p>try: http://localhost:3031/mknb?url=http://lipdverse.org/Temp12k/1_0_2/Svartvatnet-Norway.Seppa.2009.lpd&amp;ext=application%2Fzip%20%3B%20http%3A%2F%2Flinked.earth%2Fontology%2Fcore%2F1.2.0%2Findex-en.html%3Chash%3EDataset&amp;urn=urn:gleaner:milled:lipdverse:509e465d0793506b237cea8069c3cb2d276fe9c2&amp;encoding=application%2Fzip%20%3B%20http%3A%2F%2Flinked.earth%2Fontology%2Fcore%2F1.2.0%2Findex-en.html%3Chash%3EDataset&amp;</p>"},{"location":"notebook_proxy/#testing-and-development-notes","title":"Testing and development notes","text":"<p>from raw.github http://localhost:5000/mknb?url=http://lipdverse.org/Temp12k/1_0_2/Svartvatnet-Norway.Seppa.2009.lpd&amp;ext=application%2Fzip%20%3B%20http%3A%2F%2Flinked.earth%2Fontology%2Fcore%2F1.2.0%2Findex-en.html%3Chash%3EDataset&amp;urn=urn:gleaner:milled:lipdverse:509e465d0793506b237cea8069c3cb2d276fe9c2&amp;encoding=application%2Fzip%20%3B%20http%3A%2F%2Flinked.earth%2Fontology%2Fcore%2F1.2.0%2Findex-en.html%3Chash%3EDataset&amp;template=https://raw.githubusercontent.com/earthcube/NotebookTemplates/geocodes_template/GeoCODEStemplates/ARGO/SA_01_Argo_Data_Exploration.ipynb</p> <p>http://localhost:5000/mknb?url=http://lipdverse.org/Temp12k/Svartvatnet-Norway.Seppa.2009.lpd&amp;ext=application%2Fzip%20%3B%20http%3A%2F%2Flinked.earth%2Fontology%2Fcore%2F1.2.0%2Findex-en.html%3Chash%3EDataset&amp;urn=urn:gleaner:milled:lipdverse:509e465d0793506b237cea8069c3cb2d276fe9c2&amp;encoding=application%2Fzip%20%3B%20http%3A%2F%2Flinked.earth%2Fontology%2Fcore%2F1.2.0%2Findex-en.html%3Chash%3EDataset&amp;template=https://github.com/earthcube/NotebookTemplates/blob/geocodes_template/GeoCODEStemplates/Development/2_numpy.ipynb</p>"},{"location":"src/ec/graph/sparql_files/NOTE/","title":"sparql notes","text":"<p>RENAME WITH CARE The filenames are coded into some of the courtesy classes.</p>"},{"location":"src/ec/graph/sparql_files/NOTE/#regex-notes","title":"Regex notes","text":"<pre><code>## the regesx replace grabs the first portioon, replaces it with nothing,\n## then grabs the last part and replaces it with nothing,\n## NOTE: double \\\\ the escape sequeqnce akd \\w+ becomes \\\\w+\n#((\\w+):(\\w+))$$\n#(?2)(:(\\w+))\n\n##&lt;urn:gleaner:summoned:opentopography:0024e35144d902d8b413ffd400ede6a27efe2146&gt;\n# urn:\\w+:summoned:(\\w+)\n#urn:(?:\\w+):summoned:(\\w+)\n.</code></pre>"},{"location":"src/ec/graph/sparql_files/NOTE/#notes-on-developing-new-sparql-queries","title":"notes on developing new sparql queries","text":"<p>graph: top level subj: https://portal.opentopography.org/raster?opentopoID=OTSDEM.052015.26910.1 urn all triples in graph: 88 <pre><code>SELECT distinct ?s ?p ?o ?g\nWHERE     {\n    GRAPH &lt;urn:gleaner:summoned:opentopography:237ccc9501fc0e0349c8643e61e9db1e82dbd9ac&gt; {?s ?p ?o .\n    bind(&lt;urn:gleaner:summoned:opentopography:237ccc9501fc0e0349c8643e61e9db1e82dbd9ac&gt;   as ?g)\n} }</code></pre> <p>subj_construct with blank nodes: 69</p> <pre><code>CONSTRUCT {?x ?y ?z . ?z ?w ?v } WHERE\n  {\n    &lt;https://portal.opentopography.org/raster?opentopoID=OTSDEM.052015.26910.1&gt;  ?y ?z .\n    BIND(&lt;https://portal.opentopography.org/raster?opentopoID=OTSDEM.052015.26910.1&gt;  as ?x)\n    OPTIONAL {\n      ?z ?w ?v\n      FILTER (isBlank(?z) &amp;&amp; !isBlank(?v))\n     }\n        OPTIONAL {\n      ?z ?w ?v\n      FILTER (!isBlank(?z) )\n    }\n\n}</code></pre> <p>V3 subj_construct with blank nodes: 82 mising the six  identifier triples https://doi.org/10.5069/G9J10130 <pre><code>CONSTRUCT {?x ?y ?z . ?z ?w ?v . ?z ?w1 ?v1 . ?v2 ?w3 ?v3} WHERE\n  {\n    &lt;https://portal.opentopography.org/raster?opentopoID=OTSDEM.052015.26910.1&gt;  ?y ?z .\n    BIND(&lt;https://portal.opentopography.org/raster?opentopoID=OTSDEM.052015.26910.1&gt;  as ?x)\n    OPTIONAL {\n      ?z ?w ?v\n      FILTER (isBlank(?z) &amp;&amp; !isBlank(?v))\n\n     }\n     OPTIONAL {\n      ?z ?w1 ?v1     \n    }\n    OPTIONAL {\n      ?z ?w2 ?v2\n      FILTER (isBlank(?z) &amp;&amp; isBlank(?v2))\n      ?v2 ?w3 ?v3\n     }\n\n}</code></pre></p>"},{"location":"src/ec/graph/sparql_files/__init__/","title":"init","text":""},{"location":"summarize/outdated/call-1st/","title":"Call 1st","text":""},{"location":"summarize/outdated/call-1st/#what-you-call-to-run-initial-summarization","title":"what you call to run initial summarization","text":""},{"location":"summarize/outdated/call-1st/#for-the-initial-summarziation-we-can-pull-from-blaze-and-summarize-all-the-repos-in-the-namespace-filled-by-the-crawl","title":"For the initial summarziation, we can pull from blaze and summarize all the repos in the namespace filled by the crawl","text":"<pre><code>flowchart TD;\nSR[summarize_namespace.sh];\nSR -- calls --&gt; F2[2: tsum.py] -- 1:queries --&gt; BMN[blaze's big namespace];\nF2 -- 2:produces --&gt; RT(namespace.ttl) -- ttl2blaze.sh --&gt; B[blaze's summary];\n</code></pre>"},{"location":"summarize/outdated/call-1st/#after-that-we-would-go-back-to-adding-on-a-per-repo-basis-as-see-here","title":"after that we would go back to adding on a per repo basis, as see here","text":"<p>as soon as the system is made more modular, the repo.nq can come from my crawl as well</p>"},{"location":"summarize/outdated/call-rg/","title":"Call rg","text":""},{"location":"summarize/outdated/call-rg/#what-you-call-to-run-summarization","title":"what you call to run summarization","text":""},{"location":"summarize/outdated/call-rg/#from-getting-gleaner-crawlers-quadrepo-through-making-the-summary-triples-and-loading-them","title":"from getting (gleaner) crawler's quad/repo  through making the summary triples, and loading them","text":"<pre><code>flowchart TD;\nR2S[repo_to_summary.sh]  -- calls --&gt; G[1: get_repo_release_graph.py];\nG -- produces --&gt; nq(repo.nq);\nR2S -- calls --&gt;  F2[2:materialize_summary.py] -- loads2tmp_blaze --&gt; nq;\nF2 -- produces --&gt; RT(2: repo.ttl) -- ttl2blaze.sh --&gt; B[blazegraph];\n</code></pre>"},{"location":"summarize/outdated/call-rg/#there-is-a-simplification-of-this-that-would-skip-a-server-and-just-query-a-file-here","title":"there is a simplification of this, that would skip a server and just query a file, here","text":"<p>as soon as the system is made more modular, the repo.nq can come from my crawl as well</p>"},{"location":"summarize/outdated/call/","title":"Call","text":""},{"location":"summarize/outdated/call/#what-you-call-to-run-summarization","title":"what you call to run summarization","text":""},{"location":"summarize/outdated/call/#from-getting-gleaner-crawlers-quadrepo-through-making-the-summary-triples-and-loading-them","title":"from getting (gleaner) crawler's quad/repo  through making the summary triples, and loading them","text":"<pre><code>flowchart TD;\nR2S[repo_to_summary.sh]  -- calls --&gt; G[1: fix_runX.sh];\nG -- calls --&gt; gr[1:get_repo.py]\nG -- calls --&gt; r2n[2:run2nq.py];\nr2n -- produces --&gt; nq(repo.nq);\nR2S -- calls --&gt;  F2[2:materialize_summary.py] -- loads2tmp_blaze --&gt; nq;\nF2 -- produces --&gt; RT(2: repo.ttl) -- ttl2blaze.sh --&gt; B[blazegraph];\n</code></pre>"},{"location":"summarize/outdated/call/#there-is-a-simplification-of-this-that-would-skip-a-server-and-just-query-a-file-here","title":"there is a simplification of this, that would skip a server and just query a file, here","text":"<p>as soon as the system is made more modular, the repo.nq can come from my crawl as well</p>"},{"location":"summarize/outdated/summarize/","title":"Summarize","text":""},{"location":"summarize/outdated/summarize/#-1make-summary-triples","title":"- 1:make summary triples","text":""},{"location":"summarize/outdated/summarize/#getting-crawl-quadsrepo-and-sumarizing-to-endpoint-in-one-callgraph-here","title":"getting crawl quads/repo and sumarizing to endpoint, in one callgraph here","text":""},{"location":"summarize/outdated/summarize/#in-the-new-decoder-staging-will-reduce-dependancy-on-fuseki-to-get-this-call-graph","title":"in the new decoder staging, will reduce dependancy on fuseki, to get this call graph","text":""},{"location":"summarize/outdated/summarize/#prereq-get-soon-also-have-quadsrepo-to","title":"prereq: get/(&amp; soon also have) quads/repo to","text":""},{"location":"summarize/outdated/summarize/#summarize_reposh-runs","title":"summarize_repo.sh runs:","text":""},{"location":"summarize/outdated/summarize/#fnqpy-on-a-repo-to-load-the-quads-into-repo-namespace-in-fuseki","title":"fnq.py on a repo, to load the quads into repo namespace in fuseki","text":""},{"location":"summarize/outdated/summarize/#tsumpy-for-repo-to-read-that-namespace-and-dump-summary-ttl-triples","title":"tsum.py for repo, to read that namespace and dump summary ttl triples","text":"<pre><code>flowchart TD;\nrunX[runX w/loadable repo.nq files] -- fnq:load_to_repo_namespace --&gt; F[fuseki_repo_namespace];\ntsum[tsum: cache complex query as ttl] -- reads_repo_namesapce --&gt; F;\ntsum -- dump_ttl_triples --&gt; T[repo_ttl];\n</code></pre>"},{"location":"summarize/outdated/summarize/#-2use-for-fast-sparql-on-summary-namespace","title":"- 2:use for fast sparql on summary namespace","text":"<pre><code>flowchart TD;\nT[ttl per repo]  -- load --&gt; B[blaze:summary namespace];\nsearch[ui/nb] -- 1:query --&gt; B -- 2:return_same_facets --&gt; search;\n</code></pre>"},{"location":"summarize/outdated/summarize/#-also-get-rdf-from-endpoint","title":"- also: get RDF from endpoint","text":""},{"location":"summarize/outdated/summarize/#which-gets-rid-of-need-for-a-duplicate-cache","title":"which gets rid of need for a duplicate cache","text":"<pre><code>flowchart TD;\nB[blaze] -- query_ret_facets_incl_g --&gt; U[ui/nb]; \nU -- use_g_to_get_rdf --&gt; B;\n</code></pre>"},{"location":"summarize/outdated/summarize/#-ps-metadata-from-crawls-now-easier-to-use","title":"- ps. metadata from crawl/s, now easier to use","text":""},{"location":"summarize/outdated/summarize/#runx-quads-could-come-from-gleaner-or-extruct-crawls-as-they-are-not-coupled-to-outside-systems-now","title":"runX 'quads' could come from gleaner or extruct crawls, as they are not coupled to outside systems now","text":""},{"location":"summarize/outdated/v2_proposal/","title":"v2 summary","text":""},{"location":"summarize/outdated/v2_proposal/#v2","title":"v2","text":""},{"location":"summarize/outdated/v2_proposal/#dependencies","title":"Dependencies","text":"<ul> <li>stack</li> <li>s3</li> <li>blazegraph</li> <li>glcon/nabu</li> <li>python</li> </ul>"},{"location":"summarize/outdated/v2_proposal/#workflow","title":"Workflow","text":"<p>curl and wget are not standard packages in the base docker images. Use python requests.</p>"},{"location":"summarize/outdated/v2_proposal/#summarize-a-python-script-to-call-the-methods-needed","title":"summarize a python script to call the methods needed.","text":"<ul> <li>parameters  for </li> <li>nabu_cfg. glcon generates a gleaner and a nabu file. where is this nabu file.</li> <li>repo name</li> <li>(override) graphendpoint</li> <li>(override) path to glcon</li> <li>graphsummary if true, upload summary triples to summary_namespace<ul> <li>(opt) summary_namespace (default: {repo}summary)</li> </ul> </li> <li>(later) upload to s3</li> <li>workflow</li> <li>see if nabu_cfg exists, get sparql.endpoint from file</li> <li>create temp_namespace</li> <li>run nabu to make quads into temp namespace</li> <li>run summarize_materializedview.py to create summary triples</li> <li>if uploadSummary, use python requests to upload summary</li> </ul> <pre><code>sequenceDiagram\n    participant nabu_config\n    participant summarize_repo.py\n    participant manageGraph\n    participant nabu\n    participant summarize_materializedview.py\n    participant repo.ttl\n    participant graphstore\n\n\n    summarize_repo.py-&gt;&gt;nabu_config: reads sparql from nabu\n    summarize_repo.py-&gt;&gt;manageGraph: call to create managegraph instance for repo_temp \n    summarize_repo.py-&gt;&gt;manageGraph: call to create namespace for repo_temp \n    manageGraph-&gt;&gt;graphstore: creates namespace  for repo_temp\n\n    summarize_repo.py-&gt;&gt;manageGraph: call to create managegraph instance for repo_summary \n    summarize_repo.py-&gt;&gt;manageGraph: call to create namespace for repo_summary\n\n    manageGraph-&gt;&gt;graphstore: creates namespace  for repo_summary, if it does not exist\n    summarize_repo.py-&gt;&gt;summarize_repo.py: create nabu config to load to temp  \n    summarize_repo.py-&gt;&gt;nabu:  execute nabu with modified config file\n    nabu-&gt;&gt;nabu:  reads configuration, files from s3, converts to quads \n    nabu-&gt;&gt;graphstore:  writes quads to repo_temp namespace\n    summarize_repo.py-&gt;&gt;summarize_materializedview.py: call maternilize summary\n    graphstore-&gt;&gt;summarize_materializedview.py: query temp namespace in graph store\n    summarize_materializedview.py-&gt;&gt;summarize_materializedview.py: create summary triples\n    summarize_materializedview.py-&gt;&gt;repo.ttl: write out summary triples to file\n    summarize_repo.py-&gt;&gt;manageGraph: call insert repo.ttl to repo_summary namespace\n    summarize_repo.py-&gt;&gt;manageGraph: pass  ttl string as byte[] for uploading to blazegraph\n    manageGraph-&gt;&gt;graphstore: insert ttl to repo_summary namespace\n    summarize_repo.py-&gt;&gt;manageGraph: delete repo_temp namespace\n\n</code></pre> <p>details</p> <ol> <li>cli to read parameters</li> <li> <p>read sparql.endpoint from nabu file (unless overriden by graphendpoint ) ```minio:     address: oss.geocodes.ncsa.illinois.edu     port: 443     ssl: true     accesskey: worldsbestaccesskey     secretkey: worldsbestsecretkey     bucket: gleaner objects:     bucket: gleaner     domain: us-east-1     prefix:         - summoned/iris         - org     prefixoff: [] sparql:     endpoint: https://graph.geocodes.ncsa.illinois.edu/blazegraph/namespace/iris_nabu/sparql     authenticate: false     username: \"\"     password: \"\" <pre><code>4. python request to create a temp_repo namespaces in a blazegraph (repo_temp, repo_summary)\n5. Quads/Nabu step (future:  nabu can write out a file. It now can, but no binaries done yet )\n    * modify nabu file with a correct sparql.endpoint (aka temp_repo)\n    * run nabu for repository: `glcon nabu prefix --cfg {nabu_cfg} --prefix summonned/{repo}` \n* use methods in  summarize_materializedview.py\n* ( write out to file, or whatever\n*  upload to repo_summary with a python script. read file a binary, insert\n* (  python request to delete temp namespace\n\n#### summarize_materializedview.py to make graph using rdflib.\nideas on how to improve the summary generate to make less use of print, and more use of a \nlibrary to generate triples/quad, etc\n\n1. create an rdflib graph\n```python\nfrom rdflib import Graph\n\ng = Graph()</code></pre></p> </li> <li> <p>read all graphs identifiers from temp_repo</p> </li> <li>for each graph, g<ol> <li>create rdf triples using rdflib, </li> </ol> </li> </ol> <p><pre><code>from rdflib import URIRef, BNode, Literal\nfrom rdflib import Namespace\n\nn = Namespace(\"https://schema.org/\")\ntripleuri = URIRef(g)\ntitle = Literal({title})  # passing a string\ng.add((tripleuri, n.title, title))\n## repeat for other summary triples</code></pre> 3. add tripe to graph 4. every (100/1000) save graph to file. 5. serialize <pre><code>g.serialize(format=\"nquads\", destination=\"{repo}.ttl\")</code></pre> 6. write to std.out, </p>"}]}